%%%%%%%%% ABSTRACT

\begin{abstract}
\jvl{
This work explores knowledge distillation (KD) for visually-rich document (VRD) applications such as document layout analysis (DLA) and document image classification (DIC). While VRD research is dependent on increasingly sophisticated and cumbersome models, the field has neglected to study efficiency via model compression. Here, we 
design a KD experimentation methodology\footnote{Code available at: \url{https://github.com/Jordy-VL/DistilDoc_ICDAR24}}
for more lean, performant models on document understanding (DU) tasks that are integral within larger task pipelines.
\\
We carefully selected KD strategies (\textsl{response-based, feature-based}) for distilling knowledge to and from backbones with different architectures (\textsl{ResNet, ViT, DiT}) and capacities (\textsl{base-small-tiny}).
We study what affects the teacher-student knowledge gap and find that some methods (tuned \textsl{vanilla KD}, \textsl{MSE}, \textsl{SimKD} with an apt projector) can consistently outperform supervised student training. Furthermore, we design downstream task setups to evaluate covariate shift and the robustness of distilled DLA models on zero-shot layout-aware document visual question answering (DocVQA). \\
DLA-KD experiments result in a large mAP knowledge gap, which unpredictably translates to downstream robustness, accentuating the need to further explore how to efficiently obtain more semantic document layout awareness.
}

%We design two downstream task setups where the output of distilled DLA models will 

%In ablation, we employ linear probes to investigate cross-architecture and cross-task distillation in an effort to better understand the impact of KD on visual document representations.
   % Knowledge distillation had been one of the emerging fields of research in computer vision and deep learning communities and is used in abundance for improving efficiency via model compression. However, document research, largely dependent on novel strategies and sophisticated models from the vision community, has not delved much into knowledge distillation. 
   % Here, we explore the scope of knowledge distillation in document research, as well as what benefits it can bring along, if any. We carefully selected a multitude of knowledge distillation strategies, that work differently in terms of working principles, for distilling knowledge to and from backbones with different architectures. We compare their performances with a fully supervised baseline showing that the knowledge distillation strategies mostly perform at par with supervision if not outperform. We also analyze how knowledge distillation impacts generalization on different document tasks and observed that none of the distillation strategies fully dominate over the other terms of performance or generalization and have their unique advantages and disadvantages.
\end{abstract}


% We compare their performance with a fully supervised baseline showing that the knowledge distillation strategies mostly perform at par with supervision if not outperform. We also analyze how knowledge distillation impacts generalization on different document tasks and observed that none of the distillation strategies fully dominate over the other terms of performance or generalization and have their unique advantages and disadvantages.
%region-based analysis -> better and leaner DLA models will help to more efficiently model long documents, now the models are of the same size 

