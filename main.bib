
@string(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})

@string(IJCV = {Int. J. Comput. Vis.})

@string(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})

@string(ICCV= {Int. Conf. Comput. Vis.})

@string(ECCV= {Eur. Conf. Comput. Vis.})

@string(NIPS= {Adv. Neural Inform. Process. Syst.})

@string(ICPR = {Int. Conf. Pattern Recog.})

@string(BMVC= {Brit. Mach. Vis. Conf.})

@string(TOG= {ACM Trans. Graph.})

@string(TIP  = {IEEE Trans. Image Process.})

@string(TVCG  = {IEEE Trans. Vis. Comput. Graph.})

@string(TMM  = {IEEE Trans. Multimedia})

@string(ACMMM= {ACM Int. Conf. Multimedia})

@string(ICME = {Int. Conf. Multimedia and Expo})

@string(ICASSP= {ICASSP})

@string(ICIP = {IEEE Int. Conf. Image Process.})

@string(ACCV  = {ACCV})

@string(ICLR = {Int. Conf. Learn. Represent.})

@string(IJCAI = {IJCAI})

@string(PR   = {Pattern Recognition})

@string(AAAI = {AAAI})

@string(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})

@string(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@string(SPL = {IEEE Sign. Process. Letters})

@string(VR   = {Vis. Res.})

@string(JOV  = {J. Vis.})

@string(TVC  = {The Vis. Comput.})

@string(JCST  = {J. Comput. Sci. Tech.})

@string(CGF  = {Comput. Graph. Forum})

@string(CVM = {Computational Visual Media})

@string(PAMI  = {IEEE TPAMI})

@string(IJCV  = {IJCV})

@string(CVPR  = {CVPR})

@string(ICCV  = {ICCV})

@string(ECCV  = {ECCV})

@string(NIPS  = {NeurIPS})

@string(ICPR  = {ICPR})

@string(BMVC  = {BMVC})

@string(TOG   = {ACM TOG})

@string(TIP   = {IEEE TIP})

@string(TVCG  = {IEEE TVCG})

@string(TCSVT = {IEEE TCSVT})

@string(TMM   = {IEEE TMM})

@string(ACMMM = {ACM MM})

@string(ICME  = {ICME})

@string(ICASSP= {ICASSP})

@string(ICIP  = {ICIP})

@string(ACCV  = {ACCV})

@string(ICLR  = {ICLR})

@string(IJCAI = {IJCAI})

@string(PR = {PR})

@string(AAAI = {AAAI})

@string(CVPRW= {CVPRW})

@string(CSVT = {IEEE TCSVT})

%my own works

@article{brap,
	author       = {Woef, Jordy},
	year         = 2019,
	title        = {{Boring}},
	journal      = {Information},
	publisher    = {MDPI},
	volume       = 10,
	number       = 8, 
	pages        = 248
} 


@article{francis2019transfer,
	author       = {Francis, Sumam and Van Landeghem, Jordy and Moens, Marie-Francine},
	year         = 2019,
	title        = {{Transfer Learning for Named Entity Recognition In Financial and Biomedical Documents}},
	journal      = {Information},
	publisher    = {MDPI},
	volume       = 10,
	number       = 8, 
	pages        = 248
} 

@inproceedings{VanLandeghem2020a,
	author       = {Van Landeghem, Jordy and Blaschko, Matthew B and Anckaert, Bertrand and Moens, Marie-Francine},
	year         = 2020,
	title        = {{Predictive Uncertainty for Probabilistic Novelty Detection in Text Classification}},
	booktitle    = {ICML Workshop on Uncertainty and Robustness in Deep Learning}
}

@article{VanLandeghem2022a,
	author       = {Van Landeghem, Jordy and Blaschko, Matthew and Anckaert, Bertrand and Moens, Marie-Francine},
	year         = 2022,
	title        = {{Benchmarking Scalable Predictive Uncertainty in Text Classification}},
	journal      = {IEEE Access},
	doi          = {10.1109/ACCESS.2022.3168734}
}

@inproceedings{VanLandeghem2023dude,
	author       = {Van Landeghem, Jordy and Tito, Rub{\`e}n and Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and Joziak, Pawel and Powalski, Rafal and Jurkiewicz, Dawid and Coustaty, Micka{\"e}l and Anckaert, Bertrand and Valveny, Ernest and Blaschko, Matthew and Moens, Marie-Francine and Stanis{\l}awek, Tomasz},
	year         = 2023,
	title        = {{Document Understanding Dataset and Evaluation (DUDE)}},
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {19528--19540}
}

@inproceedings{VanLandeghem2023icdar,
	author       = {Van Landeghem, Jordy and Tito, Rub{\`e}n and Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and Jurkiewicz, Dawid and Powalski, Rafa{\l} and J{\'o}ziak, Pawe{\l} and Biswas, Sanket and Coustaty, Micka{\"e}l and Stanis{\l}awek, Tomasz},
	year         = 2023,
	title        = {{ICDAR 2023 Competition on Document UnderstanDing of Everything (DUDE)}},
	booktitle    = {International Conference on Document Analysis and Recognition},
	pages        = {420--434},
	organization = {Springer}
}

@inproceedings{VanLandeghem2024bdpc,
	author       = {Van Landeghem, Jordy and Biswas, Sanket and Blaschko, Matthew and Moens, Marie-Francine},
	year         = 2024,
	title        = {{Beyond Document Page Classification: Design, Datasets, and Challenges}},
	booktitle    = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
	pages        = {2962--2972}
}

@inproceedings{VanLandeghem2024kdd,
	author       = {Van Landeghem, Jordy and Maity, Subhajit and Banerjee, Ayan and Blaschko, Matthew B and Moens, Marie-Francine and Llados, Josep and Biswas, Sanket},
	year         = 2024,
	title        = {{DistilDoc: Knowledge Distillation for Visually-Rich Document Applications}},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (under review)}
}


%others
%others
@inproceedings{10.1007/978-3-031-06555-2_44,
	author       = {Mahamoud, Ibrahim Souleiman and Coustaty, Micka{\"e}l and Joseph, Aur{\'e}lie and d'Andecy, Vincent Poulain and Ogier, Jean-Marc},
	year         = 2022,
	title        = {{QAlayout: Question Answering Layout Based on Multimodal Attention for Visual Question Answering on Corporate Document}},
	booktitle    = {{Document Analysis Systems}},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {659--673},
	isbn         = {978-3-031-06555-2},
	editor       = {Uchida, Seiichi and Barney, Elisa and Eglin, V{\'e}ronique},
	abstract     = {The extraction of information from corporate documents is increasing in the research field both for its economic aspect and a scientific challenge. To extract this information the use of textual and visual content becomes unavoidable to understand the inherent information of the image. The information to be extracted is most often fixed beforehand (i.e. classification of words by date, total amount, etc.). The information to be extracted is evolving, so we would not like to be restricted to predefine word classes. We would like to question a document such as ``which is the address of invoicing?'' as we can have several addresses in an invoice. We formulate our request as a question and our model will try to answer. Our model got the result 77.65{\%} on the Docvqa dataset while drastically reducing the number of model parameters to allow us to use it in an industrial context and we use an attention model using several modalities that help us in the interpertation of the results obtained. Our other contribution in this paper is a new dataset for Visual Question answering on corporate document of invoices from RVL-CDIP [8]. The public data on corporate documents are less present in the state-of-the-art, this contribution allow us to test our models to the invoice data with the VQA methods.}
}

@inproceedings{10.1145/3340531.3412760,
	author       = {Souza Costa, Tarc\'{\i}sio and Gottschalk, Simon and Demidova, Elena},
	year         = 2020,
	title        = {{Event-QA: A Dataset for Event-Centric Question Answering over Knowledge Graphs}},
	booktitle    = {{Proceedings of the 29th ACM International Conference on Information \& Knowledge Management}},
	location     = {Virtual Event, Ireland},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {Cikm '20},
	pages        = {3157–3164},
	doi          = {10.1145/3340531.3412760},
	isbn         = 9781450368599,
	url          = {https://doi.org/10.1145/3340531.3412760},
	abstract     = {Semantic Question Answering (QA) is a crucial technology to facilitate intuitive user access to semantic information stored in knowledge graphs. Whereas most of the existing QA systems and datasets focus on entity-centric questions, very little is known about these systems' performance in the context of events. As new event-centric knowledge graphs emerge, datasets for such questions gain importance. In this paper, we present the Event-QA dataset for answering event-centric questions over knowledge graphs. Event-QA contains 1000 semantic queries and the corresponding English, German and Portuguese verbalizations for EventKG - an event-centric knowledge graph with more than 970 thousand events.},
	numpages     = 8,
	keywords     = {knowledge graphs, dataset, events, question answering}
}

@article{abdar2020review,
	author       = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U Rajendra and others},
	year         = 2021,
	title        = {{A review of uncertainty quantification in deep learning: Techniques, applications and challenges}},
	journal      = {Information Fusion},
	publisher    = {Elsevier}
}

@inproceedings{adhikari2019docbert,
	author       = {Adhikari, Ashutosh and Ram, Achyudh and Tang, Raphael and Hamilton, William L and Lin, Jimmy},
	year         = 2020,
	title        = {{Exploring the Limits of Simple Learners in Knowledge Distillation for Document Classification with DocBERT}},
	booktitle    = {{Proceedings of the 5th Workshop on Representation Learning for NLP}},
	pages        = {72--77}
}

@inproceedings{aditya2019spatial,
	author       = {Aditya, Somak and Saha, Rudra and Yang, Yezhou and Baral, Chitta},
	year         = 2019,
	title        = {{Spatial knowledge distillation to aid visual reasoning}},
	booktitle    = {{2019 IEEE Winter Conference on Applications of Computer Vision (WACV)}},
	pages        = {227--235}
}

@inproceedings{agashe-etal-2019-juice,
	author       = {Agashe, Rajas  and Iyer, Srinivasan  and Zettlemoyer, Luke},
	year         = 2019,
	month        = nov,
	title        = {{JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation}},
	booktitle    = {{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}},
	publisher    = {Association for Computational Linguistics},
	address      = {Hong Kong, China},
	pages        = {5436--5446},
	doi          = {10.18653/v1/D19-1546},
	url          = {https://aclanthology.org/D19-1546},
	abstract     = {Interactive programming with interleaved code snippet cells and natural language markdown is recently gaining popularity in the form of Jupyter notebooks, which accelerate prototyping and collaboration. To study code generation conditioned on a long context history, we present JuICe, a corpus of 1.5 million examples with a curated test set of 3.7K instances based on online programming assignments. Compared with existing contextual code generation datasets, JuICe provides refined human-curated data, open-domain code, and an order of magnitude more training data. Using JuICe, we train models for two tasks: (1) generation of the API call sequence in a code cell, and (2) full code cell generation, both conditioned on the NL-Code history up to a particular code cell. Experiments using current baseline code generation models show that both context and distant supervision aid in generation, and that the dataset is challenging for current systems.}
}

@inproceedings{ahn2019variational,
	author       = {Ahn, Sungsoo and Hu, Shell Xu and Damianou, Andreas and Lawrence, Neil D and Dai, Zhenwen},
	year         = 2019,
	title        = {{Variational information distillation for knowledge transfer}},
	booktitle    = {{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}},
	pages        = {9163--9171}
}

@article{ainslie2023gqa,
	author       = {Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
	year         = 2023,
	title        = {{GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints}},
	journal      = {arXiv preprint arXiv:2305.13245}
}

@misc{aless2019information,
	author       = {Alessandro Achille and Giovanni Paolini and Stefano Soatto},
	year         = 2019,
	title        = {{Where is the Information in a Deep Neural Network?}},
	eprint       = {1905.12213},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

@inproceedings{alibi,
	author       = {Ofir Press and Noah Smith and Mike Lewis},
	year         = 2022,
	title        = {{Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation}},
	booktitle    = {{International Conference on Learning Representations}},
	url          = {https://openreview.net/forum?id=R8sQPpGCv0}
}

@article{amara2022bd,
	author       = {Amara, Ibtihel and Sepahvand, Nazanin and Meyer, Brett H and Gross, Warren J and Clark, James J},
	year         = 2022,
	title        = {{BD-KD: Balancing the Divergences for Online Knowledge Distillation}},
	journal      = {arXiv preprint arXiv:2212.12965}
}

@article{amodei2016concrete,
	author       = {Dario Amodei and Chris Olah and Jacob Steinhardt and Paul F. Christiano and John Schulman and Dan Man{\'{e}}},
	year         = 2016,
	title        = {{Concrete Problems in AI Safety}},
	journal      = {CoRR},
	volume       = {abs/1606.06565},
	url          = {http://arxiv.org/abs/1606.06565},
	archiveprefix = {arXiv},
	eprint       = {1606.06565},
	timestamp    = {Mon, 13 Aug 2018 16:48:59 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/AmodeiOSCSM16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{anonymous2022calibration,
	author       = {Anonymous},
	year         = 2022,
	title        = {{Calibration Regularized Training of Deep Neural Networks using Kernel Density Estimation}},
	booktitle    = {{Submitted to The Tenth International Conference on Learning Representations}},
	url          = {https://openreview.net/forum?id=1-lFH8oYTI},
	note         = {under review}
}

@inproceedings{antol2015vqa,
	author       = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
	year         = 2015,
	title        = {{Vqa: Visual question answering}},
	booktitle    = {{Proceedings of the IEEE international conference on computer vision}},
	pages        = {2425--2433}
}

@inproceedings{antonacopoulos2009realistic,
	author       = {Antonacopoulos, Apostolos and Bridson, David and Papadopoulos, Christos and Pletschacher, Stefan},
	year         = 2009,
	title        = {{A realistic dataset for performance evaluation of document layout analysis}},
	booktitle    = {{2009 10th International Conference on Document Analysis and Recognition}},
	pages        = {296--300},
	organization = {Ieee}
}

@inproceedings{appalaraju2021docformer,
	author       = {Appalaraju, Srikar and Jasani, Bhavan and Kota, Bhargava Urala and Xie, Yusheng and Manmatha, R},
	year         = 2021,
	title        = {{Docformer: End-to-end transformer for document understanding}},
	booktitle    = {{Proceedings of the IEEE/CVF International Conference on Computer Vision}},
	pages        = {993--1003}
}

@article{appalaraju2023docformerv2,
	author       = {Appalaraju, Srikar and Tang, Peng and Dong, Qi and Sankaran, Nishant and Zhou, Yichu and Manmatha, R},
	year         = 2023,
	title        = {{DocFormerv2: Local Features for Document Understanding}},
	journal      = {arXiv preprint arXiv:2306.01733}
}

@article{APTE94,
	author       = {Chidanand Apt{\'{e}} and Fred Damerau and Sholom M. Weiss},
	year         = 1994,
	title        = {{Automated Learning of Decision Rules for Text Categorization}},
	journal      = {ACM Transactions on Information Systems}
}

@inproceedings{araujo2022entropy,
	author       = {Araujo, Vladimir and Hurtado, Julio and Soto, Alvaro and Moens, Marie-Francine},
	year         = 2022,
	title        = {{Entropy-based Stability-Plasticity for Lifelong Learning}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
	pages        = {3721--3728}
}

@phdthesis{arjovsky2021out,
	author       = {Arjovsky, Martin},
	year         = 2020,
	title        = {{Out of Distribution Generalization in Machine Learning}},
	school       = {New York University}
}

@inproceedings{artieres2010neural,
	author       = {Artieres, Thierry and others},
	year         = 2010,
	title        = {{Neural conditional random fields}},
	booktitle    = {{Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics}},
	pages        = {177--184},
	organization = {JMLR Workshop and Conference Proceedings}
}

@inproceedings{ashukha_pitfalls,
	author       = {Ashukha, Arsenii and Lyzhov, Alexander and Molchanov, Dmitry and Vetrov, Dmitry},
	year         = 2019,
	title        = {{Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning}},
	booktitle    = {{International Conference on Learning Representations}},
	pages        = 11,
	abstract     = {Uncertainty estimation and ensembling methods go hand-in-hand. Uncertainty estimation is one of the main benchmarks for assessment of ensembling performance. At the same time, deep learning ensembles have provided state-of-the-art results in uncertainty estimation. In this work, we focus on in-domain uncertainty for image classiﬁcation. We explore the standards for its quantiﬁcation and point out pitfalls of existing metrics. Avoiding these pitfalls, we perform a broad study of different ensembling techniques. To provide more insight in this study, we introduce the deep ensemble equivalent (DEE) and show that many sophisticated ensembling techniques are equivalent to an ensemble of very few independently trained networks in terms of the test log-likelihood.},
	language     = {en},
	file         = {Ashukha et al. - Pitfalls of In-Domain Uncertainty Estimation and E.pdf:/home/jordy/snap/zotero-snap/common/Zotero/storage/UJ2GCL5X/Ashukha et al. - Pitfalls of In-Domain Uncertainty Estimation and E.pdf:application/pdf}
}

@article{atanov_uncertainty_2018,
	author       = {Atanov, Andrei and Ashukha, Arsenii and Molchanov, Dmitry and Neklyudov, Kirill and Vetrov, Dmitry},
	year         = 2018,
	month        = mar,
	title        = {{Uncertainty Estimation via Stochastic Batch Normalization}},
	journal      = {arXiv:1802.04893 [cs, stat]},
	url          = {http://arxiv.org/abs/1802.04893},
	urldate      = {2020-04-24},
	note         = {arXiv: 1802.04893},
	abstract     = {In this work, we investigate Batch Normalization technique and propose its probabilistic interpretation. We propose a probabilistic model and show that Batch Normalization maximazes the lower bound of its marginalized log-likelihood. Then, according to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -- an efficient approximation of proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file         = {arXiv Fulltext PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/XG2WGC3Y/Atanov et al. - 2018 - Uncertainty Estimation via Stochastic Batch Normal.pdf:application/pdf;arXiv.org Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/KAGSX56J/1802.html:text/html}
}

@article{ba2014deep,
	author       = {Ba, Jimmy and Caruana, Rich},
	year         = 2014,
	title        = {{Do deep nets really need to be deep?}},
	journal      = {Advances in neural information processing systems}
}

@article{bagherinezhad2018label,
	author       = {Bagherinezhad, Hessam and Horton, Maxwell and Rastegari, Mohammad and Farhadi, Ali},
	year         = 2018,
	title        = {{Label refinery: Improving imagenet classification through label progression}},
	journal      = {arXiv preprint arXiv:1805.02641}
}

@article{bahri2021explaining,
	author       = {Bahri, Y. and Dyer, E. and Kaplan, J. and Lee, J. and Sharma, U.},
	year         = 2021,
	title        = {{Explaining neural scaling laws}},
	journal      = {arXiv preprint arXiv:2102.06701}
}

@article{bai2021don,
	author       = {Bai, Yu and Mei, Song and Wang, Huan and Xiong, Caiming},
	year         = 2021,
	title        = {{Don't Just Blame Over-parametrization for Over-confidence: Theoretical Analysis of Calibration in Binary Classification}},
	journal      = {arXiv preprint arXiv:2102.07856}
}

@book{bakir2007predicting,
	author       = {Bakir, G{\"o}khan and Hofmann, Thomas and Smola, Alexander J and Sch{\"o}lkopf, Bernhard and Taskar, Ben},
	year         = 2007,
	title        = {{Predicting structured data}},
	publisher    = {MIT press}
}

@article{bakkali2023vlcdoc,
	author       = {Bakkali, Souhail and Ming, Zuheng and Coustaty, Mickael and Rusi{\~n}ol, Mar{\c{c}}al and Terrades, Oriol Ramos},
	year         = 2023,
	title        = {{VLCDoC: Vision-Language contrastive pre-training model for cross-Modal document classification}},
	journal      = {Pattern Recognition},
	publisher    = {Elsevier},
	volume       = 139,
	pages        = 109419
}

@inproceedings{bao2022beit,
	author       = {Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
	year         = 2022,
	title        = {{BEiT: BERT Pre-Training of Image Transformers}},
	booktitle    = {{International Conference on Learning Representations}},
	url          = {https://openreview.net/forum?id=p-BhZSz59o4}
}

@inproceedings{barman2018online,
	author       = {Barman, Siddharth and Gopalan, Aditya and Saha, Aadirupa},
	year         = 2018,
	title        = {{Online learning for structured loss spaces}},
	booktitle    = {{Proceedings of the AAAI Conference on Artificial Intelligence}},
	volume       = 32,
	number       = 1
}

@article{bartlett2006convexity,
	author       = {Bartlett, Peter L and Jordan, Michael I and McAuliffe, Jon D},
	year         = 2006,
	title        = {{Convexity, classification, and risk bounds}},
	journal      = {Journal of the American Statistical Association},
	publisher    = {Taylor \& Francis},
	volume       = 101,
	number       = 473,
	pages        = {138--156}
}

@article{baum1966statistical,
	author       = {Baum, Leonard E and Petrie, Ted},
	year         = 1966,
	title        = {{Statistical inference for probabilistic functions of finite state Markov chains}},
	journal      = {The annals of mathematical statistics},
	publisher    = {Jstor},
	volume       = 37,
	number       = 6,
	pages        = {1554--1563}
}

@article{baviskar2021multi,
	author       = {Baviskar, D. and Ahirrao, S. and Kotecha, K.},
	year         = 2021,
	title        = {{Multi-layout Unstructured Invoice Documents Dataset: A dataset for Template-free Invoice Processing and its Evaluation using AI Approaches}},
	journal      = {IEEE Access},
	volume       = 9,
	pages        = {101494--101512}
}

@article{bayes1763essay,
	author       = {Bayes, Thomas},
	year         = 1763,
	title        = {{An Essay towards solving a Problem in the Doctrine of Chances}},
	journal      = {Philosophical Transactions of the Royal Society of London},
	volume       = 53,
	pages        = {370--418}
}

@unknown{BDVQA,
	author       = {Raja, Sachin and Mondal, Ajoy and Jawahar, C.},
	year         = 2023,
	month        = {02},
	title        = {{ICDAR 2023 Competition on Visual Question Answering on Business Document Images}}
}

@article{beal2021checking,
	author       = {B{\'e}al, Marie-Pierre and Crochemore, Maxime},
	year         = 2021,
	title        = {{Checking whether a word is Hamming-isometric in linear time}},
	journal      = {arXiv preprint arXiv:2106.10541}
}

@article{belkin2018reconciling,
	author       = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	year         = 2019,
	title        = {{Reconciling modern machine-learning practice and the classical bias--variance trade-off}},
	journal      = {Proceedings of the National Academy of Sciences},
	publisher    = {National Acad Sciences},
	volume       = 116,
	number       = 32,
	pages        = {15849--15854}
}

@article{beltagy2020longformer,
	author       = {Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
	year         = 2020,
	title        = {{Longformer: The long-document transformer}},
	journal      = {arXiv preprint arXiv:2004.05150}
}

@inproceedings{bender-koller-2020-climbing,
	author       = {Bender, Emily M.  and Koller, Alexander},
	year         = 2020,
	month        = jul,
	title        = {{Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data}},
	booktitle    = {{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {5185--5198},
	doi          = {10.18653/v1/2020.acl-main.463},
	url          = {https://aclanthology.org/2020.acl-main.463},
	editor       = {Jurafsky, Dan  and Chai, Joyce  and Schluter, Natalie  and Tetreault, Joel},
	abstract     = {The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as {``}understanding{''} language or capturing {``}meaning{''}. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of {``}Taking Stock of Where We{'}ve Been and Where We{'}re Going{''}, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.}
}

@inproceedings{bensch2021key,
	author       = {Bensch, Oliver and Popa, Mirela and Spille, Constantin},
	year         = 2021,
	title        = {{Key Information Extraction From Documents: Evaluation And Generator}},
	booktitle    = {{European Semantic Web Conference (ESWC 2021) and 2nd International Workshop, in conjunction with ESWC 2021: Workshop: Deep Learning meets Ontologies and Natural Language Processing}}
}

@article{berger-etal-1996-maximum,
	author       = {Berger, Adam L.  and Della Pietra, Stephen A.  and Della Pietra, Vincent J.},
	year         = 1996,
	title        = {{A Maximum Entropy Approach to Natural Language Processing}},
	journal      = {Computational Linguistics},
	volume       = 22,
	number       = 1,
	pages        = {39--71}
}

@article{besag1974spatial,
	author       = {Besag, Julian},
	year         = 1974,
	title        = {{Spatial interaction and the statistical analysis of lattice systems}},
	journal      = {Journal of the Royal Statistical Society: Series B (Methodological)},
	publisher    = {Wiley Online Library},
	volume       = 36,
	number       = 2,
	pages        = {192--225}
}

@article{beyer2020we,
	author       = {Beyer, Lucas and H{\'e}naff, Olivier J and Kolesnikov, Alexander and Zhai, Xiaohua and Oord, A{\"a}ron van den},
	year         = 2020,
	title        = {{Are we done with imagenet?}},
	journal      = {arXiv preprint arXiv:2006.07159}
}

@article{bhagat2023sample,
	author       = {Bhagat, Sarthak and Stepputtis, Simon and Campbell, Joseph and Sycara, Katia},
	year         = 2023,
	title        = {{Sample-Efficient Learning of Novel Visual Concepts}},
	journal      = {arXiv preprint arXiv:2306.09482}
}

@inproceedings{bhojanapalli2021understanding,
	author       = {Bhojanapalli, Srinadh and Chakrabarti, Ayan and Glasner, Daniel and Li, Daliang and Unterthiner, Thomas and Veit, Andreas},
	year         = 2021,
	title        = {{Understanding robustness of transformers for image classification}},
	booktitle    = {{Proceedings of the IEEE/CVF international conference on computer vision}},
	pages        = {10231--10241}
}

@article{bickel2009discriminative,
	author       = {Bickel, Steffen and Br{\"u}ckner, Michael and Scheffer, Tobias},
	year         = 2009,
	title        = {{Discriminative learning under covariate shift.}},
	journal      = {Journal of Machine Learning Research},
	volume       = 10,
	number       = 9
}

@article{binmakhashen2019document,
	author       = {Binmakhashen, Galal M and Mahmoud, Sabri A},
	year         = 2019,
	title        = {{Document layout analysis: a comprehensive survey}},
	journal      = {ACM Computing Surveys (CSUR)},
	publisher    = {ACM New York, NY, USA},
	volume       = 52,
	number       = 6,
	pages        = {1--36}
}

@article{bishop1994novelty,
	author       = {Bishop, Christopher M},
	year         = 1994,
	title        = {{Novelty Detection and Neural Network Validation}},
	journal      = {IEE Proceedings-Vision, Image and Signal processing},
	publisher    = {Iet},
	volume       = 141,
	number       = 4,
	pages        = {217--222}
}

@article{biswas2021beyond,
	author       = {Biswas, Sanket and Riba, Pau and Llad{\'o}s, Josep and Pal, Umapada},
	year         = 2021,
	title        = {{Beyond document object detection: instance-level segmentation of complex layouts}},
	journal      = {International Journal on Document Analysis and Recognition (IJDAR)},
	publisher    = {Springer},
	volume       = 24,
	number       = 3,
	pages        = {269--281}
}

@inproceedings{biswas2021docsynth,
	author       = {Biswas, Sanket and Riba, Pau and Llad{\'o}s, Josep and Pal, Umapada},
	year         = 2021,
	title        = {{Docsynth: a layout guided approach for controllable document image synthesis}},
	booktitle    = {{International Conference on Document Analysis and Recognition}},
	pages        = {555--568},
	organization = {Springer}
}

%% Competitions and Challenges
@inproceedings{biten2019icdar,
	author       = {Biten, Ali Furkan and Tito, Ruben and Mafla, Andres and Gomez, Lluis and Rusinol, Mar{\c{c}}al and Mathew, Minesh and Jawahar, CV and Valveny, Ernest and Karatzas, Dimosthenis},
	year         = 2019,
	title        = {{Icdar 2019 competition on scene text visual question answering}},
	booktitle    = {{2019 International Conference on Document Analysis and Recognition (ICDAR)}},
	pages        = {1563--1570},
	organization = {Ieee}
}

@inproceedings{biten2019scene,
	author       = {Biten, Ali Furkan and Tito, Ruben and Mafla, Andres and Gomez, Lluis and Rusinol, Mar{\c{c}}al and Valveny, Ernest and Jawahar, CV and Karatzas, Dimosthenis},
	year         = 2019,
	title        = {{Scene text visual question answering}},
	booktitle    = {{Proceedings of the IEEE/CVF international conference on computer vision}}
}

@article{biten2022ocr,
	author       = {Biten, Ali Furkan and Tito, Ruben and Gomez, Lluis and Valveny, Ernest and Karatzas, Dimosthenis},
	year         = 2022,
	title        = {{Ocr-idl: Ocr annotations for industry document library dataset}},
	journal      = {arXiv preprint arXiv:2202.12985}
}

@inproceedings{bjerva-etal-2020-subjqa,
	author       = {Bjerva, Johannes  and Bhutani, Nikita  and Golshan, Behzad  and Tan, Wang-Chiew  and Augenstein, Isabelle},
	year         = 2020,
	month        = nov,
	title        = {{SubjQA: A Dataset for Subjectivity and Review Comprehension}},
	booktitle    = {{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {5480--5494},
	doi          = {10.18653/v1/2020.emnlp-main.442},
	url          = {https://aclanthology.org/2020.emnlp-main.442},
	abstract     = {Subjectivity is the expression of internal opinions or beliefs which cannot be objectively observed or verified, and has been shown to be important for sentiment analysis and word-sense disambiguation. Furthermore, subjectivity is an important aspect of user-generated data. In spite of this, subjectivity has not been investigated in contexts where such data is widespread, such as in question answering (QA). We develop a new dataset which allows us to investigate this relationship. We find that subjectivity is an important feature in the case of QA, albeit with more intricate interactions between subjectivity and QA performance than found in previous work on sentiment analysis. For instance, a subjective question may or may not be associated with a subjective answer. We release an English QA dataset (SubjQA) based on customer reviews, containing subjectivity annotations for questions and answer spans across 6 domains.}
}

@inproceedings{blaschko2013taxonomic,
	author       = {Blaschko, Matthew B and Zaremba, Wojciech and Gretton, Arthur},
	year         = 2013,
	title        = {{Taxonomic prediction with tree-structured covariances}},
	booktitle    = {{Joint European Conference on Machine Learning and Knowledge Discovery in Databases}},
	pages        = {304--319},
	organization = {Springer}
}

@article{blasiok2023does,
	author       = {B{\l}asiok, Jaros{\l}aw and Gopalan, Parikshit and Hu, Lunjia and Nakkiran, Preetum},
	year         = 2023,
	title        = {{When Does Optimizing a Proper Loss Yield Calibration?}},
	journal      = {arXiv preprint arXiv:2305.18764}
}

@inproceedings{blasiok2023unifying,
	author       = {B{\l}asiok, Jaros{\l}aw and Gopalan, Parikshit and Hu, Lunjia and Nakkiran, Preetum},
	year         = 2023,
	title        = {{A unifying theory of distance from calibration}},
	booktitle    = {{Proceedings of the 55th Annual ACM Symposium on Theory of Computing}},
	pages        = {1727--1740}
}

@article{Blei_VI_2017,
	author       = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	year         = 2017,
	month        = {Feb},
	title        = {{Variational Inference: A Review for Statisticians}},
	journal      = {Journal of the American Statistical Association},
	publisher    = {Informa UK Limited},
	volume       = 112,
	number       = 518,
	pages        = {859–877},
	doi          = {10.1080/01621459.2017.1285773},
	issn         = {1537-274x},
	url          = {http://dx.doi.org/10.1080/01621459.2017.1285773}
}

@inproceedings{blitzer2007biographies,
	author       = {Blitzer, John and Dredze, Mark and Pereira, Fernando},
	year         = 2007,
	title        = {{Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification}},
	booktitle    = {{Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics}},
	pages        = {440--447}
}

@misc{blogpost,
	author       = {Gelada, Carles and Buckman, Jacob},
	year         = 2020,
	title        = {{Bayesian Neural Networks Need Not Concentrate}},
	howpublished = {\url{https://jacobbuckman.com/2020-01-22-bayesian-neural-networks-need-not-concentrate/}}
}

@article{blondel2020learning,
	author       = {Blondel, Mathieu and Martins, Andr{\'e} FT and Niculae, Vlad},
	year         = 2020,
	title        = {{Learning with Fenchel-Young losses.}},
	journal      = {Journal of Machine Learning Research},
	volume       = 21,
	number       = 35,
	pages        = {1--69}
}

@article{blundell2015weight,
	author       = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	year         = 2015,
	title        = {{Weight uncertainty in neural networks}},
	journal      = {arXiv preprint arXiv:1505.05424}
}

@inproceedings{borchmann2021due,
	author       = {Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and Stanislawek, Tomasz and Jurkiewicz, Dawid and Turski, Micha{\l} and Szyndler, Karolina and Grali{\'n}ski, Filip},
	year         = 2021,
	title        = {{DUE: End-to-End Document Understanding Benchmark}},
	booktitle    = {{Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)}}
}

%% calibration and selective classification
@book{bornet2021intelligent,
	author       = {Bornet, Pascal and Barkin, Ian and Wirtz, Jochen},
	year         = 2021,
	title        = {{Intelligent automation: Welcome to the world of hyperautomation: learn how to harness artificial intelligence to boost business \& make our world more human}},
	publisher    = {World Scientific}
}

%% calibration and selective classification
@inproceedings{bottou2003stochastic,
	author       = {Bottou, L{\'e}on},
	year         = 2003,
	title        = {{Stochastic learning}},
	booktitle    = {{Summer School on Machine Learning}},
	pages        = {146--168},
	organization = {Springer}
}

@inproceedings{boughorbel2005conditionally,
	author       = {Boughorbel, Sabri and Tarel, J-P and Boujemaa, Nozha},
	year         = 2005,
	title        = {{Conditionally positive definite kernels for svm based image recognition}},
	booktitle    = {{2005 IEEE International Conference on Multimedia and Expo}},
	pages        = {113--116},
	organization = {Ieee}
}

@misc{braverman2020calibration,
	author       = {Mark Braverman and Xinyi Chen and Sham Kakade and Karthik Narasimhan and Cyril Zhang and Yi Zhang},
	year         = 2020,
	title        = {{Calibration, Entropy Rates, and Memory in Language Models}},
	url          = {https://openreview.net/forum?id=B1eQcCEtDB},
	note         = {apparently rejected}
}

@article{brazowski_collective_2020,
	author       = {Brazowski, Benjamin and Schneidman, Elad},
	year         = 2020,
	month        = jun,
	title        = {{Collective Learning by Ensembles of Altruistic Diversifying Neural Networks}},
	journal      = {arXiv:2006.11671 [cs, stat]},
	urldate      = {2020-08-21},
	note         = {arXiv: 2006.11671},
	abstract     = {Combining the predictions of collections of neural networks often outperforms the best single network. Such ensembles are typically trained independently, and their superior `wisdom of the crowd' originates from the differences between networks. Collective foraging and decision making in socially interacting animal groups is often improved or even optimal thanks to local information sharing between conspecifics. We therefore present a model for co-learning by ensembles of interacting neural networks that aim to maximize their own performance but also their functional relations to other networks. We show that ensembles of interacting networks outperform independent ones, and that optimal ensemble performance is reached when the coupling between networks increases diversity and degrades the performance of individual networks. Thus, even without a global goal for the ensemble, optimal collective behavior emerges from local interactions between networks. We show the scaling of optimal coupling strength with ensemble size, and that networks in these ensembles specialize functionally and become more `confident' in their assessments. Moreover, optimal co-learning networks differ structurally, relying on sparser activity, a wider range of synaptic weights, and higher firing rates - compared to independently trained networks. Finally, we explore interactions-based co-learning as a framework for expanding and boosting ensembles.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning}
}

@article{brier,
	author       = {Brier, Glenn W.},
	year         = 1950,
	title        = {{Verification Of Forecasts Expressed In Terms Of Probability}},
	journal      = {Monthly Weather Review},
	volume       = 78,
	number       = 1,
	pages        = {1--3},
	doi          = {10.1175/1520-0493(1950)078<0001:vofeit>2.0.co;2}
}

@article{brocker2009reliability,
	author       = {Br{\"o}cker, Jochen},
	year         = 2009,
	title        = {{Reliability, sufficiency, and the decomposition of proper scores}},
	journal      = {Quarterly Journal of the Royal Meteorological Society: A journal of the atmospheric sciences, applied meteorology and physical oceanography},
	publisher    = {Wiley Online Library},
	volume       = 135,
	number       = 643,
	pages        = {1512--1519}
}

@inproceedings{broder1997resemblance,
	author       = {Broder, A. Z.},
	year         = 1997,
	month        = {June},
	title        = {{On the resemblance and containment of documents}},
	booktitle    = {{Proceedings. Compression and Complexity of SEQUENCES 1997}},
	pages        = {21--29},
	organization = {Ieee}
}

@article{bronstein2021geometric,
	author       = {Bronstein, M. M. and Bruna, J. and Cohen, T. and Veli{\v{c}}kovi{\'c}, P.},
	year         = 2021,
	title        = {{Geometric deep learning: Grids, groups, graphs, geodesics, and gauges}},
	journal      = {arXiv preprint arXiv:2104.13478}
}

%% generic
@article{brown2020language,
	author       = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	year         = 2020,
	title        = {{Language models are few-shot learners}},
	journal      = {Advances in neural information processing systems},
	volume       = 33,
	pages        = {1877--1901}
}

@article{buckland2018document,
	author       = {Buckland, Michael},
	year         = 2018,
	title        = {{Document theory}},
	journal      = {KO Knowledge Organization},
	publisher    = {Nomos Verlagsgesellschaft mbH \& Co. KG},
	volume       = 45,
	number       = 5,
	pages        = {425--436}
}

@misc{bui2016deep,
	author       = {Thang D. Bui and Daniel Hernández-Lobato and Yingzhen Li and José Miguel Hernández-Lobato and Richard E. Turner},
	year         = 2016,
	title        = {{Deep Gaussian Processes for Regression using Approximate Expectation Propagation}},
	eprint       = {1602.04133},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}

@article{bulusu2020anomalous,
	author       = {Bulusu, Saikiran and Kailkhura, Bhavya and Li, Bo and Varshney, Pramod K and Song, Dawn},
	year         = 2020,
	title        = {{Anomalous example detection in deep learning: A survey}},
	journal      = {IEEE Access},
	publisher    = {Ieee},
	volume       = 8,
	pages        = {132330--132347}
}

@inproceedings{buttler2004short,
	author       = {Buttler, D.},
	year         = 2004,
	month        = {June},
	title        = {{A short survey of document structure similarity algorithms}},
	booktitle    = {{International conference on internet computing}},
	volume       = 7
}

@article{byrne2016note,
	author       = {Byrne, Simon and others},
	year         = 2016,
	title        = {{A note on the use of empirical AUC for evaluating probabilistic forecasts}},
	journal      = {Electronic Journal of Statistics},
	publisher    = {The Institute of Mathematical Statistics and the Bernoulli Society},
	volume       = 10,
	number       = 1,
	pages        = {380--393}
}

@inproceedings{cai2018efficient,
	author       = {Cai, Han and Chen, Tianyao and Zhang, Weinan and Yu, Yong and Wang, Jun},
	year         = 2018,
	title        = {{Efficient architecture search by network transformation}},
	booktitle    = {{Proceedings of the AAAI Conference on Artificial Intelligence}},
	volume       = 32,
	number       = 1
}

@article{caldeira2020deeply,
	author       = {Caldeira, Jo{\~a}o and Nord, Brian},
	year         = 2020,
	title        = {{Deeply uncertain: comparing methods of uncertainty quantification in deep learning algorithms}},
	journal      = {Machine Learning: Science and Technology},
	publisher    = {IOP Publishing},
	volume       = 2,
	number       = 1,
	pages        = {015002}
}

@inproceedings{Cao_2023_ICCV,
	author       = {Cao, Haoyu and Bao, Changcun and Liu, Chaohu and Chen, Huang and Yin, Kun and Liu, Hao and Liu, Yinsong and Jiang, Deqiang and Sun, Xing},
	year         = 2023,
	month        = {October},
	title        = {{Attention Where It Matters: Rethinking Visual Document Understanding with Selective Region Concentration}},
	booktitle    = {{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}},
	pages        = {19517--19527}
}

@inproceedings{cao2017deep,
	author       = {Cao, Yue and Long, Mingsheng and Wang, Jianmin and Liu, Shichen},
	year         = 2017,
	title        = {{Deep visual-semantic quantization for efficient image retrieval}},
	booktitle    = {{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}},
	pages        = {1328--1337}
}

@article{cao2022generalizing,
	author       = {Cao, Yuzhou and Cai, Tianchi and Feng, Lei and Gu, Lihong and Gu, Jinjie and An, Bo and Niu, Gang and Sugiyama, Masashi},
	year         = 2022,
	title        = {{Generalizing consistent multi-class classification with rejection to be compatible with arbitrary losses}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 35,
	pages        = {521--534}
}

@article{carmeli10_vector_valued_reprod_kernel_hilber_spaces_univer,
	author       = {Carmeli, C. and De Vito, E. and Toigo, A. and Umanit{\`a}, V.},
	year         = 2010,
	month        = {01},
	title        = {{Vector Valued Reproducing Kernel Hilbert Spaces and Universality}},
	journal      = {Analysis and Applications},
	volume       = {08},
	number       = {01},
	pages        = {19--61}
}

@article{cartuyvels2021discrete,
	author       = {Cartuyvels, Ruben and Spinks, Graham and Moens, Marie-Francine},
	year         = 2021,
	title        = {{Discrete and continuous representations and processing in deep learning: Looking forward}},
	journal      = {AI Open},
	publisher    = {Elsevier},
	volume       = 2,
	pages        = {143--159}
}

@inproceedings{castro-etal-2020-lifeqa,
	author       = {Castro, Santiago  and Azab, Mahmoud  and Stroud, Jonathan  and Noujaim, Cristina  and Wang, Ruoyao  and Deng, Jia  and Mihalcea, Rada},
	year         = 2020,
	month        = may,
	title        = {{LifeQA: A Real-life Dataset for Video Question Answering}},
	booktitle    = {{Proceedings of the Twelfth Language Resources and Evaluation Conference}},
	publisher    = {European Language Resources Association},
	address      = {Marseille, France},
	pages        = {4352--4358},
	isbn         = {979-10-95546-34-4},
	url          = {https://aclanthology.org/2020.lrec-1.536},
	abstract     = {We introduce LifeQA, a benchmark dataset for video question answering that focuses on day-to-day real-life situations. Current video question answering datasets consist of movies and TV shows. However, it is well-known that these visual domains are not representative of our day-to-day lives. Movies and TV shows, for example, benefit from professional camera movements, clean editing, crisp audio recordings, and scripted dialog between professional actors. While these domains provide a large amount of data for training models, their properties make them unsuitable for testing real-life question answering systems. Our dataset, by contrast, consists of video clips that represent only real-life scenarios. We collect 275 such video clips and over 2.3k multiple-choice questions. In this paper, we analyze the challenging but realistic aspects of LifeQA, and we apply several state-of-the-art video question answering models to provide benchmarks for future research. The full dataset is publicly available at https://lit.eecs.umich.edu/lifeqa/.},
	language     = {English}
}

@inproceedings{castro-etal-2022-wild,
	author       = {Castro, Santiago  and Deng, Naihao  and Huang, Pingxuan  and Burzo, Mihai  and Mihalcea, Rada},
	year         = 2022,
	month        = oct,
	title        = {{In-the-Wild Video Question Answering}},
	booktitle    = {{Proceedings of the 29th International Conference on Computational Linguistics}},
	publisher    = {International Committee on Computational Linguistics},
	address      = {Gyeongju, Republic of Korea},
	pages        = {5613--5635},
	url          = {https://aclanthology.org/2022.coling-1.496},
	abstract     = {Existing video understanding datasets mostly focus on human interactions, with little attention being paid to the {``}in the wild{''} settings, where the videos are recorded outdoors. We propose WILDQA, a video understanding dataset of videos recorded in outside settings. In addition to video question answering (Video QA), we also introduce the new task of identifying visual support for a given question and answer (Video Evidence Selection). Through evaluations using a wide range of baseline models, we show that WILDQA poses new challenges to the vision and language research communities. The dataset is available at https: //lit.eecs.umich.edu/wildqa/.}
}

@inproceedings{cesarini2002trainable,
	author       = {Cesarini, F. and Marinai, S. and Sarti, L. and Soda, G.},
	year         = 2002,
	month        = {August},
	title        = {{Trainable table location in document images}},
	booktitle    = {{Object recognition supported by user interaction for service robots}},
	volume       = 3,
	pages        = {236--240},
	organization = {Ieee}
}

@inproceedings{chandrasekaran2008complexity,
	author       = {Chandrasekaran, Venkat and Srebro, Nathan and Harsha, Prahladh},
	year         = 2008,
	title        = {{Complexity of inference in graphical models}},
	booktitle    = {{Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence}},
	pages        = {70--78}
}

@inproceedings{chen-etal-2021-geoqa,
	author       = {Chen, Jiaqi  and Tang, Jianheng  and Qin, Jinghui  and Liang, Xiaodan  and Liu, Lingbo  and Xing, Eric  and Lin, Liang},
	year         = 2021,
	month        = aug,
	title        = {{GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning}},
	booktitle    = {{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {513--523},
	doi          = {10.18653/v1/2021.findings-acl.46},
	url          = {https://aclanthology.org/2021.findings-acl.46}
}

@article{chen2017learning,
	author       = {Chen, Guobin and Choi, Wongun and Yu, Xiang and Han, Tony and Chandraker, Manmohan},
	year         = 2017,
	title        = {{Learning efficient object detection models with knowledge distillation}},
	journal      = {Advances in neural information processing systems},
	volume       = 30
}

@inproceedings{chen2019large,
	author       = {Chen, Wenhu  and Su, Yu  and Shen, Yilin  and Chen, Zhiyu  and Yan, Xifeng  and Wang, William Yang},
	year         = 2019,
	month        = jun,
	title        = {{How Large a Vocabulary Does Text Classification Need? A Variational Approach to Vocabulary Selection}},
	booktitle    = {{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics}},
	publisher    = {Association for Computational Linguistics},
	address      = {Minneapolis, Minnesota},
	pages        = {3487--3497},
	doi          = {10.18653/v1/N19-1352},
	url          = {https://www.aclweb.org/anthology/N19-1352}
}

@inproceedings{chen2020online,
	author       = {Chen, Defang and Mei, Jian-Ping and Wang, Can and Feng, Yan and Chen, Chun},
	year         = 2020,
	title        = {{Online knowledge distillation with diverse peers}},
	booktitle    = {{Proceedings of the AAAI conference on artificial intelligence}},
	volume       = 34,
	number       = {04},
	pages        = {3430--3437}
}

@inproceedings{chen2020uniter,
	author       = {Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
	year         = 2020,
	title        = {{Uniter: Universal image-text representation learning}},
	booktitle    = {{European conference on computer vision}},
	pages        = {104--120},
	organization = {Springer}
}

@inproceedings{chen2021cross,
	author       = {Chen, Defang and Mei, Jian-Ping and Zhang, Yuan and Wang, Can and Wang, Zhe and Feng, Yan and Chen, Chun},
	year         = 2021,
	title        = {{Cross-layer distillation with semantic calibration}},
	booktitle    = {{Proceedings of the AAAI Conference on Artificial Intelligence}}
}

@inproceedings{chen2021distilling,
	author       = {Chen, Pengguang and Liu, Shu and Zhao, Hengshuang and Jia, Jiaya},
	year         = 2021,
	title        = {{Distilling knowledge via knowledge review}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}}
}

@inproceedings{chen2022knowledge,
	author       = {Chen, Defang and Mei, Jian-Ping and Zhang, Hailin and Wang, Can and Feng, Yan and Chen, Chun},
	year         = 2022,
	title        = {{Knowledge distillation with the reused teacher classifier}},
	booktitle    = {{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}}
}

@inproceedings{chen2022meta,
	author       = {Chen, Yanda and Zhong, Ruiqi and Zha, Sheng and Karypis, George and He, He},
	year         = 2022,
	title        = {{Meta-learning via Language Model In-context Tuning}},
	booktitle    = {{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}},
	pages        = {719--730}
}

@inproceedings{chen2022xdoc,
	author       = {Chen, Jingye and Lv, Tengchao and Cui, Lei and Zhang, Cha and Wei, Furu},
	year         = 2022,
	title        = {{XDoc: Unified Pre-training for Cross-Format Document Understanding}},
	booktitle    = {{Findings of the Association for Computational Linguistics: EMNLP 2022}},
	pages        = {1006--1016}
}

@book{chevallier2016strategic,
	author       = {Chevallier, Arnaud},
	year         = 2016,
	title        = {{Strategic thinking in complex problem solving}},
	publisher    = {Oxford University Press}
}

@article{chi2020harnessing,
	author       = {Chi, Yuejie and Da Costa, Maxime Ferreira},
	year         = 2020,
	title        = {{Harnessing sparsity over the continuum: Atomic norm minimization for superresolution}},
	journal      = {IEEE Signal Processing Magazine},
	publisher    = {Ieee},
	volume       = 37,
	number       = 2,
	pages        = {39--57}
}

@article{choi_uncertainty-aware_2017,
	author       = {Choi, Sungjoon and Lee, Kyungjae and Lim, Sungbin and Oh, Songhwai},
	year         = 2017,
	month        = sep,
	title        = {{Uncertainty-Aware Learning from Demonstration using Mixture Density Networks with Sampling-Free Variance Modeling}},
	journal      = {arXiv:1709.02249 [cs]},
	url          = {http://arxiv.org/abs/1709.02249},
	urldate      = {2020-04-27},
	note         = {arXiv: 1709.02249},
	abstract     = {In this paper, we propose an uncertainty-aware learning from demonstration method by presenting a novel uncertainty estimation method utilizing a mixture density network appropriate for modeling complex and noisy human behaviors. The proposed uncertainty acquisition can be done with a single forward path without Monte Carlo sampling and is suitable for real-time robotics applications. The properties of the proposed uncertainty measure are analyzed through three different synthetic examples, absence of data, heavy measurement noise, and composition of functions scenarios. We show that each case can be distinguished using the proposed uncertainty measure and presented an uncertainty-aware learn- ing from demonstration method of an autonomous driving using this property. The proposed uncertainty-aware learning from demonstration method outperforms other compared methods in terms of safety using a complex real-world driving dataset.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
	file         = {arXiv Fulltext PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/P3AZBDJP/Choi et al. - 2017 - Uncertainty-Aware Learning from Demonstration usin.pdf:application/pdf;arXiv.org Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/BS4ACTMC/1709.html:text/html}
}

@inproceedings{choi2007node,
	author       = {Choi, Arthur and Chavira, Mark and Darwiche, Adrian},
	year         = 2007,
	title        = {{Node splitting: a scheme for generating upper bounds in Bayesian networks}},
	booktitle    = {{Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence}},
	pages        = {57--66}
}

@article{ciliberto2019localized,
	author       = {Ciliberto, Carlo and Bach, Francis and Rudi, Alessandro},
	year         = 2019,
	title        = {{Localized structured prediction}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 32
}

@inproceedings{clark2020electra,
	author       = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
	year         = 2019,
	title        = {{ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}},
	booktitle    = {{International Conference on Learning Representations}}
}

@inproceedings{colas-etal-2020-tutorialvqa,
	author       = {Colas, Anthony  and Kim, Seokhwan  and Dernoncourt, Franck  and Gupte, Siddhesh  and Wang, Zhe  and Kim, Doo Soon},
	year         = 2020,
	month        = may,
	title        = {{TutorialVQA: Question Answering Dataset for Tutorial Videos}},
	booktitle    = {{Proceedings of the Twelfth Language Resources and Evaluation Conference}},
	publisher    = {European Language Resources Association},
	address      = {Marseille, France},
	pages        = {5450--5455},
	isbn         = {979-10-95546-34-4},
	url          = {https://aclanthology.org/2020.lrec-1.670},
	abstract     = {Despite the number of currently available datasets on video-question answering, there still remains a need for a dataset involving multi-step and non-factoid answers. Moreover, relying on video transcripts remains an under-explored topic. To adequately address this, we propose a new question answering task on instructional videos, because of their verbose and narrative nature. While previous studies on video question answering have focused on generating a short text as an answer, given a question and video clip, our task aims to identify a span of a video segment as an answer which contains instructional details with various granularities. This work focuses on screencast tutorial videos pertaining to an image editing program. We introduce a dataset, TutorialVQA, consisting of about 6,000 manually collected triples of (video, question, answer span). We also provide experimental results with several baseline algorithms using the video transcripts. The results indicate that the task is challenging and call for the investigation of new algorithms.},
	language     = {English}
}

@inproceedings{collier2021correlated,
	author       = {Collier, Mark and Mustafa, Basil and Kokiopoulou, Efi and Jenatton, Rodolphe and Berent, Jesse},
	year         = 2021,
	title        = {{Correlated input-dependent label noise in large-scale image classification}},
	booktitle    = {{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}},
	pages        = {1551--1560}
}

@inproceedings{collins2001convolution,
	author       = {Collins, Michael and Duffy, Nigel},
	year         = 2001,
	title        = {{Convolution kernels for natural language}},
	booktitle    = {{Advances in neural information processing systems}},
	pages        = {625--632}
}

@article{collobert2011natural,
	author       = {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	year         = 2011,
	title        = {{Natural language processing (almost) from scratch}},
	journal      = {Journal of machine learning research},
	volume       = 12,
	number       = {Article},
	pages        = {2493--2537}
}

@article{compositelikelihoodsurvey,
	author       = {Cristiano Varin and Nancy Reid and David Firth},
	year         = 2011,
	title        = {{An Overview Of Composite Likelihood Methods}},
	journal      = {Statistica Sinica},
	publisher    = {Institute of Statistical Science, Academia Sinica},
	volume       = 21,
	number       = 1,
	pages        = {5--42},
	issn         = {10170405, 19968507},
	url          = {http://www.jstor.org/stable/24309261},
	abstract     = {A survey of recent developments in the theory and application of composite likelihood is provided, building on the review paper of Varin (2008). A range of application areas, including geostatistics, spatial extremes, and space-time models, as well as clustered and longitudinal data and time series are considered. The important area of applications to statistical genetics is omitted, in light of Larribe and Fearnhead (2011). Emphasis is given to the development of the theory, and the current state of knowledge on efficiency and robustness of composite likelihood inference.}
}

@article{cooper1990computational,
	author       = {Cooper, Gregory F},
	year         = 1990,
	title        = {{The computational complexity of probabilistic inference using Bayesian belief networks}},
	journal      = {Artificial intelligence},
	publisher    = {Elsevier},
	volume       = 42,
	number       = {2-3},
	pages        = {393--405}
}

@article{cordonnier2019relationship,
	author       = {Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
	year         = 2019,
	title        = {{On the relationship between self-attention and convolutional layers}},
	journal      = {arXiv preprint arXiv:1911.03584}
}

@inproceedings{corringdocgeneration2023,
	author       = {He, Liu and Lu, Yijuan and Corring, John and Florencio, Dinei and Zhang, Cha},
	year         = 2023,
	title        = {{Diffusion-Based Document Layout Generation}},
	booktitle    = {{Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos\'{e}, CA, USA, August 21–26, 2023, Proceedings, Part I}},
	location     = {San Jos\'{e}, CA, USA},
	publisher    = {Springer-Verlag},
	address      = {Berlin, Heidelberg},
	pages        = {361–378},
	doi          = {10.1007/978-3-031-41676-7_21},
	isbn         = {978-3-031-41675-0},
	url          = {https://doi.org/10.1007/978-3-031-41676-7\%5F21},
	abstract     = {We develop a diffusion-based approach for various document layout sequence generation. Layout sequences specify the contents of a document design in an explicit format. Our novel diffusion-based approach works in the sequence domain rather than the image domain in order to permit more complex and realistic layouts. We also introduce a new metric, Document Earth Mover’s Distance (Doc-EMD). By considering similarity between heterogeneous categories document designs, we handle the shortcomings of prior document metrics that only evaluate the same category of layouts. Our empirical analysis shows that our diffusion-based approach is comparable to or outperforming other previous methods for layout generation across various document datasets. Moreover, our metric is capable of differentiating documents better than previous metrics for specific cases.},
	numpages     = 18,
	keywords     = {Document layout, Generative models, Structured document generation, Diffusion methods}
}

@article{corso2023holistic,
	author       = {Corso, Anthony and Karamadian, David and Valentin, Romeo and Cooper, Mary and Kochenderfer, Mykel J},
	year         = 2023,
	title        = {{A Holistic Assessment of the Reliability of Machine Learning Systems}},
	journal      = {arXiv preprint arXiv:2307.10586}
}

@article{cortes2016structured,
	author       = {Cortes, Corinna and Kuznetsov, Vitaly and Mohri, Mehryar and Yang, Scott},
	year         = 2016,
	title        = {{Structured prediction theory based on factor graph complexity}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 29
}

@article{cortes2018efficient,
	author       = {Cortes, Corinna and Kuznetsov, Vitaly and Mohri, Mehryar and Storcheus, Dmitry and Yang, Scott},
	year         = 2018,
	title        = {{Efficient gradient computation for structured output learning with rational and tropical losses}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 31,
	pages        = {6810--6821}
}

@inproceedings{crflikessvm,
	author       = {Hazan, Tamir and Urtasun, Raquel},
	year         = 2010,
	title        = {{A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction}},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	publisher    = {Curran Associates, Inc.},
	volume       = 23,
	url          = {https://proceedings.neurips.cc/paper/2010/file/dc912a253d1e9ba40e2c597ed2376640-Paper.pdf},
	editor       = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta}
}

%% DocAI
@article{cui2021document,
	author       = {Cui, Lei and Xu, Yiheng and Lv, Tengchao and Wei, Furu},
	year         = 2021,
	title        = {{Document ai: Benchmarks, models and applications}},
	journal      = {arXiv preprint arXiv:2111.08609}
}

@inproceedings{culotta2004confidence,
	author       = {Culotta, Aron and McCallum, Andrew},
	year         = 2004,
	title        = {{Confidence estimation for information extraction}},
	booktitle    = {{Proceedings of HLT-NAACL 2004: Short Papers}},
	pages        = {109--112}
}

@inproceedings{da2023vision,
	author       = {Da, Cheng and Luo, Chuwei and Zheng, Qi and Yao, Cong},
	year         = 2023,
	title        = {{Vision Grid Transformer for Document Layout Analysis}},
	booktitle    = {{Proceedings of the IEEE/CVF International Conference on Computer Vision}},
	pages        = {19462--19472}
}

@article{dagum1993approximating,
	author       = {Dagum, Paul and Luby, Michael},
	year         = 1993,
	title        = {{Approximating probabilistic inference in Bayesian belief networks is NP-hard}},
	journal      = {Artificial intelligence},
	publisher    = {Elsevier},
	volume       = 60,
	number       = 1,
	pages        = {141--153}
}

@inproceedings{dancette2023improving,
	author       = {Dancette, Corentin and Whitehead, Spencer and Maheshwary, Rishabh and Vedantam, Ramakrishna and Scherer, Stefan and Chen, Xinlei and Cord, Matthieu and Rohrbach, Marcus},
	year         = 2023,
	title        = {{Improving Selective Visual Question Answering by Learning from Your Peers}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
	pages        = {24049--24059}
}

@inproceedings{dandecy2018field,
	author       = {d'Andecy, V. P. and Hartmann, E. and Rusinol, M.},
	year         = 2018,
	month        = {April},
	title        = {{Field extraction by hybrid incremental and a-priori structural templates}},
	booktitle    = {{2018 13th IAPR International Workshop on Document Analysis Systems (DAS)}},
	pages        = {251--256},
	organization = {Ieee}
}

@article{dao2022flashattention,
	author       = {Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
	year         = 2022,
	title        = {{Flashattention: Fast and memory-efficient exact attention with io-awareness}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 35,
	pages        = {16344--16359}
}

@article{dasigi2019quoref,
	author       = {Dasigi, Pradeep and Liu, Nelson F and Marasovi{\'c}, Ana and Smith, Noah A and Gardner, Matt},
	year         = 2019,
	title        = {{Quoref: A reading comprehension dataset with questions requiring coreferential reasoning}},
	journal      = {arXiv preprint arXiv:1908.05803}
}

@inproceedings{daume2009frustratingly,
	author       = {Daum{\'e} III, Hal},
	year         = 2007,
	month        = jun,
	title        = {{Frustratingly Easy Domain Adaptation}},
	booktitle    = {{Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics}},
	publisher    = {Association for Computational Linguistics},
	address      = {Prague, Czech Republic},
	pages        = {256--263},
	url          = {https://www.aclweb.org/anthology/P07-1033}
}

@article{dave2021evaluating,
	author       = {Dave, Achal and Doll{\'a}r, Piotr and Ramanan, Deva and Kirillov, Alexander and Girshick, Ross},
	year         = 2021,
	title        = {{Evaluating Large-Vocabulary Object Detectors: The Devil is in the Details}},
	journal      = {arXiv preprint arXiv:2102.01066}
}

@inproceedings{davis2006relationship,
	author       = {Davis, Jesse and Goadrich, Mark},
	year         = 2006,
	title        = {{The relationship between Precision-Recall and ROC curves}},
	booktitle    = {{Proceedings of the 23rd International Conference on Machine Learning}},
	pages        = {233--240}
}

@article{davis2022end,
	author       = {Davis, Brian and Morse, Bryan and Price, Bryan and Tensmeyer, Chris and Wigington, Curtis and Morariu, Vlad},
	year         = 2022,
	title        = {{End-to-end Document Recognition and Understanding with Dessurt}},
	journal      = {arXiv e-prints},
	pages        = {arXiv--2203}
}

@article{dawid1982well,
	author       = {Dawid, A Philip},
	year         = 1982,
	title        = {{The well-calibrated Bayesian}},
	journal      = {Journal of the American Statistical Association},
	publisher    = {Taylor \& Francis},
	volume       = 77,
	number       = 379,
	pages        = {605--610}
}

@article{de2021continual,
	author       = {De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ale{\v{s}} and Slabaugh, Gregory and Tuytelaars, Tinne},
	year         = 2021,
	title        = {{A continual learning survey: Defying forgetting in classification tasks}},
	journal      = {IEEE transactions on pattern analysis and machine intelligence},
	publisher    = {Ieee},
	volume       = 44,
	number       = 7,
	pages        = {3366--3385}
}

@article{degroot1983comparison,
	author       = {DeGroot, Morris H and Fienberg, Stephen E},
	year         = 1983,
	title        = {{The comparison and evaluation of forecasters}},
	journal      = {Journal of the Royal Statistical Society: Series D (The Statistician)},
	publisher    = {Wiley Online Library},
	volume       = 32,
	number       = {1-2},
	pages        = {12--22}
}

@book{demsar_statistical_2006,
	author       = {Demsar, Janez},
	year         = 2006,
	title        = {{Statistical Comparisons of Classifiers over Multiple Data Sets}},
	journal      = {Journal of Machine Learning Research},
	volume       = 7,
	number       = 1,
	pages        = {1--30},
	abstract     = {While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.},
	file         = {Citeseer - Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/PLYXHDK7/summary.html:text/html;Citeseer - Full Text PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/DMJ5S2AF/Demsar - 2006 - Statistical Comparisons of Classifiers over Multip.pdf:application/pdf}
}

@inproceedings{deng2009imagenet,
	author       = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year         = 2009,
	title        = {{Imagenet: A large-scale hierarchical image database}},
	booktitle    = {{2009 IEEE conference on computer vision and pattern recognition}},
	pages        = {248--255},
	organization = {Ieee}
}

@article{denker1987large,
	author       = {Denker, John and Schwartz, Daniel and Wittner, Ben and Solla, Sara and Howard, Richard and Jackel, Lawrence and Hopfield, John},
	year         = 1987,
	title        = {{Large automatic learning, rule extraction, and generalization}},
	journal      = {Complex Systems},
	volume       = 1,
	number       = 5,
	pages        = {877--922}
}

@inproceedings{depeweg2018decomposition,
	author       = {Depeweg, Stefan and Hernandez-Lobato, Jose-Miguel and Doshi-Velez, Finale and Udluft, Steffen},
	year         = 2018,
	title        = {{Decomposition of uncertainty in Bayesian deep learning for efficient and risk-sensitive learning}},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {1184--1193},
	organization = {Pmlr}
}

@inproceedings{desai2020calibration,
	author       = {Desai, Shrey and Durrett, Greg},
	year         = 2020,
	title        = {{Calibration of Pre-trained Transformers}},
	booktitle    = {{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}},
	pages        = {295--302}
}

@inproceedings{deshwal2019learning,
	author       = {Deshwal, Aryan and Doppa, Janardhan Rao and Roth, Dan},
	year         = 2019,
	title        = {{Learning and inference for structured prediction: A unifying perspective}},
	booktitle    = {{Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)}}
}

@article{dettmers2023qlora,
	author       = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	year         = 2023,
	title        = {{Qlora: Efficient finetuning of quantized llms}},
	journal      = {arXiv preprint arXiv:2305.14314}
}

@article{devlin2018bert,
	author       = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year         = 2018,
	month        = jun,
	title        = {{Bert: Pre-training of deep bidirectional transformers for language understanding}},
	journal      = {arXiv preprint arXiv:1810.04805},
	booktitle    = {{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics}},
	publisher    = {Association for Computational Linguistics},
	address      = {Minneapolis, Minnesota},
	pages        = {4171--4186},
	doi          = {10.18653/v1/N19-1423},
	url          = {https://www.aclweb.org/anthology/N19-1423},
	abstract     = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@inproceedings{devlin2019bert,
	author       = {Devlin, J. and Chang, M. W. and Lee, K. and Toutanova, K.},
	year         = 2019,
	month        = {January},
	title        = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
	booktitle    = {{Naacl-hlt (1)}}
}

%% Patterns & Transformers 



% BERT
@misc{devries2018learning,
	author       = {Terrance DeVries and Graham W. Taylor},
	year         = 2018,
	title        = {{Learning Confidence for Out-of-Distribution Detection in Neural Networks}},
	eprint       = {1802.04865},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}

@inproceedings{dhuliawala2022calibration,
	author       = {Dhuliawala, Shehzaad  and Adolphs, Leonard  and Das, Rajarshi  and Sachan, Mrinmaya},
	year         = 2022,
	month        = may,
	title        = {{Calibration of Machine Reading Systems at Scale}},
	booktitle    = {{Findings of the Association for Computational Linguistics: ACL 2022}},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland},
	pages        = {1682--1693},
	doi          = {10.18653/v1/2022.findings-acl.133},
	url          = {https://aclanthology.org/2022.findings-acl.133}
}

@inproceedings{diao_2014,
	author       = {Diao, Qiming and Qiu, Minghui and Wu, Chao-Yuan and Smola, Alexander J and Jiang, Jing and Wang, Chong},
	year         = 2014,
	title        = {{Jointly Modeling Aspects, Ratings and Sentiments for Movie Recommendation (JMARS)}},
	booktitle    = {{Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}},
	pages        = {193--202}
}

@inproceedings{dillon2009statistical,
	author       = {Dillon, Joshua and Lebanon, Guy},
	year         = 2009,
	title        = {{Statistical and computational tradeoffs in stochastic composite likelihood}},
	booktitle    = {{Artificial Intelligence and Statistics}},
	pages        = {129--136},
	organization = {Pmlr}
}

@techreport{dimmick1992nist,
	author       = {Dimmick, DL and Garris, MD and Wilson, CL},
	year         = 1992,
	title        = {{Nist special database 6. structured forms database 2}},
	institution  = {Technical report, National Institute od Standards and Technology. Advanced~…}
}

@misc{ding2019revisiting,
	author       = {Yukun Ding and Jinglan Liu and Jinjun Xiong and Yiyu Shi},
	year         = 2019,
	title        = {{Revisiting the Evaluation of Uncertainty Estimation and Its Application to Explore Model Complexity-Uncertainty Trade-Off}},
	eprint       = {1903.02050},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

@inproceedings{ding2020revisiting,
	author       = {Ding, Yukun and Liu, Jinglan and Xiong, Jinjun and Shi, Yiyu},
	year         = 2020,
	title        = {{Revisiting the evaluation of uncertainty estimation and its application to explore model complexity-uncertainty trade-off}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops}},
	pages        = {4--5}
}

@inproceedings{ding2022v,
	author       = {Ding, Yihao and Huang, Zhe and Wang, Runlin and Zhang, YanHang and Chen, Xianru and Ma, Yuzhong and Chung, Hyunsuk and Han, Soyeon Caren},
	year         = 2022,
	title        = {{V-Doc: Visual questions answers with Documents}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
	pages        = {21492--21498}
}

@article{ding2023longnet,
	author       = {Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Zheng, Nanning and Wei, Furu},
	year         = 2023,
	title        = {{Longnet: Scaling transformers to 1,000,000,000 tokens}},
	journal      = {arXiv preprint arXiv:2307.02486}
}

@inproceedings{divmbest_ECCV2012,
	author       = {Dhruv Batra, Payman Yadollahpour, Abner Guzman-Rivera, Greg Shakhnarovich},
	year         = 2012,
	title        = {{Diverse M-Best Solutions in Markov Random Fields}},
	booktitle    = {{European Conference on Computer Vision (ECCV)}}
}

@article{dolan2002benchmarking,
	author       = {Dolan, Elizabeth D and Mor{\'e}, Jorge J},
	year         = 2002,
	title        = {{Benchmarking optimization software with performance profiles}},
	journal      = {Mathematical programming},
	publisher    = {Springer},
	volume       = 91,
	number       = 2,
	pages        = {201--213}
}

@article{domke2013structured,
	author       = {Domke, Justin},
	year         = 2013,
	title        = {{Structured learning via logistic regression}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 26
}

@inproceedings{dong-etal-2018-confidence,
	author       = {Dong, Li  and Quirk, Chris  and Lapata, Mirella},
	year         = 2018,
	month        = jul,
	title        = {{Confidence Modeling for Neural Semantic Parsing}},
	booktitle    = {{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}},
	publisher    = {Association for Computational Linguistics},
	address      = {Melbourne, Australia},
	pages        = {743--753},
	doi          = {10.18653/v1/P18-1069},
	url          = {https://aclanthology.org/P18-1069}
}

@article{dosovitskiy2020image,
	author       = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
	year         = 2020,
	title        = {{An image is worth 16x16 words: Transformers for image recognition at scale}},
	journal      = {arXiv preprint arXiv:2010.11929}
}

@article{Douzon2023LongRangeTA,
	author       = {Thibault Douzon and Stefan Duffner and Christophe Garcia and J{\'e}r{\'e}my Espinas},
	year         = 2023,
	title        = {{Long-Range Transformer Architectures for Document Understanding}},
	journal      = {ArXiv},
	volume       = {abs/2309.05503},
	url          = {https://api.semanticscholar.org/CorpusID:261065956}
}

@inproceedings{du2020adversarial,
	author       = {Du, Chunning and Sun, Haifeng and Wang, Jingyu and Qi, Qi and Liao, Jianxin},
	year         = 2020,
	title        = {{Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis}},
	booktitle    = {{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}},
	pages        = {4019--4028}
}

@article{duchi2018learning,
	author       = {Duchi, John and Namkoong, Hongseok},
	year         = 2018,
	title        = {{Learning models with uniform performance via distributionally robust optimization}},
	journal      = {arXiv preprint arXiv:1810.08750}
}

@article{dude2023dataset,
	author       = {Jordy Landeghem and Rubén Tito and \L{}ukasz Borchmann and Micha\l{} Pietruszka and Pawe\l{} J\'{o}ziak and Rafa\l{} Powalski and Dawid Jurkiewicz and Mickaël Coustaty and Bertrand Ackaert and Ernest Valveny and Matthew Blaschko and Sien Moens and Tomasz Stanis\l{}awek},
	year         = 2023,
	title        = {{Document Understanding Dataset and Evaluation (DUDE)}},
	journal      = {arXiv},
	url          = {https://arxiv.org/abs/2305.08455},
	primaryclass = {cs.CV}
}

@inproceedings{dude2023dataset_OLD,
	author       = {Van Landeghem, Jordy and Borchmann, Lukasz and  Tito, Rubèn and  Pietruszka, Michał and Jurkiewicz, Dawid,  and  Powalski, Rafał and Józiak, Paweł and Coustaty, Mickaël, and Valveny, Ernest and Anckaert, Bertrand and Blaschko, Matthew and Moens, Marie-Francine and  Stanisławek, Tomasz},
	year         = 2023,
	title        = {{Document Understanding Dataset and Evaluation (DUDE)}},
	booktitle    = {{Under Review}},
	pages        = {19528--19540},
	url          = {https://smartandeasy-my.sharepoint.com/:b:/g/personal/jordy\%5Fcontract\%5Ffit/EfGR0QLYkKRNp3N0xlpFjWoBDwlCcHKDrGW3VgBTPBCICQ?e=8pPWB0},
	eprint       = {2305.08455},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}

%% qa specific
@inproceedings{dude2023icdar,
	author       = {Van Landeghem, Jordy and Borchmann, Lukasz and  Tito, Rubèn and  Pietruszka, Michał and Jurkiewicz, Dawid  and  Powalski, Rafał and Józiak, Paweł and Biswas, Sanket and Coustaty, Mickaël and  Stanisławek, Tomasz},
	year         = 2023,
	title        = {{ICDAR 2023 Competition on Document UnderstanDing of Everything (DUDE)}},
	booktitle    = {{Proceedings of the ICDAR 2023}},
	pages        = {420--434},
	organization = {Springer}
}

@inproceedings{dundar2007learning,
	author       = {Dundar, Murat and Krishnapuram, Balaji and Bi, Jinbo and Rao, R Bharat},
	year         = 2007,
	title        = {{Learning classifiers when the training data is not IID.}},
	booktitle    = {{Ijcai}},
	volume       = 2007,
	pages        = {756--61}
}

@inproceedings{dutt-etal-2022-perkgqa,
	author       = {Dutt, Ritam  and Bhattacharjee, Kasturi  and Gangadharaiah, Rashmi  and Roth, Dan  and Rose, Carolyn},
	year         = 2022,
	month        = jul,
	title        = {{PerKGQA: Question Answering over Personalized Knowledge Graphs}},
	booktitle    = {{Findings of the Association for Computational Linguistics: NAACL 2022}},
	publisher    = {Association for Computational Linguistics},
	address      = {Seattle, United States},
	pages        = {253--268},
	doi          = {10.18653/v1/2022.findings-naacl.19},
	url          = {https://aclanthology.org/2022.findings-naacl.19},
	abstract     = {Previous studies on question answering over knowledge graphs have typically operated over a single knowledge graph (KG). This KG is assumed to be known a priori and is lever- aged similarly for all users{'} queries during inference. However, such an assumption is not applicable to real-world settings, such as health- care, where one needs to handle queries of new users over unseen KGs during inference. Furthermore, privacy concerns and high computational costs render it infeasible to query the single KG that has information about all users while answering a specific user{'}s query. The above concerns motivate our question answer- ing setting over personalized knowledge graphs (PERKGQA) where each user has restricted access to their KG. We observe that current state-of-the-art KGQA methods that require learning prior node representations fare poorly. We propose two complementary approaches, PATHCBR and PATHRGCN for PERKGQA. The former is a simple non-parametric technique that employs case-based reasoning, while the latter is a parametric approach using graph neural networks. Our proposed methods circumvent learning prior representations, can generalize to unseen KGs, and outperform strong baselines on an academic and an internal dataset by 6.5{\%} and 10.5{\%}.}
}

@inproceedings{duvenaud2014avoiding,
	author       = {Duvenaud, David and Rippel, Oren and Adams, Ryan and Ghahramani, Zoubin},
	year         = 2014,
	title        = {{Avoiding pathologies in very deep networks}},
	booktitle    = {{Artificial Intelligence and Statistics}},
	pages        = {202--210},
	organization = {Pmlr}
}

@article{emmott2015meta,
	author       = {A. Emmott and S. Das and Thomas G. Dietterich and A. Fern and W. Wong},
	year         = 2015,
	title        = {{A Meta-Analysis of the Anomaly Detection Problem}},
	journal      = {arXiv: Artificial Intelligence}
}

@article{eskenazi2017comprehensive,
	author       = {Eskenazi, S. and Gomez-Kr{\"a}mer, P. and Ogier, J. M.},
	year         = 2017,
	title        = {{A comprehensive survey of mostly textual document segmentation algorithms since 2008}},
	journal      = {Pattern Recognition},
	volume       = 64,
	pages        = {1--14}
}

@techreport{EuropeanCommission2021,
	year         = 2021,
	title        = {{Laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain union legislative acts}},
	address      = {Office for Official Publications of the European Communities Luxembourg},
	institution  = {European Commission}
}

@article{everingham2010pascal,
	author       = {Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
	year         = 2010,
	title        = {{The Pascal Visual Object Classes (VOC) challenge}},
	journal      = {International Journal of Computer Vision},
	publisher    = {Springer},
	volume       = 88,
	number       = 2,
	pages        = {303--338}
}

@misc{ExecutiveOrder2020,
	year         = 2020,
	title        = {{Executive order 13960: Promoting the use of trustworthy artificial intelligence in the federal government}}
}

@book{facchinei2007finite,
	author       = {Facchinei, Francisco and Pang, Jong-Shi},
	year         = 2007,
	title        = {{Finite-dimensional variational inequalities and complementarity problems}},
	publisher    = {Springer Science \& Business Media}
}

@article{fadeeva2023lm,
	author       = {Fadeeva, Ekaterina and Vashurin, Roman and Tsvigun, Akim and Vazhentsev, Artem and Petrakov, Sergey and Fedyanin, Kirill and Vasilev, Daniil and Goncharova, Elizaveta and Panchenko, Alexander and Panov, Maxim and others},
	year         = 2023,
	title        = {{LM-Polygraph: Uncertainty Estimation for Language Models}},
	journal      = {arXiv preprint arXiv:2311.07383}
}

@inproceedings{failingloudly2019neurips,
	author       = {Rabanser, Stephan and G\"{u}nnemann, Stephan and Lipton, Zachary},
	year         = 2019,
	title        = {{Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift}},
	journal      = {arXiv preprint arXiv:1810.11953},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	publisher    = {Curran Associates, Inc.},
	volume       = 32,
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}

@inproceedings{faraone2018syq,
	author       = {Faraone, Julian and Fraser, Nicholas and Blott, Michaela and Leong, Philip HW},
	year         = 2018,
	title        = {{Syq: Learning symmetric quantization for efficient deep neural networks}},
	booktitle    = {{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}},
	pages        = {4300--4309}
}

@misc{farquhar2019radial,
	author       = {Sebastian Farquhar and Michael Osborne and Yarin Gal},
	year         = 2019,
	title        = {{Radial Bayesian Neural Networks: Beyond Discrete Support In Large-Scale Bayesian Deep Learning}},
	note         = {arXiv:1907.00865},
	eprint       = {1907.00865},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}

@misc{feinman2017detecting,
	author       = {Reuben Feinman and Ryan R. Curtin and Saurabh Shintre and Andrew B. Gardner},
	year         = 2017,
	title        = {{Detecting Adversarial Samples from Artifacts}},
	eprint       = {1703.00410},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}

@article{feng2022stop,
	author       = {Feng, Leo and Ahmed, Mohamed Osama and Hajimirsadeghi, Hossein and Abdi, Amir},
	year         = 2022,
	title        = {{Stop overcomplicating selective classification: Use max-logit}},
	journal      = {arXiv preprint arXiv:2206.09034}
}

@inproceedings{feng2023towards,
	author       = {Leo Feng and Mohamed Osama Ahmed and Hossein Hajimirsadeghi and Amir H. Abdi},
	year         = 2023,
	title        = {{Towards Better Selective Classification}},
	booktitle    = {{International Conference on Learning Representations}},
	url          = {https://openreview.net/forum?id=5gDz\%5FyTcst}
}

@article{filos_benchmarking_bayes,
	author       = {Filos, Angelos and Farquhar, Sebastian and Gomez, Aidan N and Rudner, Tim G J and Kenton, Zachary and Smith, Lewis and Alizadeh, Milad and de Kroon, Arnoud and Gal, Yarin},
	year         = 2019,
	title        = {{Benchmarking Bayesian Deep Learning with Diabetic Retinopathy Diagnosis}},
	journal      = {arXiv preprint arXiv:1912.10481},
	pages        = 11,
	abstract     = {We propose a new Bayesian deep learning ({BDL}) benchmark, inspired by a realworld medical imaging application on diabetic retinopathy diagnosis. In contrast to popular toy regression experiments on the {UCI} datasets, our benchmark can be used to assess both the scalability and the effectiveness of different techniques for uncertainty estimation, going beyond {RMSE} and {NLL}. A binary classiﬁcation task on visual inputs (512 × 512 {RGB} images of retinas) is considered, where model uncertainty is used for medical pre-screening—i.e. to refer patients to an expert when model diagnosis is uncertain. We provide a comprehensive comparison of well-tuned {BDL} techniques on the benchmark, including Monte Carlo dropout, mean-ﬁeld variational inference, an ensemble of deep models, an ensemble of dropout models, as well as a deterministic (deep) model. Baselines are ranked according to metrics derived from expert-domain to reﬂect real-world use of model uncertainty in automated diagnosis. We show that some current techniques which solve benchmarks such as {UCI} ‘overﬁt’ their uncertainty to {UCI}—when evaluated on our benchmark these underperform in comparison to simpler baselines—while other techniques that solve {UCI} do not scale or fail on the new benchmark. The code for the benchmark, its baselines, and a simple {API} for evaluating new models are made available at https://github.com/oatml/bdl-benchmarks.},
	langid       = {english},
	file         = {Filos et al. - Benchmarking Bayesian Deep Learning with Diabetic .pdf:/home/jordy/snap/zotero-snap/common/Zotero/storage/ZCKPAKSL/Filos et al. - Benchmarking Bayesian Deep Learning with Diabetic .pdf:application/pdf}
}

@inproceedings{finkel2006solving,
	author       = {Finkel, Jenny Rose and Manning, Christopher D and Ng, Andrew Y},
	year         = 2006,
	title        = {{Solving the problem of cascading errors: Approximate bayesian inference for linguistic annotation pipelines}},
	booktitle    = {{Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing}},
	pages        = {618--626}
}

@article{fisch2022calibrated,
	author       = {Fisch, Adam and Jaakkola, Tommi and Barzilay, Regina},
	year         = 2022,
	title        = {{Calibrated selective classification}},
	journal      = {arXiv preprint arXiv:2208.12084}
}

@inbook{Flach2016,
	author       = {Flach, Peter A.},
	year         = 2016,
	title        = {{Classifier Calibration}},
	booktitle    = {{Encyclopedia of Machine Learning and Data Mining}},
	publisher    = {Springer US},
	address      = {Boston, MA},
	pages        = {1--8},
	doi          = {10.1007/978-1-4899-7502-7_900-1},
	isbn         = {978-1-4899-7502-7},
	url          = {https://doi.org/10.1007/978-1-4899-7502-7\%5F900-1},
	editor       = {Sammut, Claude and Webb, Geoffrey I.},
	abstract     = {Classifier calibration is concerned with the scale on which a classifier's scores are expressed. While a classifier ultimately maps instances to discrete classes, it is often beneficial to decompose this mapping into a scoring classifier which outputs one or more real-valued numbers and a decision rule which converts these numbers into predicted classes. For example, a linear classifiermight output a positive or negative score whose magnitude is proportional to the distance between the instance and the decision boundary, in which case the decision rule would be a simple threshold on that score. The advantage of calibrating these scores to a known, domain-independent scale is that the decision rule then also takes a domain-independent form and does not have to be learned. The best-known example of this occurs when the classifier's scores approximate, in a precise sense, the posterior probabilityover the classes; the main advantage of this is that the optimal decision rule is to predict the class that minimizes expected cost averaged over all possible true classes.The main methods to obtain calibrated scores are logistic calibration, which is a parametric method that assumes that the distances on either side of the decision boundary are normally distributed and a nonparametric alternative that is variously known as isotonic regression, the pool adjacent violators (PAV) method or the ROC convex hull (ROCCH) method.}
}

@article{foong2019between,
	author       = {Foong, Andrew YK and Li, Yingzhen and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Turner, Richard E},
	year         = 2019,
	title        = {{In-Between Uncertainty in Bayesian Neural Networks}},
	journal      = {ICML Workshop on Uncertainty and Robustness in Deep Learning}
}

@misc{foong2019expressiveness,
	author       = {Andrew Y. K. Foong and David R. Burt and Yingzhen Li and Richard E. Turner},
	year         = 2019,
	title        = {{On the Expressiveness of Approximate Inference in Bayesian Neural Networks}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	eprint       = {1909.00719},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}

@inproceedings{forouzan2015incremental,
	author       = {Forouzan, Sholeh and Ihler, Alexander T},
	year         = 2015,
	title        = {{Incremental Region Selection for Mini-bucket Elimination Bounds.}},
	booktitle    = {{Uai}},
	pages        = {268--277}
}

@inproceedings{fort_ensemble_implications,
	author       = {Fort, Stanislav and Jastrzebski, Stanislaw},
	year         = 2019,
	title        = {{Large Scale Structure of Neural Network Loss Landscapes}},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	publisher    = {Curran Associates, Inc.},
	volume       = 32,
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\` Alch\'{e}-Buc and E. Fox and R. Garnett}
}

@article{fort2019deep,
	author       = {Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
	year         = 2019,
	title        = {{Deep ensembles: A loss landscape perspective}},
	journal      = {arXiv preprint arXiv:1912.02757}
}

@article{fortuin2021priors,
	author       = {Fortuin, Vincent},
	year         = 2021,
	title        = {{Priors in bayesian deep learning: A review}},
	journal      = {arXiv preprint arXiv:2105.06868}
}

@article{franzese2020isotropic,
	author       = {Franzese, Giulio and Candela, Rosa and Milios, Dimitrios and Filippone, Maurizio and Michiardi, Pietro},
	year         = 2020,
	title        = {{Isotropic SGD: a Practical Approach to Bayesian Posterior Sampling}},
	journal      = {arXiv preprint arXiv:2006.05087}
}

@inproceedings{fujinuma2023multi,
	author       = {Fujinuma, Yoshinari and Varia, Siddharth and Sankaran, Nishant and Appalaraju, Srikar and Min, Bonan and Vyas, Yogarshi},
	year         = 2023,
	title        = {{A Multi-Modal Multilingual Benchmark for Document Image Classification}},
	booktitle    = {{Findings of the Association for Computational Linguistics: EMNLP 2023}},
	pages        = {14361--14376}
}

@inproceedings{gal_dropout_2016,
	author       = {Gal, Yarin and Ghahramani, Zoubin},
	year         = 2016,
	title        = {{Dropout as a Bayesian approximation: Representing model uncertainty in deep learning}},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {1050--1059},
	abstract     = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.}
}

@phdthesis{Gal2016Uncertainty,
	author       = {Gal, Yarin},
	year         = 2016,
	title        = {{Uncertainty in Deep Learning}},
	school       = {University of Cambridge}
}

@inproceedings{gal2017concrete,
	author       = {Gal, Yarin and Hron, Jiri and Kendall, Alex},
	year         = 2017,
	title        = {{Concrete dropout}},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	pages        = {3581--3590}
}

@article{gal2018sufficient,
	author       = {Gal, Yarin and Smith, Lewis},
	year         = 2018,
	title        = {{Sufficient conditions for idealised models to have no adversarial examples: a theoretical and empirical study with Bayesian neural networks}},
	journal      = {arXiv preprint arXiv:1806.00667}
}

@article{galil2023can,
	author       = {Galil, Ido and Dabbah, Mohammed and El-Yaniv, Ran},
	year         = 2023,
	title        = {{What can we learn from the selective prediction and uncertainty estimation performance of 523 imagenet classifiers}},
	journal      = {arXiv preprint arXiv:2302.11874}
}

@inproceedings{galil2023what,
	author       = {Ido Galil and Mohammed Dabbah and Ran El-Yaniv},
	year         = 2023,
	title        = {{What Can we Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers?}},
	booktitle    = {{The Eleventh International Conference on Learning Representations}},
	url          = {https://openreview.net/forum?id=p66AzKi6Xim}
}

@inproceedings{gallo2016deep,
	author       = {Gallo, Ignazio and Noce, Lucia and Zamberletti, Alessandro and Calefati, Alessandro},
	year         = 2016,
	title        = {{Deep neural networks for page stream segmentation and classification}},
	booktitle    = {{2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)}},
	pages        = {1--7},
	organization = {Ieee}
}

@inproceedings{gandrabur2003confidence,
	author       = {Gandrabur, Simona and Foster, George},
	year         = 2003,
	title        = {{Confidence estimation for translation prediction}},
	booktitle    = {{Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003}},
	pages        = {95--102}
}

@article{ganin2016domain,
	author       = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
	year         = 2016,
	title        = {{Domain-adversarial training of neural networks}},
	journal      = {The Journal of Machine Learning Research},
	publisher    = {JMLR. org},
	volume       = 17,
	number       = 1,
	pages        = {2096--2030}
}

@article{gao2020pile,
	author       = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
	year         = 2020,
	title        = {{The pile: An 800gb dataset of diverse text for language modeling}},
	journal      = {arXiv preprint arXiv:2101.00027}
}

@inproceedings{gao2021network,
	author       = {Gao, Shangqian and Huang, Feihu and Cai, Weidong and Huang, Heng},
	year         = 2021,
	title        = {{Network pruning via performance maximization}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
	pages        = {9270--9280}
}

@article{garcez2020neurosymbolic,
	author       = {Garcez, Artur d'Avila and Lamb, Luis C},
	year         = 2020,
	title        = {{Neurosymbolic AI: the 3rd Wave}},
	journal      = {arXiv preprint arXiv:2012.05876}
}

@article{garimella2016identification,
	author       = {Garimella, Siddharth},
	year         = 2016,
	title        = {{Identification of receipts in a multi-receipt image using spectral clustering}},
	journal      = {International Journal of Computer Applications},
	publisher    = {Foundation of Computer Science},
	volume       = 155,
	number       = 2
}

@article{garipov_loss_2018,
	author       = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
	year         = 2018,
	month        = oct,
	title        = {{Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs}},
	journal      = {arXiv:1802.10026 [cs, stat]},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	pages        = {8789--8798},
	url          = {http://arxiv.org/abs/1802.10026},
	urldate      = {2020-04-24},
	note         = {arXiv: 1802.10026},
	abstract     = {The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file         = {arXiv Fulltext PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/PWNF83PH/Garipov et al. - 2018 - Loss Surfaces, Mode Connectivity, and Fast Ensembl.pdf:application/pdf;arXiv.org Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/Q2E6AGVA/1802.html:text/html}
}

@inproceedings{garncarek2020lambert,
	author       = {Garncarek, {\L}ukasz and Powalski, Rafa{\l} and Stanis{\l}awek, Tomasz and Topolski, Bartosz and Halama, Piotr and Turski, Micha{\l} and Grali{\'n}ski, Filip},
	year         = 2021,
	title        = {{Lambert: Layout-aware language modeling for information extraction}},
	booktitle    = {{International Conference on Document Analysis and Recognition}},
	pages        = {532--547},
	organization = {Springer}
}

@misc{Gartner2023,
	author       = {Gartner,Inc},
	year         = 2023,
	title        = {{Gartner Peer Community, Hyperautomation: Are You Automating Your Decision Making? survey}},
	note         = {[Online; accessed Dec. 30, 2023]},
	howpublished = {https://www.gartner.com/peer-community/oneminuteinsights/hyperautomation-automating-processes-q2t}
}

@article{gawlikowski2021survey,
	author       = {Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and others},
	year         = 2021,
	title        = {{A survey of uncertainty in deep neural networks}},
	journal      = {arXiv preprint arXiv:2107.03342}
}

@article{gebruDatasheetsDatasets2020,
	author       = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daume{\'e} III, Hal and Crawford, Kate},
	year         = 2020,
	month        = jan,
	title        = {{Datasheets for Datasets}},
	journal      = {arXiv:1803.09010 [cs]},
	abstract     = {The machine learning community currently has no standardized process for documenting datasets. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.},
	archiveprefix = {arXiv},
	eprint       = {1803.09010},
	eprinttype   = {arxiv},
	file         = {/Users/audrey/Zotero/storage/AUYFW6Q4/Gebru et al. - 2020 - Datasheets for Datasets.pdf},
	keywords     = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Machine Learning},
	language     = {en},
	primaryclass = {cs}
}

@article{gehrmann2023repairing,
	author       = {Gehrmann, Sebastian and Clark, Elizabeth and Sellam, Thibault},
	year         = 2023,
	title        = {{Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text}},
	journal      = {Journal of Artificial Intelligence Research},
	volume       = 77,
	pages        = {103--166}
}

@article{geifman2017selective,
	author       = {Geifman, Yonatan and El-Yaniv, Ran},
	year         = 2017,
	title        = {{Selective classification for deep neural networks}},
	journal      = {Advances in neural information processing systems},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	publisher    = {Curran Associates, Inc.},
	volume       = 30,
	editor       = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}

@inproceedings{geifman2018bias,
	author       = {Geifman, Yonatan and Uziel, Guy and El-Yaniv, Ran},
	year         = 2018,
	title        = {{Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers}},
	booktitle    = {{International Conference on Learning Representations}}
}

@misc{geifman2018biasreduced,
	author       = {Yonatan Geifman and Guy Uziel and Ran El-Yaniv},
	year         = 2018,
	title        = {{Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers}},
	eprint       = {1805.08206},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

@misc{geng2021romebert,
	author       = {Shijie Geng and Peng Gao and Zuohui Fu and Yongfeng Zhang},
	year         = 2021,
	title        = {{RomeBERT: Robust Training of Multi-Exit BERT}},
	eprint       = {2101.09755},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@article{geweke_analysis_2014,
	author       = {Geweke, John and Amisano, Gianni},
	year         = 2014,
	month        = feb,
	title        = {{Analysis of Variance for Bayesian Inference}},
	journal      = {Econometric Reviews},
	volume       = 33,
	number       = {1-4},
	pages        = {270--288},
	doi          = {10.1080/07474938.2013.807182},
	issn         = {0747-4938, 1532-4168},
	url          = {http://www.tandfonline.com/doi/abs/10.1080/07474938.2013.807182},
	urldate      = {2020-04-27},
	abstract     = {This paper develops a multi-way analysis of variance for non-Gaussian multivariate distributions and provides a practical simulation algorithm to estimate the corresponding components of variance. It specifically addresses variance in Bayesian predictive distributions, showing that it may be decomposed into the sum of extrinsic variance, arising from posterior uncertainty about parameters, and intrinsic variance, which would exist even if parameters were known. Depending on the application at hand, further decomposition of extrinsic or intrinsic variance (or both) may be useful. The paper shows how to produce simulation-consistent estimates of all of these components, and the method demands little additional effort or computing time beyond that already invested in the posterior simulator. It illustrates the methods using a dynamic stochastic general equilibrium model of the US economy, both before and during the global financial crisis.},
	language     = {en},
	file         = {Geweke and Amisano - 2014 - Analysis of Variance for Bayesian Inference.pdf:/home/jordy/snap/zotero-snap/common/Zotero/storage/G4AM9RBN/Geweke and Amisano - 2014 - Analysis of Variance for Bayesian Inference.pdf:application/pdf}
}

@inproceedings{ghahramani2016history,
	author       = {Ghahramani, Zoubin},
	year         = 2016,
	title        = {{A history of Bayesian neural networks}},
	booktitle    = {{NIPS Workshop on Bayesian Deep Learning}}
}

@book{gilks1995markov,
	author       = {Gilks, W.R. and Richardson, S. and Spiegelhalter, D.},
	year         = 1995,
	title        = {{Markov Chain Monte Carlo in Practice}},
	publisher    = {Taylor \& Francis},
	series       = {Chapman \& Hall/CRC Interdisciplinary Statistics},
	isbn         = 9780412055515,
	url          = {http://books.google.com/books?id=TRXrMWY\%5Fi2IC},
	added-at     = {2014-03-16t11:12:07.000+0100},
	biburl       = {https://www.bibsonomy.org/bibtex/2193890cd11f59aab40ec3b7e54660383/peter.ralph},
	interhash    = {043efb21ab1baccde979a4c67aec6e4f},
	intrahash    = {193890cd11f59aab40ec3b7e54660383},
	keywords     = {MCMC reference},
	lccn         = 98033429,
	timestamp    = {2014-03-16t11:12:07.000+0100}
}

@article{gilmer2018motivating,
	author       = {Gilmer, Justin and Adams, Ryan P and Goodfellow, Ian and Andersen, David and Dahl, George E},
	year         = 2018,
	title        = {{Motivating the rules of the game for adversarial example research}},
	journal      = {arXiv preprint arXiv:1807.06732}
}

@inproceedings{gimpel2013systematic,
	author       = {Gimpel, Kevin and Batra, Dhruv and Dyer, Chris and Shakhnarovich, Gregory},
	year         = 2013,
	title        = {{A systematic exploration of diversity in machine translation}},
	booktitle    = {{Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing}},
	pages        = {1100--1111}
}

@inproceedings{glaser2021anonymization,
	author       = {Glaser, Ingo and Schamberger, Tom and Matthes, Florian},
	year         = 2021,
	title        = {{Anonymization of german legal court rulings}},
	booktitle    = {{Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law}},
	pages        = {205--209}
}

@article{gneiting2007probabilistic,
	author       = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E},
	year         = 2007,
	title        = {{Probabilistic forecasts, calibration and sharpness}},
	journal      = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	publisher    = {Wiley Online Library},
	volume       = 69,
	number       = 2,
	pages        = {243--268}
}

@misc{gomes2023a,
	author       = {Eduardo Dadalto C{\^a}mara Gomes and Marco Romanelli and Federica Granese and Pablo Piantanida},
	year         = 2023,
	title        = {{A simple Training-Free Method for Rejection Option}},
	url          = {https://openreview.net/forum?id=K1DdnjL6p7}
}

@article{gong2021confidence,
	author       = {Gong, Yunye and Lin, Xiao and Yao, Yi and Dietterich, Thomas G and Divakaran, Ajay and Gervasio, Melinda},
	year         = 2021,
	title        = {{Confidence Calibration for Domain Generalization under Covariate Shift}},
	journal      = {arXiv preprint arXiv:2104.00742}
}

@article{goodfellow2014explaining,
	author       = {Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
	year         = 2014,
	title        = {{Explaining and harnessing adversarial examples}},
	journal      = {arXiv preprint arXiv:1412.6572}
}

@inproceedings{gordo2010bag,
	author       = {Gordo, Albert and Perronnin, Florent},
	year         = 2010,
	title        = {{A bag-of-pages approach to unordered multi-page document classification}},
	booktitle    = {{2010 20th International Conference on Pattern Recognition}},
	pages        = {1920--1923},
	organization = {Ieee}
}

@inproceedings{gordo2013document,
	author       = {Gordo, Albert and Rusinol, Mar{\c{c}}al and Karatzas, Dimosthenis and Bagdanov, Andrew D},
	year         = 2013,
	title        = {{Document classification and page stream segmentation for digital mailroom applications}},
	booktitle    = {{2013 12th International Conference on Document Analysis and Recognition}},
	pages        = {621--625},
	organization = {Ieee}
}

@article{gotmare2018using,
	author       = {Gotmare, Akhilesh and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
	year         = 2018,
	title        = {{Using mode connectivity for loss landscape analysis}},
	journal      = {ICML Workshop on Modern Trends in Nonconvex Optimization for Machine Learning}
}

@phdthesis{gotovos2019sampling,
	author       = {Gotovos, Alkis},
	year         = 2019,
	title        = {{Sampling from probabilistic submodular models}},
	school       = {ETH Zurich}
}

@article{gou2021knowledge,
	author       = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
	year         = 2021,
	title        = {{Knowledge distillation: A survey}},
	journal      = {International Journal of Computer Vision},
	publisher    = {Springer},
	volume       = 129,
	pages        = {1789--1819}
}

@article{gouk2021regularisation,
	author       = {Gouk, Henry and Frank, Eibe and Pfahringer, Bernhard and Cree, Michael J},
	year         = 2018,
	title        = {{Regularisation of neural networks by enforcing Lipschitz continuity}},
	journal      = {Machine Learning},
	publisher    = {Springer},
	volume       = 110,
	number       = 2,
	pages        = {393--416}
}

@article{granese2021doctor,
	author       = {Granese, Federica and Romanelli, Marco and Gorla, Daniele and Palamidessi, Catuscia and Piantanida, Pablo},
	year         = 2021,
	title        = {{Doctor: A simple method for detecting misclassification errors}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34,
	pages        = {5669--5681}
}

@inproceedings{graves2011practical,
	author       = {Graves, Alex},
	year         = 2011,
	title        = {{Practical variational inference for neural networks}},
	booktitle    = {{Advances in neural information processing systems}},
	pages        = {2348--2356}
}

@article{Gretton2012,
	author       = {A. Gretton and K. M. Borgwardt and M. J. Rasch and B. Sch{{\"o}}lkopf and A. J. Smola},
	year         = 2012,
	title        = {{A Kernel Two-Sample Test}},
	journal      = {Journal of Machine Learning Research},
	volume       = 13,
	number       = 25,
	pages        = {723--773}
}

@article{grimmett1973theorem,
	author       = {Grimmett, Geoffrey R},
	year         = 1973,
	title        = {{A theorem about random fields}},
	journal      = {Bulletin of the London Mathematical society},
	publisher    = {Citeseer},
	volume       = 5,
	number       = 1,
	pages        = {81--84}
}

@article{grishman_2019,
	author       = {Grishman, Ralph},
	year         = 2019,
	title        = {{Twenty-five years of information extraction}},
	journal      = {Natural Language Engineering},
	publisher    = {Cambridge University Press},
	volume       = 25,
	number       = 6,
	pages        = {677–692},
	doi          = {10.1017/s1351324919000512}
}

@article{gross2006training,
	author       = {Gross, Samuel and Russakovsky, Olga and Batzoglou, Serafim and others},
	year         = 2006,
	title        = {{Training conditional random fields for maximum labelwise accuracy}},
	journal      = {Advances in Neural Information Processing Systems},
	publisher    = {Citeseer},
	volume       = 19,
	pages        = {529--536}
}

@inproceedings{Gu_2022_CVPR,
	author       = {Gu, Zhangxuan and Meng, Changhua and Wang, Ke and Lan, Jun and Wang, Weiqiang and Gu, Ming and Zhang, Liqing},
	year         = 2022,
	month        = {June},
	title        = {{XYLayoutLM: Towards Layout-Aware Multimodal Networks for Visually-Rich Document Understanding}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}},
	pages        = {4583--4592}
}

@article{gu2021unidoc,
	author       = {Gu, Jiuxiang and Kuen, Jason and Morariu, Vlad I and Zhao, Handong and Jain, Rajiv and Barmpalios, Nikolaos and Nenkova, Ani and Sun, Tong},
	year         = 2021,
	title        = {{Unidoc: Unified pretraining framework for document understanding}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34,
	pages        = {39--50}
}

@article{gu2023knowledge,
	author       = {Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
	year         = 2023,
	title        = {{Knowledge Distillation of Large Language Models}},
	journal      = {arXiv preprint arXiv:2306.08543}
}

@article{gu2023mamba,
	author       = {Gu, Albert and Dao, Tri},
	year         = 2023,
	title        = {{Mamba: Linear-Time Sequence Modeling with Selective State Spaces}},
	journal      = {arXiv preprint arXiv:2312.00752}
}

@inproceedings{gui-etal-2017-question,
	author       = {Gui, Lin  and Hu, Jiannan  and He, Yulan  and Xu, Ruifeng  and Lu, Qin  and Du, Jiachen},
	year         = 2017,
	month        = sep,
	title        = {{A Question Answering Approach for Emotion Cause Extraction}},
	booktitle    = {{Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}},
	publisher    = {Association for Computational Linguistics},
	address      = {Copenhagen, Denmark},
	pages        = {1593--1602},
	doi          = {10.18653/v1/D17-1167},
	url          = {https://aclanthology.org/D17-1167},
	abstract     = {Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text. It is a much more difficult task compared to emotion classification. Inspired by recent advances in using deep memory networks for question answering (QA), we propose a new approach which considers emotion cause identification as a reading comprehension task in QA. Inspired by convolutional neural networks, we propose a new mechanism to store relevant context in different memory slots to model context information. Our proposed approach can extract both word level sequence features and lexical features. Performance evaluation shows that our method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01{\%} in F-measure.}
}

@inproceedings{guo2017calibration,
	author       = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
	year         = 2017,
	title        = {{On Calibration of Modern Neural Networks}},
	journal      = {arXiv preprint arXiv:1706.04599},
	booktitle    = {{Proceedings of the 34th International Conference on Machine Learning - Volume 70}},
	location     = {Sydney, NSW, Australia},
	series       = {Icml'17},
	pages        = {1321–1330},
	abstract     = {Confidence calibration - the problem of predicting probability estimates representative of the true correctness likelihood - is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling - a single-parameter variant of Platt Scaling - is surprisingly effective at calibrating predictions.},
	numpages     = 10
}

@article{guo2021distilling,
	author       = {Guo, Shuxuan and Alvarez, Jose M and Salzmann, Mathieu},
	year         = 2021,
	title        = {{Distilling image classifiers in object detectors}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34,
	pages        = {1036--1047}
}

@inproceedings{gupta-demner-fushman-2022-overview,
	author       = {Gupta, Deepak  and Demner-Fushman, Dina},
	year         = 2022,
	month        = may,
	title        = {{Overview of the MedVidQA 2022 Shared Task on Medical Video Question-Answering}},
	booktitle    = {{Proceedings of the 21st Workshop on Biomedical Language Processing}},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland},
	pages        = {264--274},
	doi          = {10.18653/v1/2022.bionlp-1.25},
	url          = {https://aclanthology.org/2022.bionlp-1.25},
	abstract     = {In this paper, we present an overview of the MedVidQA 2022 shared task, collocated with the 21st BioNLP workshop at ACL 2022. The shared task addressed two of the challenges faced by medical video question answering: (I) a video classification task that explores new approaches to medical video understanding (labeling), and (ii) a visual answer localization task. Visual answer localization refers to the identification of the relevant temporal segments (start and end timestamps) in the video where the answer to the medical question is being shown or illustrated. A total of thirteen teams participated in the shared task challenges, with eleven system descriptions submitted to the workshop. The descriptions present monomodal and multi-modal approaches developed for medical video classification and visual answer localization. This paper describes the tasks, the datasets, evaluation metrics, and baseline systems for both tasks. Finally, the paper summarizes the techniques and results of the evaluation of the various approaches explored by the participating teams.}
}

@article{gupta2020calibration,
	author       = {Gupta, Kartik and Rahimi, Amir and Ajanthan, Thalaiyasingam and Mensink, Thomas and Sminchisescu, Cristian and Hartley, Richard},
	year         = 2020,
	title        = {{Calibration of neural networks using splines}},
	journal      = {arXiv preprint arXiv:2006.12800}
}

@article{gupta2021distribution,
	author       = {Gupta, Chirag and Ramdas, Aaditya K},
	year         = 2021,
	title        = {{Distribution-free calibration guarantees for histogram binning without sample splitting}},
	journal      = {arXiv preprint arXiv:2105.04656}
}

@article{gupta2021top,
	author       = {Gupta, Chirag and Ramdas, Aaditya K},
	year         = 2021,
	title        = {{Top-label calibration and multiclass-to-binary reductions}},
	journal      = {arXiv preprint arXiv:2107.08353},
	booktitle    = {{International Conference on Learning Representations}}
}

@article{gupta2022visual,
	author       = {Gupta, Tanmay and Kembhavi, Aniruddha},
	year         = 2022,
	title        = {{Visual Programming: Compositional visual reasoning without training}},
	journal      = {arXiv preprint arXiv:2211.11559}
}

@inproceedings{gururangan2018annotation,
	author       = {Suchin Gururangan and Swabha Swayamdipta and Omer Levy and Roy Schwartz and Samuel R. Bowman and Noah A. Smith},
	year         = 2018,
	title        = {{Annotation Artifacts in Natural Language Inference Data}},
	booktitle    = {{Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics,}}
}

@inproceedings{gustafsson2020evaluating,
	author       = {Gustafsson, Fredrik K and Danelljan, Martin and Schon, Thomas B},
	year         = 2020,
	title        = {{Evaluating scalable Bayesian deep learning methods for robust computer vision}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops}},
	pages        = {318--319}
}

@inproceedings{guzman2014efficiently,
	author       = {Guzman-Rivera, Abner and Kohli, Pushmeet and Batra, Dhruv and Rutenbar, Rob},
	year         = 2014,
	title        = {{Efficiently enforcing diversity in multi-output structured prediction}},
	booktitle    = {{Artificial Intelligence and Statistics}},
	pages        = {284--292},
	organization = {Pmlr}
}

@inproceedings{gygli2017deep,
	author       = {Gygli, Michael and Norouzi, Mohammad and Angelova, Anelia},
	year         = 2017,
	title        = {{Deep value networks learn to evaluate and iteratively refine structured outputs}},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {1341--1351},
	organization = {Pmlr}
}

@article{hammersleyclifford,
	author       = {Hammersley, John, and Clifford, Peter},
	year         = 1971,
	title        = {{Markov fields on finite graphs and lattices}},
	journal      = {Unpublished manuscript}
}

@inproceedings{haralick1994document,
	author       = {Haralick},
	year         = 1994,
	title        = {{Document image understanding: Geometric and logical layout}},
	booktitle    = {{1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition}},
	pages        = {385--390},
	organization = {Ieee}
}

@inproceedings{harley2015evaluation,
	author       = {Harley, Adam W and Ufkes, Alex and Derpanis, Konstantinos G},
	year         = 2015,
	title        = {{Evaluation of deep convolutional nets for document image classification and retrieval}},
	booktitle    = {{2015 13th International Conference on Document Analysis and Recognition (ICDAR)}},
	pages        = {991--995},
	organization = {Ieee}
}

@techreport{haussler1999convolution,
	author       = {Haussler, David},
	year         = 1999,
	title        = {{Convolution kernels on discrete structures}},
	institution  = {Technical report, Department of Computer Science, University of California~…}
}

@inproceedings{he2016deep,
	author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year         = 2016,
	title        = {{Deep residual learning for image recognition}},
	booktitle    = {{Proceedings of the IEEE conference on computer vision and pattern recognition}},
	pages        = {770--778}
}

@inproceedings{he2017mask,
	author       = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
	year         = 2017,
	title        = {{Mask r-cnn}},
	booktitle    = {{Proceedings of the IEEE international conference on computer vision}},
	pages        = {2961--2969}
}

@inproceedings{he2021distilling,
	author       = {He, Yin-Yin and Wu, Jianxin and Wei, Xiu-Shen},
	year         = 2021,
	title        = {{Distilling virtual examples for long-tailed recognition}},
	booktitle    = {{Proceedings of the IEEE/CVF International Conference on Computer Vision}},
	pages        = {235--244}
}

@inproceedings{he23good,
	author       = {HE, Jiabang and HU, Yi and WANG, Lei and XU, Xing and LIU, Ning and LIU, Hui},
	title        = {{Do-GOOD: Towards distribution shift evaluation for pre-trained visual document understanding models.(2023)}},
	booktitle    = {{Sigir}},
	volume       = 23,
	pages        = {23--27}
}

@article{hendrickx2021machine,
	author       = {Hendrickx, Kilian and Perini, Lorenzo and Van der Plas, Dries and Meert, Wannes and Davis, Jesse},
	year         = 2021,
	title        = {{Machine learning with a reject option: A survey}},
	journal      = {arXiv preprint arXiv:2107.11277}
}

@article{hendrycks2016baseline,
	author       = {Hendrycks, Dan and Gimpel, Kevin},
	year         = 2016,
	title        = {{A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks}},
	journal      = {arXiv preprint arXiv:1610.02136}
}

@article{hendrycks2020many,
	author       = {Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
	year         = 2020,
	title        = {{The many faces of robustness: A critical analysis of out-of-distribution generalization}},
	journal      = {arXiv preprint arXiv:2006.16241}
}

@inproceedings{hendrycks2020pretrained,
	author       = {Hendrycks, Dan  and Liu, Xiaoyuan  and Wallace, Eric  and Dziedzic, Adam  and Krishnan, Rishabh  and Song, Dawn},
	year         = 2020,
	month        = jul,
	title        = {{Pretrained Transformers Improve Out-of-Distribution Robustness}},
	journal      = {arXiv preprint arXiv:2004.06100},
	booktitle    = {{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {2744--2751},
	doi          = {10.18653/v1/2020.acl-main.244},
	url          = {https://www.aclweb.org/anthology/2020.acl-main.244}
}

@inproceedings{heo2019knowledge,
	author       = {Heo, Byeongho and Lee, Minsik and Yun, Sangdoo and Choi, Jin Young},
	year         = 2019,
	title        = {{Knowledge transfer via distillation of activation boundaries formed by hidden neurons}},
	booktitle    = {{Proceedings of the AAAI Conference on Artificial Intelligence}},
	volume       = 33,
	number       = {01},
	pages        = {3779--3787}
}

@article{herdingOzkanRGB16,
	author       = {Ece Ozkan and Gemma Roig and Orcun Goksel and Xavier Boix},
	year         = 2016,
	title        = {{Herding Generalizes Diverse M -Best Solutions}},
	journal      = {CoRR},
	volume       = {abs/1611.04353},
	url          = {http://arxiv.org/abs/1611.04353},
	eprinttype   = {arXiv},
	eprint       = {1611.04353},
	timestamp    = {Tue, 17 Sep 2019 14:15:22 +0200}
}

@article{hernandez2012unified,
	author       = {Hern{\'a}ndez-Orallo, Jos{\'e} and Flach, Peter and Ferri, C{\`e}sar},
	year         = 2012,
	title        = {{A unified view of performance metrics: translating threshold choice into expected classification loss}},
	journal      = {The Journal of Machine Learning Research},
	publisher    = {JMLR. org},
	volume       = 13,
	number       = 1,
	pages        = {2813--2869}
}

@inproceedings{hernandez2015probabilistic,
	author       = {Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Adams, Ryan},
	year         = 2015,
	title        = {{Probabilistic backpropagation for scalable learning of bayesian neural networks}},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {1861--1869}
}

@inproceedings{hinton1993keeping,
	author       = {Hinton, Geoffrey E and Van Camp, Drew},
	year         = 1993,
	title        = {{Keeping the neural networks simple by minimizing the description length of the weights}},
	booktitle    = {{Proceedings of the Sixth Annual Conference on Computational Learning Theory}},
	pages        = {5--13}
}

@article{hinton2015distilling,
	author       = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	year         = 2015,
	title        = {{Distilling the knowledge in a neural network}},
	journal      = {arXiv preprint arXiv:1503.02531}
}

@misc{hllermeier2019aleatoric,
	author       = {Eyke Hüllermeier and Willem Waegeman},
	year         = 2019,
	title        = {{Aleatoric and Epistemic Uncertainty in Machine Learning: An Introduction to Concepts and Methods}},
	journal      = {Machine Learning},
	publisher    = {Springer},
	volume       = 110,
	number       = 3,
	pages        = {457--506},
	eprint       = {1910.09457},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

@article{hochreiter1998vanishing,
	author       = {Hochreiter, Sepp},
	year         = 1998,
	title        = {{The vanishing gradient problem during learning recurrent neural nets and problem solutions}},
	journal      = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	publisher    = {World Scientific},
	volume       = 6,
	number       = {02},
	pages        = {107--116}
}

@article{hoffman_langevin_2019,
	author       = {Hoffman, Matt},
	year         = 2019,
	title        = {{Langevin Dynamics as Nonparametric Variational Inference}},
	journal      = {2nd Symposium on Advances in Approximate Bayesian Inference},
	abstract     = {Variational inference (VI) and Markov chain Monte Carlo (MCMC) are approximate posterior inference algorithms that are often said to have complementary strengths, with VI being fast but biased and MCMC being slower but asymptotically unbiased. In this paper, we analyze gradient-based MCMC and VI procedures and find theoretical and empirical evidence that these procedures are not as different as one might think. In particular, a close examination of the Fokker-Planck equation that governs the Langevin dynamics (LD) MCMC procedure reveals that LD implicitly follows a gradient flow that corresponds to a variational inference procedure based on optimizing a nonparametric normalizing flow. This result suggests that the transient bias of LD (due to too few warmup steps) may track that of VI (due to too few optimization steps), up to differences due to VI’s parameterization and asymptotic bias. Empirically, we find that the transient biases of these algorithms (and momentum-accelerated versions) do evolve similarly. This suggests that practitioners with a limited time budget may get more accurate results by running an MCMC procedure (even if it’s far from burned in) than a VI procedure, as long as the variance of the MCMC estimator can be dealt with (e.g., by running many parallel chains).},
	file         = {Full Text PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/5R3T2QNM/Hoffman - 2019 - Langevin Dynamics as Nonparametric Variational Inf.pdf:application/pdf}
}

@inproceedings{Holtzman2020NucleusSampling,
	author       = {Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
	year         = 2020,
	title        = {{The Curious Case of Neural Text Degeneration}},
	booktitle    = {{International Conference on Learning Representations}},
	url          = {https://openreview.net/forum?id=rygGQyrFvH}
}

@inproceedings{hong2022bros,
	author       = {Hong, Teakgyu and Kim, Donghyun and Ji, Mingi and Hwang, Wonseok and Nam, Daehyun and Park, Sungrae},
	year         = 2022,
	title        = {{Bros: A pre-trained language model focusing on text and layout for better key information extraction from documents}},
	booktitle    = {{Proceedings of the AAAI Conference on Artificial Intelligence}},
	volume       = 36,
	number       = 10,
	pages        = {10767--10775}
}

@inproceedings{hopkins-etal-2019-semeval,
	author       = {Hopkins, Mark  and Le Bras, Ronan  and Petrescu-Prahova, Cristian  and Stanovsky, Gabriel  and Hajishirzi, Hannaneh  and Koncel-Kedziorski, Rik},
	year         = 2019,
	month        = jun,
	title        = {{SemEval-2019 Task 10: Math Question Answering}},
	booktitle    = {{Proceedings of the 13th International Workshop on Semantic Evaluation}},
	publisher    = {Association for Computational Linguistics},
	address      = {Minneapolis, Minnesota, USA},
	pages        = {893--899},
	doi          = {10.18653/v1/S19-2153},
	url          = {https://aclanthology.org/S19-2153},
	abstract     = {We report on the SemEval 2019 task on math question answering. We provided a question set derived from Math SAT practice exams, including 2778 training questions and 1082 test questions. For a significant subset of these questions, we also provided SMT-LIB logical form annotations and an interpreter that could solve these logical forms. Systems were evaluated based on the percentage of correctly answered questions. The top system correctly answered 45{\%} of the test questions, a considerable improvement over the 17{\%} random guessing baseline.}
}

@inproceedings{Hron2018VariationalBD,
	author       = {Jiri Hron and Alexander G. de G. Matthews and Zoubin Ghahramani},
	year         = 2018,
	title        = {{Variational Bayesian dropout: pitfalls and fixes}},
	booktitle    = {{Icml}}
}

@article{hsieh2023distilling,
	author       = {Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
	year         = 2023,
	title        = {{Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes}},
	journal      = {arXiv preprint arXiv:2305.02301}
}

@misc{https://doi.org/10.48550/arxiv.1505.00468,
	author       = {Agrawal, Aishwarya and Lu, Jiasen and Antol, Stanislaw and Mitchell, Margaret and Zitnick, C. Lawrence and Batra, Dhruv and Parikh, Devi},
	year         = 2015,
	title        = {{VQA: Visual Question Answering}},
	publisher    = {arXiv},
	doi          = {10.48550/arxiv.1505.00468},
	url          = {https://arxiv.org/abs/1505.00468},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences}
}

@misc{https://doi.org/10.48550/arxiv.1802.08218,
	author       = {Gurari, Danna and Li, Qing and Stangl, Abigale J. and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P.},
	year         = 2018,
	title        = {{VizWiz Grand Challenge: Answering Visual Questions from Blind People}},
	publisher    = {arXiv},
	doi          = {10.48550/arxiv.1802.08218},
	url          = {https://arxiv.org/abs/1802.08218},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences}
}

@misc{https://doi.org/10.48550/arxiv.1905.13319,
	author       = {Amini, Aida and Gabriel, Saadia and Lin, Peter and Koncel-Kedziorski, Rik and Choi, Yejin and Hajishirzi, Hannaneh},
	year         = 2019,
	title        = {{MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms}},
	publisher    = {arXiv},
	doi          = {10.48550/arxiv.1905.13319},
	url          = {https://arxiv.org/abs/1905.13319},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}

@misc{https://doi.org/10.48550/arxiv.2004.10645,
	author       = {Min, Sewon and Michael, Julian and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
	year         = 2020,
	title        = {{AmbigQA: Answering Ambiguous Open-domain Questions}},
	publisher    = {arXiv},
	doi          = {10.48550/arxiv.2004.10645},
	url          = {https://arxiv.org/abs/2004.10645},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences}
}

@misc{https://doi.org/10.48550/arxiv.2004.10796,
	author       = {Park, Jae Sung and Bhagavatula, Chandra and Mottaghi, Roozbeh and Farhadi, Ali and Choi, Yejin},
	year         = 2020,
	title        = {{VisualCOMET: Reasoning about the Dynamic Context of a Still Image}},
	publisher    = {arXiv},
	doi          = {10.48550/arxiv.2004.10796},
	url          = {https://arxiv.org/abs/2004.10796},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}

@misc{https://doi.org/10.48550/arxiv.2007.15207,
	author       = {Longpre, Shayne and Lu, Yi and Daiber, Joachim},
	year         = 2020,
	title        = {{MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering}},
	publisher    = {arXiv},
	doi          = {10.48550/arxiv.2007.15207},
	url          = {https://arxiv.org/abs/2007.15207},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}

,
  author       = {Xinbo Zhang and Changzhi Sun and Yue Zhang and Lei Li and Hao Zhou},
  year         = 2022,
  url          = {https://openreview.net/forum?id=djhu4DIZZHR}
}
@misc{https://doi.org/10.48550/arxiv.2204.07408,
	author       = {Yang, Linyi and Wang, Zhen and Wu, Yuxiang and Yang, Jie and Zhang, Yue},
	year         = 2022,
	title        = {{Towards Fine-grained Causal Reasoning and QA}},
	publisher    = {arXiv},
	doi          = {10.48550/arxiv.2204.07408},
	url          = {https://arxiv.org/abs/2204.07408},
	copyright    = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	keywords     = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Logic in Computer Science (cs.LO), FOS: Computer and information sciences, FOS: Computer and information sciences}
}

@misc{https://doi.org/10.48550/arxiv.2206.04045,
	author       = {Pietruszka, Michał and Turski, Michał and Borchmann, Łukasz and Dwojak, Tomasz and Pałka, Gabriela and Szyndler, Karolina and Jurkiewicz, Dawid and Garncarek, Łukasz},
	year         = 2022,
	title        = {{STable: Table Generation Framework for Encoder-Decoder Models}},
	publisher    = {arXiv},
	doi          = {10.48550/arxiv.2206.04045},
	url          = {https://arxiv.org/abs/2206.04045},
	copyright    = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	keywords     = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}

@misc{https://doi.org/10.48550/arxiv.2211.08545,
	author       = {Chang, Shuaichen and Palzer, David and Li, Jialin and Fosler-Lussier, Eric and Xiao, Ningchuan},
	year         = 2022,
	title        = {{MapQA: A Dataset for Question Answering on Choropleth Maps}},
	publisher    = {arXiv},
	doi          = {10.48550/arxiv.2211.08545},
	url          = {https://arxiv.org/abs/2211.08545},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}

@inproceedings{hu-etal-2022-chef,
	author       = {Hu, Xuming  and Guo, Zhijiang  and Wu, GuanYu  and Liu, Aiwei  and Wen, Lijie  and Yu, Philip},
	year         = 2022,
	month        = jul,
	title        = {{CHEF: A Pilot Chinese Dataset for Evidence-Based Fact-Checking}},
	booktitle    = {{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}},
	publisher    = {Association for Computational Linguistics},
	address      = {Seattle, United States},
	pages        = {3362--3376},
	doi          = {10.18653/v1/2022.naacl-main.246},
	url          = {https://aclanthology.org/2022.naacl-main.246},
	abstract     = {The explosion of misinformation spreading in the media ecosystem urges for automated fact-checking. While misinformation spans both geographic and linguistic boundaries, most work in the field has focused on English. Datasets and tools available in other languages, such as Chinese, are limited. In order to bridge this gap, we construct CHEF, the first CHinese Evidence-based Fact-checking dataset of 10K real-world claims. The dataset covers multiple domains, ranging from politics to public health, and provides annotated evidence retrieved from the Internet. Further, we develop established baselines and a novel approach that is able to model the evidence retrieval as a latent variable, allowing jointly training with the veracity prediction model in an end-to-end fashion. Extensive experiments show that CHEF will provide a challenging testbed for the development of fact-checking systems designed to retrieve and reason over non-English claims.}
}

@inproceedings{hu2019read+,
	author       = {Hu, Minghao and Wei, Furu and Peng, Yuxing and Huang, Zhen and Yang, Nan and Li, Dongsheng},
	year         = 2019,
	title        = {{Read+ verify: Machine reading comprehension with unanswerable questions}},
	booktitle    = {{Proceedings of the AAAI Conference on Artificial Intelligence}},
	volume       = 33,
	number       = {01},
	pages        = {6529--6537}
}

@article{hu2021lora,
	author       = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	year         = 2021,
	title        = {{Lora: Low-rank adaptation of large language models}},
	journal      = {arXiv preprint arXiv:2106.09685}
}

@inproceedings{huang2015bidirectional,
	author       = {Huang, Zhiheng and Xu, Wei and Yu, Kai},
	year         = 2015,
	title        = {{Bidirectional LSTM-CRF models for sequence tagging}},
	booktitle    = {{Proceedings of the 21st International Conference on Asian Language Processing}}
}

@inproceedings{huang2017snapshot,
	author       = {Gao Huang and Yixuan Li and Geoff Pleiss and Zhuang Liu and John E. Hopcroft and Kilian Q. Weinberger},
	year         = 2017,
	title        = {{Snapshot Ensembles: Train 1, get M for free}},
	booktitle    = {{International Conference on Learning Representations}},
	eprint       = {1704.00109},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

@inproceedings{huang2019icdar2019,
	author       = {Huang, Zheng and Chen, Kai and He, Jianhua and Bai, Xiang and Karatzas, Dimosthenis and Lu, Shijian and Jawahar, CV},
	year         = 2019,
	title        = {{Icdar2019 competition on scanned receipt ocr and information extraction}},
	booktitle    = {{2019 International Conference on Document Analysis and Recognition (ICDAR)}},
	pages        = {1516--1520},
	organization = {Ieee}
}

@article{huang2020self,
	author       = {Huang, Lang and Zhang, Chao and Zhang, Hongyang},
	year         = 2020,
	title        = {{Self-adaptive training: beyond empirical risk minimization}},
	journal      = {Advances in neural information processing systems},
	volume       = 33,
	pages        = {19365--19376}
}

@article{huang2022layoutlmv3,
	author       = {Huang, Yupan and Lv, Tengchao and Cui, Lei and Lu, Yutong and Wei, Furu},
	year         = 2022,
	title        = {{LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking}},
	journal      = {ACM International Conference on Multimedia},
	booktitle    = {{Proceedings of the 30th ACM International Conference on Multimedia}},
	location     = {Lisboa, Portugal},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {MM '22},
	pages        = {4083--4091},
	doi          = {10.1145/3503161.3548112},
	isbn         = 9781450392037,
	url          = {https://doi.org/10.1145/3503161.3548112},
	numpages     = 9,
	keywords     = {document AI, multimodal pre-training, LayoutLM, vision-and-language}
}

@inproceedings{hudson2019gqa,
	author       = {Hudson, Drew A and Manning, Christopher D},
	year         = 2019,
	title        = {{Gqa: A new dataset for real-world visual reasoning and compositional question answering}},
	booktitle    = {{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}},
	pages        = {6700--6709}
}

@article{hullermeier2019aleatoric,
	author       = {H{\"u}llermeier, Eyke and Waegeman, Willem},
	year         = 2019,
	title        = {{Aleatoric and epistemic uncertainty in machine learning: A tutorial introduction}},
	journal      = {arXiv preprint arXiv:1910.09457}
}

@techreport{IEEE27552017,
	year         = 2017,
	title        = {{IEEE Guide for Terms and Concepts in Intelligent Process Automation}},
	number       = {IEEE Std 2755-2017},
	pages        = {1--16},
	institution  = {Ieee}
}

@inproceedings{ijcai2020p0501,
	author       = {Liu, Jian and Cui, Leyang and Liu, Hanmeng and Huang, Dandan and Wang, Yile and Zhang, Yue},
	year         = 2020,
	month        = 7,
	title        = {{LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning}},
	booktitle    = {{Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20}},
	publisher    = {International Joint Conferences on Artificial Intelligence Organization},
	pages        = {3622--3628},
	doi          = {10.24963/ijcai.2020/501},
	url          = {https://doi.org/10.24963/ijcai.2020/501},
	note         = {Main track},
	editor       = {Christian Bessiere}
}

@article{ivakhnenko1966cybernetic,
	author       = {Ivakhnenko, Alekse{\u\i} Grigorevich and Lapa, Valentin Grigorevich},
	year         = 1966,
	title        = {{Cybernetic predicting devices}},
	publisher    = {Joint Publications Research Service}
}

@article{izmailov2021bayesian,
	author       = {Izmailov, Pavel and Vikram, Sharad and Hoffman, Matthew D and Wilson, Andrew Gordon},
	year         = 2021,
	title        = {{What Are Bayesian Neural Network Posteriors Really Like?}},
	journal      = {arXiv preprint arXiv:2104.14421}
}

@inproceedings{jaeger2023a,
	author       = {Paul F Jaeger and Carsten Tim L{\"u}th and Lukas Klein and Till J. Bungert},
	year         = 2023,
	title        = {{A Call to Reflect on Evaluation Practices for Failure Detection in Image Classification}},
	booktitle    = {{International Conference on Learning Representations}},
	url          = {https://openreview.net/forum?id=YnkGMIh0gvX}
}

@inproceedings{jagannatha2020calibrating,
	author       = {Jagannatha, Abhyuday and Yu, Hong},
	year         = 2020,
	title        = {{Calibrating Structured Output Predictors for Natural Language Processing}},
	booktitle    = {{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}},
	volume       = 2020,
	pages        = {2078--2092},
	organization = {NIH Public Access}
}

@inproceedings{jain2019multimodal,
	author       = {Jain, Rajiv and Wigington, Curtis},
	year         = 2019,
	title        = {{Multimodal document image classification}},
	booktitle    = {{2019 International Conference on Document Analysis and Recognition (ICDAR)}},
	pages        = {71--77},
	organization = {Ieee}
}

@inproceedings{jain2020maximizing,
	author       = {Jain, Siddhartha and Liu, Ge and Mueller, Jonas and Gifford, David},
	year         = 2020,
	title        = {{Maximizing Overall Diversity for Improved Uncertainty Estimates in Deep Ensembles.}},
	booktitle    = {{The Thirty-Fourth AAAI Conference on Artificial Intelligence}},
	pages        = {4264--4271}
}

%% Datasets
@inproceedings{jaume2019,
	author       = {Guillaume Jaume, Hazim Kemal Ekenel, Jean-Philippe Thiran},
	year         = 2019,
	title        = {{FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents}},
	booktitle    = {{Accepted to ICDAR-OST}}
}

@inproceedings{jaume2019funsd,
	author       = {Jaume, Guillaume and Ekenel, Hazim Kemal and Thiran, Jean-Philippe},
	year         = 2019,
	title        = {{Funsd: A dataset for form understanding in noisy scanned documents}},
	booktitle    = {{2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)}},
	volume       = 2,
	pages        = {1--6},
	organization = {Ieee}
}

@article{jaynes1957information,
	author       = {Jaynes, Edwin T},
	year         = 1957,
	title        = {{Information theory and statistical mechanics}},
	journal      = {Physical review},
	publisher    = {Aps},
	volume       = 106,
	number       = 4,
	pages        = 620
}

@article{ji2023survey,
	author       = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
	year         = 2023,
	title        = {{Survey of hallucination in natural language generation}},
	journal      = {ACM Computing Surveys},
	publisher    = {ACM New York, NY},
	volume       = 55,
	number       = 12,
	pages        = {1--38}
}

@article{jiang2005confidence,
	author       = {Jiang, Hui},
	year         = 2005,
	title        = {{Confidence measures for speech recognition: A survey}},
	journal      = {Speech communication},
	publisher    = {Elsevier},
	volume       = 45,
	number       = 4,
	pages        = {455--470}
}

@article{jiang2018trust,
	author       = {Jiang, Heinrich and Kim, Been and Guan, Melody and Gupta, Maya},
	year         = 2018,
	title        = {{To trust or not to trust a classifier}},
	journal      = {Advances in neural information processing systems},
	volume       = 31
}

@article{jiang2020can,
	author       = {Jiang, Zhengbao and Xu, Frank F and Araki, Jun and Neubig, Graham},
	year         = 2020,
	title        = {{How can we know what language models know?}},
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press},
	volume       = 8,
	pages        = {423--438}
}

@article{jiang2023mistral,
	author       = {Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
	year         = 2023,
	title        = {{Mistral 7B}},
	journal      = {arXiv preprint arXiv:2310.06825}
}

@inproceedings{jimeno2021icdar,
	author       = {Jimeno Yepes, Antonio and Zhong, Peter and Burdick, Douglas},
	year         = 2021,
	title        = {{ICDAR 2021 competition on scientific literature parsing}},
	booktitle    = {{Document Analysis and Recognition--ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5--10, 2021, Proceedings, Part IV 16}},
	pages        = {605--617},
	organization = {Springer}
}

@inproceedings{jin-etal-2019-pubmedqa,
	author       = {Jin, Qiao  and Dhingra, Bhuwan  and Liu, Zhengping  and Cohen, William  and Lu, Xinghua},
	year         = 2019,
	month        = nov,
	title        = {{PubMedQA: A Dataset for Biomedical Research Question Answering}},
	booktitle    = {{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}},
	publisher    = {Association for Computational Linguistics},
	address      = {Hong Kong, China},
	pages        = {2567--2577},
	doi          = {10.18653/v1/D19-1259},
	url          = {https://aclanthology.org/D19-1259},
	abstract     = {We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1{\%} accuracy, compared to single human performance of 78.0{\%} accuracy and majority-baseline of 55.2{\%} accuracy, leaving much room for improvement. PubMedQA is publicly available at https://pubmedqa.github.io.}
}

@article{JMLR:v22:20-451,
	author       = {Rémi Flamary and Nicolas Courty and Alexandre Gramfort and Mokhtar Z. Alaya and Aurélie Boisbunon and Stanislas Chambon and Laetitia Chapel and Adrien Corenflos and Kilian Fatras and Nemo Fournier and Lao Gautheron and Nathalie T.H. Gayraud and Hicham Janati and Alain Rakotomamonjy and Ievgen Redko and Antoine Rolet and Antony Schutz and Vivien Seguy and Danica J. Sutherland and Romain Tavenard and Alexander Tong and Titouan Vayer},
	year         = 2021,
	title        = {{POT: Python Optimal Transport}},
	journal      = {Journal of Machine Learning Research},
	volume       = 22,
	number       = 78,
	pages        = {1--8},
	url          = {http://jmlr.org/papers/v22/20-451.html}
}

@inproceedings{johansen2017learning,
	author       = {Johansen, Alexander and Socher, Richard},
	year         = 2017,
	title        = {{Learning when to skim and when to read}},
	booktitle    = {{Proceedings of the 2nd Workshop on Representation Learning for NLP}},
	pages        = {257--264}
}

@inproceedings{joshi2012multi,
	author       = {Joshi, Mahesh and Dredze, Mark and Cohen, William and Rose, Carolyn},
	year         = 2012,
	title        = {{Multi-domain learning: when do domains matter?}},
	booktitle    = {{Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning}},
	pages        = {1302--1312}
}

@article{joshi2020spanbert,
	author       = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer},
	year         = 2020,
	title        = {{Spanbert: Improving pre-training by representing and predicting spans}},
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press},
	volume       = 8,
	pages        = {64--77}
}

@article{jospin2020hands,
	author       = {Jospin, Laurent Valentin and Buntine, Wray and Boussaid, Farid and Laga, Hamid and Bennamoun, Mohammed},
	year         = 2020,
	title        = {{Hands-on Bayesian Neural Networks--a Tutorial for Deep Learning Users}},
	journal      = {arXiv preprint arXiv:2007.06823}
}

@article{jvanlandeghem_uncertaintybench,
	author       = {Van Landeghem, Jordy and Blaschko, Matthew and Anckaert, Bertrand and Moens, Marie-Francine},
	year         = 2022,
	title        = {{Benchmarking Scalable Predictive Uncertainty in Text Classification}},
	journal      = {IEEE Access},
	volume       = 10,
	pages        = {1--35},
	doi          = {10.1109/access.2022.3168734}
}

@inproceedings{kacupaj-etal-2021-conversational,
	author       = {Kacupaj, Endri  and Plepi, Joan  and Singh, Kuldeep  and Thakkar, Harsh  and Lehmann, Jens  and Maleshkova, Maria},
	year         = 2021,
	month        = apr,
	title        = {{Conversational Question Answering over Knowledge Graphs with Transformer and Graph Attention Networks}},
	booktitle    = {{Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume}},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {850--862},
	doi          = {10.18653/v1/2021.eacl-main.72},
	url          = {https://aclanthology.org/2021.eacl-main.72},
	abstract     = {This paper addresses the task of (complex) conversational question answering over a knowledge graph. For this task, we propose LASAGNE (muLti-task semAntic parSing with trAnsformer and Graph atteNtion nEworks). It is the first approach, which employs a transformer architecture extended with Graph Attention Networks for multi-task neural semantic parsing. LASAGNE uses a transformer model for generating the base logical forms, while the Graph Attention model is used to exploit correlations between (entity) types and predicates to produce node representations. LASAGNE also includes a novel entity recognition module which detects, links, and ranks all relevant entities in the question context. We evaluate LASAGNE on a standard dataset for complex sequential question answering, on which it outperforms existing baselines averaged on all question types. Specifically, we show that LASAGNE improves the F1-score on eight out of ten question types; in some cases, the increase is more than 20{\%} compared to state of the art (SotA).}
}

@article{kadavath2022language,
	author       = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Dodds, Zac Hatfield and DasSarma, Nova and Tran-Johnson, Eli and others},
	year         = 2022,
	title        = {{Language models (mostly) know what they know}},
	journal      = {arXiv preprint arXiv:2207.05221}
}

@inproceedings{kamath:hal-01759306,
	author       = {Kamath, Sanjay and Grau, Brigitte and Ma, Yue},
	year         = 2018,
	month        = Apr,
	title        = {{Verification of the Expected Answer Type for Biomedical Question Answering}},
	booktitle    = {{First International Workshop on Hybrid Question Answering with Structured and Unstructured Knowledge (HQA'18)}},
	publisher    = {ACM Press},
	address      = {Lyon, France},
	series       = {WWW '18 Companion Proceedings of the The Web Conference 2018},
	pages        = {1093--1097},
	doi          = {10.1145/3184558.3191542},
	url          = {https://hal.science/hal-01759306},
	pdf          = {https://hal.science/hal-01759306/file/p1093-kamath.pdf},
	hal_id       = {hal-01759306},
	hal_version  = {v1}
}

@inproceedings{kamath2020selective,
	author       = {Kamath, Amita and Jia, Robin and Liang, Percy},
	year         = 2020,
	title        = {{Selective Question Answering under Domain Shift}},
	booktitle    = {{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}},
	pages        = {5684--5696}
}

@article{kanchi2022emmdocclassifier,
	author       = {Kanchi, Shrinidhi and Pagani, Alain and Mokayed, Hamam and Liwicki, Marcus and Stricker, Didier and Afzal, Muhammad Zeshan},
	year         = 2022,
	title        = {{EmmDocClassifier: Efficient Multimodal Document Image Classifier for Scarce Data}},
	journal      = {Applied Sciences},
	publisher    = {Mdpi},
	volume       = 12,
	number       = 3,
	pages        = 1457
}

@inproceedings{kang2014convolutional,
	author       = {Kang, Le and Kumar, Jayant and Ye, Peng and Li, Yi and Doermann, David},
	year         = 2014,
	title        = {{Convolutional neural networks for document image classification}},
	booktitle    = {{2014 22nd international conference on pattern recognition}},
	pages        = {3168--3172},
	organization = {Ieee}
}

@inproceedings{kang2020improved,
	author       = {Kang, Daniel and Hashimoto, Tatsunori B},
	year         = 2020,
	title        = {{Improved Natural Language Generation via Loss Truncation}},
	booktitle    = {{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}},
	pages        = {718--731}
}

@inproceedings{kar2014online,
	author       = {Kar, Purushottam and Narasimhan, Harikrishna and Jain, Prateek},
	year         = 2014,
	title        = {{Online and stochastic gradient methods for non-decomposable loss functions}},
	booktitle    = {{Proceedings of the 27th International Conference on Neural Information Processing Systems-Volume 1}},
	pages        = {694--702}
}

@article{karandikar2021soft,
	author       = {Karandikar, Archit and Cain, Nicholas and Tran, Dustin and Lakshminarayanan, Balaji and Shlens, Jonathon and Mozer, Michael C and Roelofs, Rebecca},
	year         = 2021,
	title        = {{Soft Calibration Objectives for Neural Networks}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34
}

@article{karpas2017information,
	author       = {Karpas, Ehud D and Shklarsh, Adi and Schneidman, Elad},
	year         = 2017,
	title        = {{Information socialtaxis and efficient collective behavior emerging in groups of information-seeking agents}},
	journal      = {Proceedings of the National Academy of Sciences},
	publisher    = {National Acad Sciences},
	volume       = 114,
	number       = 22,
	pages        = {5589--5594}
}

@phdthesis{kassel1995comparison,
	author       = {Kassel, Robert H},
	year         = 1995,
	title        = {{A comparison of approaches to on-line handwritten character recognition}},
	school       = {Massachusetts Institute of Technology}
}

@article{katti2018chargrid,
	author       = {Katti, Anoop Raveendra and Reisswig, Christian and Guder, Cordula and Brarda, Sebastian and Bickel, Steffen and H{\"o}hne, Johannes and Faddoul, Jean Baptiste},
	year         = 2018,
	title        = {{Chargrid: Towards understanding 2d documents}},
	journal      = {arXiv preprint arXiv:1809.08799}
}

@article{kendall2015bayesian,
	author       = {Kendall, Alex and Badrinarayanan, Vijay and Cipolla, Roberto},
	year         = 2015,
	title        = {{Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding}},
	journal      = {arXiv preprint arXiv:1511.02680}
}

@inproceedings{kendall2017uncertainties,
	author       = {Kendall, Alex and Gal, Yarin},
	year         = 2017,
	title        = {{What uncertainties do we need in Bayesian deep learning for computer vision?}},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	pages        = {5574--5584}
}

@article{khan2023bayesian,
	author       = {Khan, Mohammad Emtiyaz and Rue, H{\aa}vard},
	year         = 2023,
	title        = {{The Bayesian learning rule}},
	journal      = {Journal of Machine Learning Research},
	volume       = 24,
	number       = 281,
	pages        = {1--46}
}

@inproceedings{kim_convolutional_2014,
	author       = {Kim, Yoon},
	year         = 2014,
	month        = oct,
	title        = {{Convolutional Neural Networks for Sentence Classification}},
	journal      = {arXiv preprint arXiv:1408.5882},
	booktitle    = {{Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing}},
	publisher    = {Association for Computational Linguistics},
	address      = {Doha, Qatar},
	pages        = {1746--1751},
	doi          = {10.3115/v1/D14-1181}
}

@article{kim2021comparing,
	author       = {Kim, Taehyeon and Oh, Jaehoon and Kim, NakYil and Cho, Sangwook and Yun, Se-Young},
	year         = 2021,
	title        = {{Comparing kullback-leibler divergence and mean squared error loss in knowledge distillation}},
	journal      = {arXiv preprint arXiv:2105.08919}
}

@inproceedings{kim2021donut,
	author       = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
	year         = 2022,
	title        = {{Ocr-free document understanding transformer}},
	booktitle    = {{European Conference on Computer Vision}},
	pages        = {498--517},
	organization = {Springer}
}

@inproceedings{kim2023web,
	author       = {Kim, Donghyun and Hong, Teakgyu and Yim, Moonbin and Kim, Yoonsik and Kim, Geewook},
	year         = 2023,
	title        = {{On Web-based Visual Corpus Construction for Visual Document Understanding}},
	booktitle    = {{International Conference on Document Analysis and Recognition}},
	pages        = {297--313},
	organization = {Springer}
}

@article{kimball1973seven,
	author       = {Kimball, John},
	year         = 1973,
	title        = {{Seven principles of surface structure parsing in natural language}},
	journal      = {Cognition},
	publisher    = {Elsevier},
	volume       = 2,
	number       = 1,
	pages        = {15--47}
}

@article{kiureghian_aleatory_2009,
	author       = {Kiureghian, Armen Der and Ditlevsen, Ove},
	year         = 2009,
	month        = mar,
	title        = {{Aleatory or epistemic? Does it matter?}},
	shorttitle   = {Aleatory or epistemic?},
	journal      = {Structural Safety},
	series       = {Risk {Acceptance} and {Risk} {Communication}},
	volume       = 31,
	number       = 2,
	pages        = {105--112},
	doi          = {10.1016/j.strusafe.2008.06.020},
	issn         = {0167-4730},
	url          = {http://www.sciencedirect.com/science/article/pii/S0167473008000556},
	urldate      = {2020-04-28},
	abstract     = {The sources and characters of uncertainties in engineering modeling for risk and reliability analyses are discussed. While many sources of uncertainty may exist, they are generally categorized as either aleatory or epistemic. Uncertainties are characterized as epistemic, if the modeler sees a possibility to reduce them by gathering more data or by refining models. Uncertainties are categorized as aleatory if the modeler does not foresee the possibility of reducing them. From a pragmatic standpoint, it is useful to thus categorize the uncertainties within a model, since it then becomes clear as to which uncertainties have the potential of being reduced. More importantly, epistemic uncertainties may introduce dependence among random events, which may not be properly noted if the character of uncertainties is not correctly modeled. Influences of the two types of uncertainties in reliability assessment, codified design, performance-based engineering and risk-based decision-making are discussed. Two simple examples demonstrate the influence of statistical dependence arising from epistemic uncertainties on systems and time-variant reliability problems.},
	language     = {en},
	keywords     = {Aleatory, Epistemic, Ergodicity, Parameter uncertainty, Predictive models, Probability distribution choice, Statistical dependence, Systems, Time-variant reliability, Uncertainty},
	file         = {ScienceDirect Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/H57BRBD6/S0167473008000556.html:text/html}
}

@inproceedings{klein-manning-2001-parsing,
	author       = {Klein, Dan  and Manning, Christopher D.},
	year         = 2001,
	month        = oct,
	title        = {{Parsing and Hypergraphs}},
	booktitle    = {{Proceedings of the Seventh International Workshop on Parsing Technologies}},
	address      = {Beijing, China},
	pages        = {123--134},
	url          = {https://aclanthology.org/W01-1812}
}

@inproceedings{kleisterStanislawekGWLK21,
	author       = {Tomasz Stanislawek and Filip Gralinski and Anna Wr{\'{o}}blewska and Dawid Lipinski and Agnieszka Kaliska and Paulina Rosalska and Bartosz Topolski and Przemyslaw Biecek},
	year         = 2021,
	title        = {{Kleister: Key Information Extraction Datasets Involving Long Documents with Complex Layouts}},
	booktitle    = {{Icdar}},
	publisher    = {Springer},
	series       = {Lecture Notes in Computer Science},
	volume       = 12821,
	pages        = {564--579},
	doi          = {10.1007/978-3-030-86549-8\_36}
}

@inproceedings{knoll2020belief,
	author       = {Knoll, Christian and Pernkopf, Franz},
	year         = 2020,
	title        = {{Belief Propagation: Accurate Marginals or Accurate Partition Function--Where is the Difference?}},
	booktitle    = {{Uncertainty in Artificial Intelligence}},
	pages        = {627--636},
	organization = {Pmlr}
}

@inproceedings{koh2021wilds,
	author       = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and others},
	year         = 2021,
	title        = {{Wilds: A benchmark of in-the-wild distribution shifts}},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {5637--5664},
	organization = {Pmlr}
}

@book{koller2009probabilistic,
	author       = {Koller, Daphne and Friedman, Nir},
	year         = 2009,
	title        = {{Probabilistic graphical models: principles and techniques}},
	publisher    = {MIT press}
}

@inproceedings{komodakis2017paying,
	author       = {Komodakis, Nikos and Zagoruyko, Sergey},
	year         = 2017,
	title        = {{Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer}},
	booktitle    = {{Iclr}}
}

@inproceedings{kong-etal-2020-calibrated,
	author       = {Kong, Lingkai  and Jiang, Haoming  and Zhuang, Yuchen  and Lyu, Jie  and Zhao, Tuo  and Zhang, Chao},
	year         = 2020,
	month        = nov,
	title        = {{Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution Data}},
	booktitle    = {{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {1326--1340},
	doi          = {10.18653/v1/2020.emnlp-main.102},
	url          = {https://aclanthology.org/2020.emnlp-main.102}
}

@inproceedings{kool2019stochastic,
	author       = {Wouter Kool and Herke van Hoof and Max Welling},
	year         = 2019,
	title        = {{Stochastic Beams and Where to Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement}},
	booktitle    = {{International Conference on Machine Learning}}
}

@article{kool2020estimating,
	author       = {Kool, Wouter and van Hoof, Herke and Welling, Max},
	year         = 2020,
	title        = {{Estimating gradients for discrete random variables by sampling without replacement}},
	journal      = {arXiv preprint arXiv:2002.06043}
}

@article{kooletalJMLR,
	author       = {Wouter Kool and Herke van Hoof and Max Welling},
	year         = 2020,
	title        = {{Ancestral Gumbel-Top-k Sampling for Sampling Without Replacement}},
	journal      = {Journal of Machine Learning Research},
	volume       = 21,
	number       = 47,
	pages        = {1--36},
	url          = {http://jmlr.org/papers/v21/19-985.html}
}

@article{kranzlein2021making,
	author       = {Kranzlein, Michael and Liu, Nelson F and Schneider, Nathan},
	year         = 2021,
	title        = {{Making Heads and Tails of Models with Marginal Calibration for Sparse Tagsets}},
	journal      = {arXiv preprint arXiv:2109.07494}
}

@article{krizhevsky2012imagenet,
	author       = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year         = 2012,
	title        = {{Imagenet classification with deep convolutional neural networks}},
	journal      = {Advances in neural information processing systems},
	volume       = 25
}

@incollection{krogh_simple_1992,
	author       = {Krogh, Anders and Hertz, John A.},
	year         = 1992,
	title        = {{A Simple Weight Decay Can Improve Generalization}},
	booktitle    = {{Advances in Neural Information Processing Systems 4}},
	publisher    = {Morgan-Kaufmann},
	pages        = {950--957},
	urldate      = {2020-04-23},
	editor       = {Moody, J. E. and Hanson, S. J. and Lippmann, R. P.},
	file         = {NIPS Full Text PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/FWC7FQR3/Krogh and Hertz - 1992 - A Simple Weight Decay Can Improve Generalization.pdf:application/pdf;NIPS Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/CE4RA2IT/563-a-simple-weight-decay-can-improve-generalization.html:text/html}
}

@inproceedings{krogh1995diversity,
	author       = {Krogh, Anders and Vedelsby, Jesper},
	year         = 1995,
	title        = {{Neural Network Ensembles, Cross Validation, and Active Learning}},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	publisher    = {MIT Press},
	volume       = 7,
	editor       = {G. Tesauro and D. Touretzky and T. Leen}
}

@article{kuhn2023semantic,
	author       = {Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian},
	year         = 2023,
	title        = {{Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation}},
	journal      = {arXiv preprint arXiv:2302.09664}
}

@article{kuleshov2015calibrated,
	author       = {Kuleshov, Volodymyr and Liang, Percy S},
	year         = 2015,
	title        = {{Calibrated structured prediction}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 28,
	pages        = {3474--3482}
}

@inproceedings{kuleshov2018accurate,
	author       = {Kuleshov, Volodymyr and Fenner, Nathan and Ermon, Stefano},
	year         = 2018,
	title        = {{Accurate uncertainties for deep learning using calibrated regression}},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {2796--2804},
	organization = {Pmlr}
}

@inproceedings{kull2019beyond,
	author       = {Kull, Meelis and Nieto, Miquel Perello and K{\"a}ngsepp, Markus and Silva Filho, Telmo and Song, Hao and Flach, Peter},
	year         = 2019,
	title        = {{Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration}},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	pages        = {12316--12326}
}

@inproceedings{kumar2013unsupervised,
	author       = {Kumar, Jayant and Doermann, David},
	year         = 2013,
	title        = {{Unsupervised classification of structurally similar document images}},
	booktitle    = {{2013 12th International Conference on Document Analysis and Recognition}},
	pages        = {1225--1229},
	organization = {Ieee}
}

@article{kumar2014structural,
	author       = {Kumar, Jayant and Ye, Peng and Doermann, David},
	year         = 2014,
	title        = {{Structural similarity for document image classification and retrieval}},
	journal      = {Pattern Recognition Letters},
	publisher    = {Elsevier},
	volume       = 43,
	pages        = {119--126}
}

@inproceedings{kumar2018trainable,
	author       = {Kumar, Aviral and Sarawagi, Sunita and Jain, Ujjwal},
	year         = 2018,
	title        = {{Trainable calibration measures for neural networks from kernel mean embeddings}},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {2805--2814},
	organization = {Pmlr}
}

@inproceedings{kumar2019calibration,
	author       = {Ananya Kumar and Percy Liang and Tengyu Ma},
	year         = 2019,
	title        = {{Verified Uncertainty Calibration}},
	booktitle    = {{Advances in Neural Information Processing Systems}}
}

@article{kumarNMT,
	author       = {Aviral Kumar and Sunita Sarawagi},
	year         = 2019,
	title        = {{Calibration of Encoder Decoder Models for Neural Machine Translation.}},
	journal      = {CoRR},
	volume       = {abs/1903.00802},
	url          = {http://arxiv.org/abs/1903.00802},
	publtype     = {informal},
	cdate        = 1546300800000
}

@incollection{kuppers2022confidence,
	author       = {K{\"u}ppers, Fabian and Haselhoff, Anselm and Kronenberger, Jan and Schneider, Jonas},
	year         = 2022,
	title        = {{Confidence calibration for object detection and segmentation}},
	booktitle    = {{Deep Neural Networks and Data for Automated Driving: Robustness, Uncertainty Quantification, and Insights Towards Safety}},
	publisher    = {Springer International Publishing Cham},
	pages        = {225--250}
}

@article{kwiatkowski2019natural,
	author       = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
	year         = 2019,
	title        = {{Natural questions: a benchmark for question answering research}},
	journal      = {Transactions of the Association for Computational Linguistics}
}

@article{kwonuncertaintyclassification,
	author       = {Kwon, Yongchan and Won, Joong-Ho and Kim, Beom Joon and Paik, Myunghee Cho},
	year         = 2018,
	title        = {{Uncertainty quantiﬁcation using Bayesian neural networks in classiﬁcation}},
	journal      = {Computational Statistics and Data Analysis},
	publisher    = {Elsevier},
	volume       = 142,
	pages        = 13
}

%here starts the old:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@misc{labach2019survey,
	author       = {Alex Labach and Hojjat Salehinejad and Shahrokh Valaee},
	year         = 2019,
	title        = {{Survey of Dropout Methods for Deep Neural Networks}},
	eprint       = {1904.13310},
	archiveprefix = {arXiv},
	primaryclass = {cs.NE}
}

@inproceedings{labelsmooth2019,
	author       = {M\"{u}ller, Rafael and Kornblith, Simon and Hinton, Geoffrey E},
	year         = 2019,
	title        = {{When does label smoothing help?}},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	publisher    = {Curran Associates, Inc.},
	volume       = 32,
	url          = {https://proceedings.neurips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}

@inproceedings{lacoste2013block,
	author       = {Lacoste-Julien, Simon and Jaggi, Martin and Schmidt, Mark and Pletscher, Patrick},
	year         = 2013,
	title        = {{Block-coordinate Frank-Wolfe optimization for structural SVMs}},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {53--61},
	organization = {Pmlr}
}

@inproceedings{lafferty2001conditional,
	author       = {Lafferty, John D and McCallum, Andrew and Pereira, Fernando CN},
	year         = 2001,
	title        = {{Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data}},
	booktitle    = {{Proceedings of the Eighteenth International Conference on Machine Learning}},
	pages        = {282--289}
}

@inproceedings{lakomkin-etal-2018-kt,
	author       = {Lakomkin, Egor  and Magg, Sven  and Weber, Cornelius  and Wermter, Stefan},
	year         = 2018,
	month        = nov,
	title        = {{KT-Speech-Crawler: Automatic Dataset Construction for Speech Recognition from YouTube Videos}},
	booktitle    = {{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}},
	publisher    = {Association for Computational Linguistics},
	address      = {Brussels, Belgium},
	pages        = {90--95},
	doi          = {10.18653/v1/D18-2016},
	url          = {https://aclanthology.org/D18-2016},
	abstract     = {We describe KT-Speech-Crawler: an approach for automatic dataset construction for speech recognition by crawling YouTube videos. We outline several filtering and post-processing steps, which extract samples that can be used for training end-to-end neural speech recognition systems. In our experiments, we demonstrate that a single-core version of the crawler can obtain around 150 hours of transcribed speech within a day, containing an estimated 3.5{\%} word error rate in the transcriptions. Automatically collected samples contain reading and spontaneous speech recorded in various conditions including background noise and music, distant microphone recordings, and a variety of accents and reverberation. When training a deep neural network on speech recognition, we observed around 40{\%} word error rate reduction on the Wall Street Journal dataset by integrating 200 hours of the collected samples into the training set.}
}

@misc{lakshminarayanan2016simple,
	author       = {Balaji Lakshminarayanan and Alexander Pritzel and Charles Blundell},
	year         = 2016,
	title        = {{Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 30,
	pages        = {6402--6413},
	eprint       = {1612.01474},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}

@article{lang199520,
	author       = {Ken Lang},
	year         = 1995,
	title        = {{Newsweeder: Learning to filter netnews. version 20news-18828}},
	journal      = {Machine Learning Proceedings 1995},
	publisher    = {Morgan Kaufmann},
	address      = {San Francisco (CA)},
	pages        = {331--339},
	doi          = {https://doi.org/10.1016/B978-1-55860-377-6.50048-7},
	isbn         = {978-1-55860-377-6},
	url          = {http://www.sciencedirect.com/science/article/pii/B9781558603776500487}
}

@inproceedings{larson2023labelnoise,
	author       = {Larson, Stefan  and Lim, Gordon  and Leach, Kevin},
	year         = 2023,
	month        = may,
	title        = {{On Evaluation of Document Classification with RVL-CDIP}},
	booktitle    = {{Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics}},
	publisher    = {Association for Computational Linguistics},
	address      = {Dubrovnik, Croatia},
	pages        = {2665--2678},
	url          = {https://aclanthology.org/2023.eacl-main.195},
	abstract     = {The RVL-CDIP benchmark is widely used for measuring performance on the task of document classification. Despite its widespread use, we reveal several undesirable characteristics of the RVL-CDIP benchmark. These include (1) substantial amounts of label noise, which we estimate to be 8.1{\%} (ranging between 1.6{\%} to 16.9{\%} per document category); (2) presence of many ambiguous or multi-label documents; (3) a large overlap between test and train splits, which can inflate model performance metrics; and (4) presence of sensitive personally-identifiable information like US Social Security numbers (SSNs). We argue that there is a risk in using RVL-CDIP for benchmarking document classifiers, as its limited scope, presence of errors (state-of-the-art models now achieve accuracy error rates that are within our estimated label error rate), and lack of diversity make it less than ideal for benchmarking. We further advocate for the creation of a new document classification benchmark, and provide recommendations for what characteristics such a resource should include.}
}

@inproceedings{larson2019evaluation,
	author       = {Larson, Stefan  and Mahendran, Anish  and Peper, Joseph J.  and Clarke, Christopher  and Lee, Andrew  and Hill, Parker  and Kummerfeld, Jonathan K.  and Leach, Kevin  and Laurenzano, Michael A.  and Tang, Lingjia  and Mars, Jason},
	year         = 2019,
	month        = nov,
	title        = {{An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction}},
	journal      = {arXiv preprint arXiv:1909.02027},
	booktitle    = {{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP-IJCNLP)}},
	publisher    = {Association for Computational Linguistics},
	address      = {Hong Kong, China},
	pages        = {1311--1316},
	doi          = {10.18653/v1/D19-1131}
}

@inproceedings{larson2022evaluating,
	author       = {Larson, Stefan and Lim, Gordon and Ai, Yutong and Kuang, David and Leach, Kevin},
	year         = 2022,
	title        = {{Evaluating Out-of-Distribution Performance on Document Image Classifiers}},
	booktitle    = {{Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track}}
}

@article{lecun1998gradient,
	author       = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
	year         = 1998,
	title        = {{Gradient-based learning applied to document recognition}},
	journal      = {Proceedings of the IEEE},
	publisher    = {Ieee},
	volume       = 86,
	number       = 11,
	pages        = {2278--2324}
}

@inproceedings{lee2018simple,
	author       = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
	year         = 2018,
	title        = {{A simple unified framework for detecting out-of-distribution samples and adversarial attacks}},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	pages        = {7167--7177}
}

@inproceedings{lee2022formnet,
	author       = {Lee, Chen-Yu and Li, Chun-Liang and Dozat, Timothy and Perot, Vincent and Su, Guolong and Hua, Nan and Ainslie, Joshua and Wang, Renshen and Fujii, Yasuhisa and Pfister, Tomas},
	year         = 2022,
	title        = {{FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction}},
	booktitle    = {{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}},
	pages        = {3735--3754}
}

@article{lee2022pix2struct,
	author       = {Lee, Kenton and Joshi, Mandar and Turc, Iulia and Hu, Hexiang and Liu, Fangyu and Eisenschlos, Julian and Khandelwal, Urvashi and Shaw, Peter and Chang, Ming-Wei and Toutanova, Kristina},
	year         = 2022,
	title        = {{Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding}},
	journal      = {arXiv preprint arXiv:2210.03347},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {18893--18912},
	organization = {Pmlr}
}

@inproceedings{lei-etal-2018-tvqa,
	author       = {Lei, Jie  and Yu, Licheng  and Bansal, Mohit  and Berg, Tamara},
	year         = 2018,
	month        = oct # {-} # nov,
	title        = {{TVQA: Localized, Compositional Video Question Answering}},
	booktitle    = {{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}},
	publisher    = {Association for Computational Linguistics},
	address      = {Brussels, Belgium},
	pages        = {1369--1379},
	doi          = {10.18653/v1/D18-1167},
	url          = {https://aclanthology.org/D18-1167},
	abstract     = {Recent years have witnessed an increasing interest in image-based question-answering (QA) tasks. However, due to data limitations, there has been much less work on video-based QA. In this paper, we present TVQA, a large-scale video QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video. Questions are designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. We provide analyses of this new dataset as well as several baselines and a multi-stream end-to-end trainable neural network framework for the TVQA task. The dataset is publicly available at \url{http://tvqa.cs.unc.edu}.}
}

@article{lei2023boosting,
	author       = {Lei, Bin and Liao, Chunhua and Ding, Caiwen and others},
	year         = 2023,
	title        = {{Boosting logical reasoning in large language models through a new framework: The graph of thought}},
	journal      = {arXiv preprint arXiv:2308.08614}
}

@article{leibig_leveraging_2017,
	author       = {Leibig, Christian and Allken, Vaneeda and Ayhan, Murat Se{\c{c}}kin and Berens, Philipp and Wahl, Siegfried},
	year         = 2017,
	title        = {{Leveraging Uncertainty Information from Deep Neural Networks for Disease Detection}},
	journal      = {Scientific reports},
	publisher    = {Nature Publishing Group},
	volume       = 7,
	number       = 1,
	pages        = {1--14}
}

@inproceedings{lester2020constrained,
	author       = {Lester, Brian and Pressel, Daniel and Hemmeter, Amy and Choudhury, Sagnik Ray and Bangalore, Srinivas},
	year         = 2020,
	title        = {{Constrained Decoding for Computationally Efficient Named Entity Recognition Taggers}},
	booktitle    = {{EMNLP (Findings)}}
}

@inproceedings{levenshtein1966binary,
	author       = {Levenshtein, Vladimir I and others},
	year         = 1966,
	title        = {{Binary codes capable of correcting deletions, insertions, and reversals}},
	booktitle    = {{Soviet physics doklady}},
	volume       = 10,
	number       = 8,
	pages        = {707--710},
	organization = {Soviet Union}
}

@inproceedings{lewis2006building,
	author       = {Lewis, David and Agam, Gady and Argamon, Shlomo and Frieder, Ophir and Grossman, David and Heard, Jefferson},
	year         = 2006,
	title        = {{Building a test collection for complex document information processing}},
	booktitle    = {{Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval}},
	pages        = {665--666}
}

@inproceedings{li-etal-2021-mlec,
	author       = {Li, Jing  and Zhong, Shangping  and Chen, Kaizhi},
	year         = 2021,
	month        = nov,
	title        = {{MLEC-QA: A Chinese Multi-Choice Biomedical Question Answering Dataset}},
	booktitle    = {{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}},
	publisher    = {Association for Computational Linguistics},
	address      = {Online and Punta Cana, Dominican Republic},
	pages        = {8862--8874},
	doi          = {10.18653/v1/2021.emnlp-main.698},
	url          = {https://aclanthology.org/2021.emnlp-main.698},
	abstract     = {Question Answering (QA) has been successfully applied in scenarios of human-computer interaction such as chatbots and search engines. However, for the specific biomedical domain, QA systems are still immature due to expert-annotated datasets being limited by category and scale. In this paper, we present MLEC-QA, the largest-scale Chinese multi-choice biomedical QA dataset, collected from the National Medical Licensing Examination in China. The dataset is composed of five subsets with 136,236 biomedical multi-choice questions with extra materials (images or tables) annotated by human experts, and first covers the following biomedical sub-fields: Clinic, Stomatology, Public Health, Traditional Chinese Medicine, and Traditional Chinese Medicine Combined with Western Medicine. We implement eight representative control methods and open-domain QA methods as baselines. Experimental results demonstrate that even the current best model can only achieve accuracies between 40{\%} to 55{\%} on five subsets, especially performing poorly on questions that require sophisticated reasoning ability. We hope the release of the MLEC-QA dataset can serve as a valuable resource for research and evaluation in open-domain QA, and also make advances for biomedical QA systems.}
}

@article{li2017dropout,
	author       = {Li, Yingzhen and Gal, Yarin},
	year         = 2017,
	title        = {{Dropout Inference in Bayesian Neural Networks with Alpha-divergences}},
	journal      = {arXiv preprint arXiv:1703.02914}
}

@article{li2018visualizing,
	author       = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	year         = 2018,
	title        = {{Visualizing the loss landscape of neural nets}},
	journal      = {Advances in neural information processing systems},
	volume       = 31
}

@article{li2019dice,
	author       = {Li, Xiaoya and Sun, Xiaofei and Meng, Yuxian and Liang, Junjun and Wu, Fei and Li, Jiwei},
	year         = 2019,
	title        = {{Dice Loss for Data-imbalanced NLP Tasks}},
	journal      = {arXiv preprint arXiv:1911.02855}
}

@article{li2020docbank,
	author       = {Li, Minghao and Xu, Yiheng and Cui, Lei and Huang, Shaohan and Wei, Furu and Li, Zhoujun and Zhou, Ming},
	year         = 2020,
	title        = {{DocBank: A benchmark dataset for document layout analysis}},
	journal      = {arXiv preprint arXiv:2006.01038},
	eprint       = {2006.01038},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@inproceedings{li2020tablebank,
	author       = {Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming and Li, Zhoujun},
	year         = 2020,
	title        = {{Tablebank: Table benchmark for image-based table detection and recognition}},
	booktitle    = {{Proceedings of the Twelfth Language Resources and Evaluation Conference}},
	pages        = {1918--1925}
}

@article{li2021benchmarking,
	author       = {Li, Yanghao and Xie, Saining and Chen, Xinlei and Dollar, Piotr and He, Kaiming and Girshick, Ross},
	year         = 2021,
	title        = {{Benchmarking detection transfer learning with vision transformers}},
	journal      = {arXiv preprint arXiv:2111.11429}
}

@inproceedings{li2021selfdoc,
	author       = {Li, Peizhao and Gu, Jiuxiang and Kuen, Jason and Morariu, Vlad I and Zhao, Handong and Jain, Rajiv and Manjunatha, Varun and Liu, Hongfu},
	year         = 2021,
	title        = {{Selfdoc: Self-supervised document representation learning}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
	pages        = {5652--5660}
}

@inproceedings{li2022blip,
	author       = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
	year         = 2022,
	title        = {{Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation}},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {12888--12900},
	organization = {Pmlr}
}

@inproceedings{li2022dit,
	author       = {Li, Junlong and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Zhang, Cha and Wei, Furu},
	year         = 2022,
	title        = {{Dit: Self-supervised pre-training for document image transformer}},
	booktitle    = {{Proceedings of the 30th ACM International Conference on Multimedia}},
	pages        = {3530--3539}
}

@article{li2022explanations,
	author       = {Li, Shiyang and Chen, Jianshu and Shen, Yelong and Chen, Zhiyu and Zhang, Xinlu and Li, Zekun and Wang, Hong and Qian, Jing and Peng, Baolin and Mao, Yi and others},
	year         = 2022,
	title        = {{Explanations from large language models make small reasoners better}},
	journal      = {arXiv preprint arXiv:2210.06726}
}

@inproceedings{li2022exploring,
	author       = {Li, Yanghao and Mao, Hanzi and Girshick, Ross and He, Kaiming},
	year         = 2022,
	title        = {{Exploring plain vision transformer backbones for object detection}},
	booktitle    = {{European Conference on Computer Vision}},
	pages        = {280--296},
	organization = {Springer}
}

@inproceedings{li2022multispanqa,
	author       = {Li, Haonan and Tomko, Martin and Vasardani, Maria and Baldwin, Timothy},
	year         = 2022,
	title        = {{MultiSpanQA: A Dataset for Multi-Span Question Answering}},
	booktitle    = {{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}},
	pages        = {1250--1260}
}

@article{li2023blip,
	author       = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
	year         = 2023,
	title        = {{Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models}},
	journal      = {arXiv preprint arXiv:2301.12597}
}

@article{li2023defining,
	author       = {Li, Sha and Han, Chi and Yu, Pengfei and Edwards, Carl and Li, Manling and Wang, Xingyao and Fung, Yi R and Yu, Charles and Tetreault, Joel R and Hovy, Eduard H and others},
	year         = 2023,
	title        = {{Defining a new nlp playground}},
	journal      = {arXiv preprint arXiv:2310.20633}
}

@inproceedings{li2023enhancing,
	author       = {Li, Qiwei and Li, Zuchao and Cai, Xiantao and Du, Bo and Zhao, Hai},
	year         = 2023,
	title        = {{Enhancing Visually-Rich Document Understanding via Layout Structure Modeling}},
	booktitle    = {{Proceedings of the 31st ACM International Conference on Multimedia}},
	pages        = {4513--4523}
}

@inproceedings{li2023vit,
	author       = {Li, Zhikai and Gu, Qingyi},
	year         = 2023,
	title        = {{I-vit: Integer-only quantization for efficient vision transformer inference}},
	booktitle    = {{Proceedings of the IEEE/CVF International Conference on Computer Vision}},
	pages        = {17065--17075}
}

@inproceedings{liang2017enhancing,
	author       = {Shiyu Liang and Yixuan Li and R. Srikant},
	year         = 2018,
	title        = {{Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks}},
	booktitle    = {{International Conference on Learning Representations}},
	url          = {https://openreview.net/forum?id=H1VGkIxRZ}
}

@inproceedings{liang2018enhancing,
	author       = {Liang, Shiyu and Li, Yixuan and Srikant, R},
	year         = 2018,
	title        = {{Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks}},
	booktitle    = {{International Conference on Learning Representations}}
}

@inproceedings{Liao_2023_ICCV,
	author       = {Liao, Haofu and RoyChowdhury, Aruni and Li, Weijian and Bansal, Ankan and Zhang, Yuting and Tu, Zhuowen and Satzoda, Ravi Kumar and Manmatha, R. and Mahadevan, Vijay},
	year         = 2023,
	month        = {October},
	title        = {{DocTr: Document Transformer for Structured Information Extraction in Documents}},
	booktitle    = {{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}},
	pages        = {19584--19594}
}

@inproceedings{liao2023doctr,
	author       = {Liao, Haofu and RoyChowdhury, Aruni and Li, Weijian and Bansal, Ankan and Zhang, Yuting and Tu, Zhuowen and Satzoda, Ravi Kumar and Manmatha, R and Mahadevan, Vijay},
	year         = 2023,
	title        = {{DocTr: Document transformer for structured information extraction in documents}},
	booktitle    = {{Proceedings of the IEEE/CVF International Conference on Computer Vision}},
	pages        = {19584--19594}
}

@inproceedings{lin2014microsoft,
	author       = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
	year         = 2014,
	title        = {{Microsoft coco: Common objects in context}},
	booktitle    = {{Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}},
	pages        = {740--755},
	organization = {Springer}
}

@inproceedings{lin2021vibertgrid,
	author       = {Lin, Weihong and Gao, Qifang and Sun, Lei and Zhong, Zhuoyao and Hu, Kai and Ren, Qin and Huo, Qiang},
	year         = 2021,
	title        = {{ViBERTgrid: a jointly trained multi-modal 2D document representation for key information extraction from documents}},
	booktitle    = {{Document Analysis and Recognition--ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5--10, 2021, Proceedings, Part I 16}},
	pages        = {548--563},
	organization = {Springer}
}

@inproceedings{lin2022scrib,
	author       = {Lin, Zhen and Glass, Lucas and Westover, M Brandon and Xiao, Cao and Sun, Jimeng},
	year         = 2022,
	title        = {{SCRIB: set-classifier with class-specific risk bounds for blackbox models}},
	booktitle    = {{Proceedings of the AAAI Conference on Artificial Intelligence}},
	volume       = 36,
	number       = 7,
	pages        = {7497--7505}
}

@article{lin2022teaching,
	author       = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
	year         = 2022,
	title        = {{Teaching Models to Express Their Uncertainty in Words}},
	journal      = {arXiv preprint arXiv:2205.14334},
	issn         = {2835-8856},
	url          = {https://openreview.net/forum?id=8s8K2UZGTZ}
}

@article{lipton2018troubling,
	author       = {Lipton, Zachary C and Steinhardt, Jacob},
	year         = 2018,
	title        = {{Troubling trends in machine learning scholarship}},
	journal      = {arXiv preprint arXiv:1807.03341}
}

@inproceedings{liu-etal-2019-xqa,
	author       = {Liu, Jiahua  and Lin, Yankai  and Liu, Zhiyuan  and Sun, Maosong},
	year         = 2019,
	month        = jul,
	title        = {{XQA: A Cross-lingual Open-domain Question Answering Dataset}},
	booktitle    = {{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}},
	publisher    = {Association for Computational Linguistics},
	address      = {Florence, Italy},
	pages        = {2358--2368},
	doi          = {10.18653/v1/P19-1227},
	url          = {https://aclanthology.org/P19-1227},
	abstract     = {Open-domain question answering (OpenQA) aims to answer questions through text retrieval and reading comprehension. Recently, lots of neural network-based models have been proposed and achieved promising results in OpenQA. However, the success of these models relies on a massive volume of training data (usually in English), which is not available in many other languages, especially for those low-resource languages. Therefore, it is essential to investigate cross-lingual OpenQA. In this paper, we construct a novel dataset XQA for cross-lingual OpenQA research. It consists of a training set in English as well as development and test sets in eight other languages. Besides, we provide several baseline systems for cross-lingual OpenQA, including two machine translation-based methods and one zero-shot cross-lingual method (multilingual BERT). Experimental results show that the multilingual BERT model achieves the best results in almost all target languages, while the performance of cross-lingual OpenQA is still much lower than that of English. Our analysis indicates that the performance of cross-lingual OpenQA is related to not only how similar the target language and English are, but also how difficult the question set of the target language is. The XQA dataset is publicly available at http://github.com/thunlp/XQA.}
}

@inproceedings{liu-wan-2021-codeqa-question,
	author       = {Liu, Chenxiao  and Wan, Xiaojun},
	year         = 2021,
	month        = nov,
	title        = {{CodeQA: A Question Answering Dataset for Source Code Comprehension}},
	booktitle    = {{Findings of the Association for Computational Linguistics: EMNLP 2021}},
	publisher    = {Association for Computational Linguistics},
	address      = {Punta Cana, Dominican Republic},
	pages        = {2618--2632},
	doi          = {10.18653/v1/2021.findings-emnlp.223},
	url          = {https://aclanthology.org/2021.findings-emnlp.223},
	abstract     = {We propose CodeQA, a free-form question answering dataset for the purpose of source code comprehension: given a code snippet and a question, a textual answer is required to be generated. CodeQA contains a Java dataset with 119,778 question-answer pairs and a Python dataset with 70,085 question-answer pairs. To obtain natural and faithful questions and answers, we implement syntactic rules and semantic analysis to transform code comments into question-answer pairs. We present the construction process and conduct systematic analysis of our dataset. Experiment results achieved by several neural baselines on our dataset are shown and discussed. While research on question-answering and machine reading comprehension develops rapidly, few prior work has drawn attention to code question answering. This new dataset can serve as a useful research benchmark for source code comprehension.}
}

@inproceedings{liu2011bounding,
	author       = {Liu, Qiang and Ihler, Alexander T},
	year         = 2011,
	title        = {{Bounding the partition function using holder's inequality}},
	booktitle    = {{Icml}}
}

@article{liu2017hierarchical,
	author       = {Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
	year         = 2017,
	title        = {{Hierarchical representations for efficient architecture search}},
	journal      = {arXiv preprint arXiv:1711.00436}
}

@inproceedings{liu2018progressive,
	author       = {Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
	year         = 2018,
	title        = {{Progressive neural architecture search}},
	booktitle    = {{Proceedings of the European conference on computer vision (ECCV)}},
	pages        = {19--34}
}

@article{liu2018rethinking,
	author       = {Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
	year         = 2018,
	title        = {{Rethinking the value of network pruning}},
	journal      = {arXiv preprint arXiv:1810.05270}
}

@article{liu2019,
	author       = {Liu, Han and Burnap, Pete and Alorainy, Wafa and Williams, Matthew},
	year         = 2019,
	month        = {04},
	title        = {{A Fuzzy Approach to Text Classification With Two-Stage Training for Ambiguous Instances}},
	volume       = 6,
	pages        = {227--240},
	doi          = {10.1109/tcss.2019.2892037}
}

@article{liu2019deep,
	author       = {Liu, Ziyin and Wang, Zhikang and Liang, Paul Pu and Salakhutdinov, Russ R and Morency, Louis-Philippe and Ueda, Masahito},
	year         = 2019,
	title        = {{Deep gamblers: Learning to abstain with portfolio theory}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 32
}

@article{liu2020energy,
	author       = {Liu, Weitang and Wang, Xiaoyun and Owens, John and Li, Yixuan},
	year         = 2020,
	title        = {{Energy-based Out-of-distribution Detection}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {21464--21475}
}

@misc{liu2020roberta,
	author       = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
	year         = 2020,
	title        = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
	url          = {https://openreview.net/forum?id=SyxS0T4tvS}
}

@article{liu2020simple,
	author       = {Liu, Jeremiah Zhe and Lin, Zi and Padhy, Shreyas and Tran, Dustin and Bedrax-Weiss, Tania and Lakshminarayanan, Balaji},
	year         = 2020,
	title        = {{Simple and principled uncertainty estimation with deterministic deep learning via distance awareness}},
	journal      = {Neural Information Processing Systems 2020}
}

@article{liu2021devil,
	author       = {Liu, Bingyuan and Ayed, Ismail Ben and Galdran, Adrian and Dolz, Jose},
	year         = 2021,
	title        = {{The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration}},
	journal      = {arXiv preprint arXiv:2111.15430}
}

@article{liu2021document,
	author       = {Liu, Li and Wang, Zhiyu and Qiu, Taorong and Chen, Qiu and Lu, Yue and Suen, Ching Y},
	year         = 2021,
	title        = {{Document image classification: Progress over two decades}},
	journal      = {Neurocomputing},
	publisher    = {Elsevier},
	volume       = 453,
	pages        = {223--240}
}

@inproceedings{liu2021swin,
	author       = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	year         = 2021,
	title        = {{Swin transformer: Hierarchical vision transformer using shifted windows}},
	booktitle    = {{Proceedings of the IEEE/CVF international conference on computer vision}}
}

@inproceedings{liu2022swin,
	author       = {Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and others},
	year         = 2022,
	title        = {{Swin transformer v2: Scaling up capacity and resolution}},
	booktitle    = {{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}},
	pages        = {12009--12019}
}

@article{Liu2023LostIT,
	author       = {Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
	year         = 2023,
	title        = {{Lost in the Middle: How Language Models Use Long Contexts}},
	journal      = {ArXiv},
	volume       = {abs/2307.03172},
	url          = {https://api.semanticscholar.org/CorpusID:259360665}
}

@inproceedings{livathinos2021robust,
	author       = {Livathinos, Nikolaos and Berrospi, Cesar and Lysak, Maksym and Kuropiatnyk, Viktor and Nassar, Ahmed and Carvalho, Andre and Dolfi, Michele and Auer, Christoph and Dinkla, Kasper and Staar, Peter},
	year         = 2021,
	title        = {{Robust PDF document conversion using recurrent neural networks}},
	booktitle    = {{Proceedings of the AAAI Conference on Artificial Intelligence}},
	volume       = 35,
	number       = 17,
	pages        = {15137--15145}
}

@book{llados2021document,
	author       = {Llad{\'o}s, Josep and Lopresti, Daniel and Uchida, Seiichi},
	year         = 2021,
	title        = {{Document Analysis and Recognition--ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5--10, 2021, Proceedings}},
	publisher    = {Springer Nature},
	volume       = 12821
}

@article{london2016stability,
	author       = {London, Ben and Huang, Bert and Getoor, Lise},
	year         = 2016,
	title        = {{Stability and generalization in structured prediction}},
	journal      = {The Journal of Machine Learning Research},
	publisher    = {JMLR. org},
	volume       = 17,
	number       = 1,
	pages        = {7808--7859}
}

@article{long2005image,
	author       = {Long, L Rodney and Antani, Sameer K and Thoma, George R},
	year         = 2005,
	title        = {{Image informatics at a national research center}},
	journal      = {Computerized Medical Imaging and Graphics},
	publisher    = {Elsevier},
	volume       = 29,
	number       = {2-3},
	pages        = {171--193}
}

@misc{lorena2018complex,
	author       = {Ana C. Lorena and Luís P. F. Garcia and Jens Lehmann and Marcilio C. P. Souto and Tin K. Ho},
	year         = 2018,
	title        = {{How Complex is your classification problem? A survey on measuring classification complexity}},
	eprint       = {1808.03591},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

@article{loshchilov_decoupled_2019,
	author       = {Loshchilov, Ilya and Hutter, Frank},
	year         = 2017,
	title        = {{Decoupled Weight Decay Regularization}},
	journal      = {arXiv preprint arXiv:1711.05101},
	booktitle    = {{International Conference on Learning Representations}},
	url          = {https://openreview.net/forum?id=Bkg6RiCqY7}
}

@article{loshchilov2016sgdr,
	author       = {Loshchilov, Ilya and Hutter, Frank},
	year         = 2017,
	title        = {{SGDR: Stochastic gradient descent with warm restarts}},
	journal      = {International Conference on Learning Representations}
}

@misc{louizos2017multiplicative,
	author       = {Christos Louizos and Max Welling},
	year         = 2017,
	title        = {{Multiplicative Normalizing Flows for Variational Bayesian Neural Networks}},
	eprint       = {1703.01961},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}

@article{lu2018learning,
	author       = {Lu, Jie and Liu, Anjin and Dong, Fan and Gu, Feng and Gama, Joao and Zhang, Guangquan},
	year         = 2018,
	title        = {{Learning under concept drift: A review}},
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	publisher    = {Ieee},
	volume       = 31,
	number       = 12,
	pages        = {2346--2363}
}

@article{lu2021probabilistic,
	author       = {Lu, Tan and Dooms, Ann},
	year         = 2021,
	title        = {{Probabilistic homogeneity for document image segmentation}},
	journal      = {Pattern Recognition},
	publisher    = {Elsevier},
	volume       = 109,
	pages        = 107591
}

@inproceedings{luo2023geolayoutlm,
	author       = {Luo, Chuwei and Cheng, Changxu and Zheng, Qi and Yao, Cong},
	year         = 2023,
	title        = {{GeoLayoutLM: Geometric Pre-training for Visual Information Extraction}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
	pages        = {7092--7101}
}

@article{luz2023sequence,
	author       = {Luz de Araujo, Pedro H and de Almeida, Ana Paula GS and Ataides Braz, Fabricio and Correia da Silva, Nilton and de Barros Vidal, Flavio and de Campos, Teofilo E},
	year         = 2023,
	title        = {{Sequence-aware multimodal page classification of Brazilian legal documents}},
	journal      = {International Journal on Document Analysis and Recognition (IJDAR)},
	publisher    = {Springer},
	volume       = 26,
	number       = 1,
	pages        = {33--49}
}

@misc{m2017distancebased,
	author       = {Amit Mandelbaum and Daphna Weinshall},
	year         = 2017,
	title        = {{Distance-based Confidence Score for Neural Network Classifiers}},
	eprint       = {1709.09844},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}

@misc{ma2019nlpaug,
	author       = {Edward Ma},
	year         = 2019,
	title        = {{NLP Augmentation}},
	howpublished = {\url{https://github.com/makcedward/nlpaug}}
}

@article{ma2021meta,
	author       = {Xingchen Ma and Matthew B. Blaschko},
	year         = 2021,
	title        = {{Meta-cal: well-controlled post-hoc calibration by ranking}},
	journal      = {Proceedings of machine learning research (PMLR)}
}

@phdthesis{mackay1992bayesian,
	author       = {MacKay, David JC},
	year         = 1992,
	title        = {{Bayesian Methods for Adaptive Models}},
	school       = {California Institute of Technology}
}

@article{mackay1995probable,
	author       = {MacKay, David JC},
	year         = 1995,
	title        = {{Probable networks and plausible predictions—a review of practical Bayesian methods for supervised neural networks}},
	journal      = {Network: Computation in Neural Systems},
	publisher    = {Taylor \& Francis},
	volume       = 6,
	number       = 3,
	pages        = {469--505}
}

@article{maddox_simple_2019,
	author       = {Maddox, Wesley and Garipov, Timur and Izmailov, Pavel and Vetrov, Dmitry and Wilson, Andrew Gordon},
	year         = 2019,
	month        = dec,
	title        = {{A Simple Baseline for Bayesian Uncertainty in Deep Learning}},
	journal      = {arXiv:1902.02476 [cs, stat]},
	volume       = 32,
	pages        = {13153--13164},
	url          = {http://arxiv.org/abs/1902.02476},
	urldate      = {2020-01-23},
	note         = {arXiv: 1902.02476 version: 2},
	abstract     = {We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.},
	keywords     = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file         = {arXiv.org Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/KLZ2ZK8A/1902.html:text/html;arXiv Fulltext PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/VZ4NPIWS/Maddox et al. - 2019 - A Simple Baseline for Bayesian Uncertainty in Deep.pdf:application/pdf}
}

@inproceedings{maheshwari2023open,
	author       = {Maheshwari, Himanshu and Shekhar, Sumit and Saxena, Apoorv and Chhaya, Niyati},
	year         = 2023,
	title        = {{Open-World Factually Consistent Question Generation}},
	booktitle    = {{Findings of the Association for Computational Linguistics: ACL 2023}},
	pages        = {2390--2404}
}

@article{maini2022augraphy,
	author       = {Maini, Samay and Groleau, Alexander and Chee, Kok Wei and Larson, Stefan and Boarman, Jonathan},
	year         = 2022,
	title        = {{Augraphy: A data augmentation library for document images}},
	journal      = {arXiv preprint arXiv:2208.14558}
}

@incollection{malinin_predictive_2018,
	author       = {Malinin, Andrey and Gales, Mark},
	year         = 2018,
	title        = {{Predictive Uncertainty Estimation via Prior Networks}},
	booktitle    = {{Advances in Neural Information Processing Systems 31}},
	publisher    = {Curran Associates, Inc.},
	pages        = {7047--7058},
	url          = {http://papers.nips.cc/paper/7936-predictive-uncertainty-estimation-via-prior-networks.pdf},
	urldate      = {2020-05-19},
	editor       = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	file         = {NIPS Full Text PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/ZLJ7DTX5/Malinin and Gales - 2018 - Predictive Uncertainty Estimation via Prior Networ.pdf:application/pdf;NIPS Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/H6BLC6AM/7936-predictive-uncertainty-estimation-via-prior-networks.html:text/html}
}

@inproceedings{malinin2019ensemble,
	author       = {Malinin, Andrey and Mlodozeniec, Bruno and Gales, Mark},
	year         = 2019,
	title        = {{Ensemble Distribution Distillation}},
	booktitle    = {{International Conference on Learning Representations}}
}

@article{malinin2020uncertainty,
	author       = {Malinin, Andrey and Gales, Mark},
	year         = 2021,
	title        = {{Uncertainty in structured prediction}},
	booktitle    = {{International Conference on Learning Representations}}
}

@inproceedings{malinin2021shifts,
	author       = {Malinin, Andrey and Band, Neil and Gal, Yarin and Gales, Mark and Ganshin, Alexander and Chesnokov, German and Noskov, Alexey and Ploskonosov, Andrey and Prokhorenkova, Liudmila and Provilkov, Ivan and others},
	year         = 2021,
	title        = {{Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks}},
	booktitle    = {{Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)}}
}

@book{manning1999foundations,
	author       = {Manning, Christopher and Schutze, Hinrich},
	year         = 1999,
	title        = {{Foundations of statistical natural language processing}},
	publisher    = {MIT press}
}

@article{marcus2018deep,
	author       = {Marcus, Gary},
	year         = 2018,
	title        = {{Deep learning: A critical appraisal}},
	journal      = {arXiv preprint arXiv:1801.00631}
}

@article{markou2003novelty,
	author       = {Markou, Markos and Singh, Sameer},
	year         = 2003,
	title        = {{Novelty detection: a review—part 1: statistical approaches}},
	journal      = {Signal processing},
	publisher    = {Elsevier},
	volume       = 83,
	number       = 12,
	pages        = {2481--2497}
}

@phdthesis{martins2012geometry,
	author       = {Martins, Andr{\'e} Filipe Torres},
	year         = 2012,
	title        = {{The geometry of constrained structured prediction: applications to inference and learning of natural language syntax}},
	school       = {Carnegie Mellon University}
}

@article{mathew2020document,
	author       = {Mathew, Minesh and Tito, Ruben and Karatzas, Dimosthenis and Manmatha, R and Jawahar, CV},
	year         = 2020,
	title        = {{Document visual question answering challenge 2020}},
	journal      = {arXiv preprint arXiv:2008.08899}
}

@inproceedings{mathew2021docvqa,
	author       = {Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
	year         = 2021,
	title        = {{Docvqa: A dataset for vqa on document images}},
	booktitle    = {{Proceedings of the IEEE/CVF winter conference on applications of computer vision}},
	pages        = {2200--2209}
}

@inproceedings{mathew2022infographicvqa,
	author       = {Mathew, Minesh and Bagal, Viraj and Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV},
	year         = 2022,
	title        = {{InfographicVQA}},
	booktitle    = {{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}},
	pages        = {1697--1706}
}

@inproceedings{mathur2023docedit,
	author       = {Mathur, Puneet and Jain, Rajiv and Gu, Jiuxiang and Dernoncourt, Franck and Manocha, Dinesh and Morariu, Vlad I},
	year         = 2023,
	title        = {{DocEdit: language-guided document editing}},
	booktitle    = {{Proceedings of the AAAI Conference on Artificial Intelligence}},
	volume       = 37,
	number       = 2,
	pages        = {1914--1922}
}

@article{matthews2018gaussian,
	author       = {Matthews, Alexander G de G and Rowland, Mark and Hron, Jiri and Turner, Richard E and Ghahramani, Zoubin},
	year         = 2018,
	title        = {{Gaussian process behaviour in wide deep neural networks}},
	journal      = {arXiv preprint arXiv:1804.11271}
}

@inproceedings{mccallum1999multi,
	author       = {McCallum, Andrew Kachites},
	year         = 1999,
	title        = {{Multi-label text classification with a mixture model trained by EM}},
	booktitle    = {{AAAI 1999 Workshop on Text Learning}},
	organization = {Citeseer}
}

@article{meanfield2020,
	author       = {Zhiyun Lu and Eugene Ie and Fei Sha},
	year         = 2020,
	title        = {{Uncertainty Estimation with Infinitesimal Jackknife, Its Distribution and Mean-Field Approximation}},
	journal      = {CoRR},
	volume       = {abs/2006.07584},
	publtype     = {informal},
	cdate        = 1577836800000
}

@article{mehta2021mobilevit,
	author       = {Mehta, Sachin and Rastegari, Mohammad},
	year         = 2021,
	title        = {{Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer}},
	journal      = {arXiv preprint arXiv:2110.02178}
}

@inproceedings{mejer2010confidence,
	author       = {Mejer, Avihai and Crammer, Koby},
	year         = 2010,
	title        = {{Confidence in structured-prediction using confidence-weighted models}},
	booktitle    = {{Proceedings of the 2010 conference on empirical methods in natural language processing}},
	pages        = {971--981}
}

@article{miok2020ban,
	author       = {Miok, Kristian and Skrlj, Blaz and Zaharie, Daniela and Robnik-Sikonja, Marko},
	year         = 2020,
	title        = {{To BAN or not to BAN: Bayesian Attention Networks for Reliable Hate Speech Detection}},
	journal      = {arXiv preprint arXiv:2007.05304}
}

@inproceedings{mirzadeh2020improved,
	author       = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
	year         = 2020,
	title        = {{Improved knowledge distillation via teacher assistant}},
	booktitle    = {{Proceedings of the AAAI conference on artificial intelligence}},
	volume       = 34,
	number       = {04},
	pages        = {5191--5198}
}

@inproceedings{mishra-etal-2022-numglue,
	author       = {Mishra, Swaroop  and Mitra, Arindam  and Varshney, Neeraj  and Sachdeva, Bhavdeep  and Clark, Peter  and Baral, Chitta  and Kalyan, Ashwin},
	year         = 2022,
	month        = may,
	title        = {{NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks}},
	booktitle    = {{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland},
	pages        = {3505--3523},
	doi          = {10.18653/v1/2022.acl-long.246},
	url          = {https://aclanthology.org/2022.acl-long.246},
	abstract     = {Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 {\%}). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 {\%} on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.}
}

@inproceedings{mishra2019ocr,
	author       = {Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
	year         = 2019,
	title        = {{Ocr-vqa: Visual question answering by reading text in images}},
	booktitle    = {{2019 international conference on document analysis and recognition (ICDAR)}},
	pages        = {947--952},
	organization = {Ieee}
}

@inproceedings{mishraICDAR19,
	author       = {Anand Mishra and Shashank Shekhar and Ajeet Kumar Singh and Anirban Chakraborty},
	year         = 2019,
	title        = {{OCR-VQA: Visual Question Answering by Reading Text in Images}},
	booktitle    = {{Icdar}}
}

@article{mitchell1997artificial,
	author       = {Mitchell, Tom M},
	year         = 1997,
	title        = {{Artificial neural networks}},
	journal      = {Machine learning},
	publisher    = {Boston, MA: McGraw-Hill},
	volume       = 45,
	number       = 81,
	pages        = 127
}

@inproceedings{mixupcalibration,
	author       = {Thulasidasan, Sunil and Chennupati, Gopinath and Bilmes, Jeff A and Bhattacharya, Tanmoy and Michalak, Sarah},
	year         = 2019,
	title        = {{On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks}},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	publisher    = {Curran Associates, Inc.},
	volume       = 32,
	url          = {https://proceedings.neurips.cc/paper/2019/file/36ad8b5f42db492827016448975cc22d-Paper.pdf},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}

@article{mizrahi2014distributed,
	author       = {Mizrahi, Yariv D and Denil, Misha and de Freitas, Nando},
	year         = 2014,
	title        = {{Distributed parameter estimation in probabilistic graphical models}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 27
}

@misc{moejko2018inhibited,
	author       = {Marcin Możejko and Mateusz Susik and Rafał Karczewski},
	year         = 2018,
	title        = {{Inhibited Softmax for Uncertainty Estimation in Neural Networks}},
	eprint       = {1810.01861},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

@inproceedings{moller-etal-2020-covid,
	author       = {M{\"o}ller, Timo  and Reina, Anthony  and Jayakumar, Raghavan  and Pietsch, Malte},
	year         = 2020,
	month        = jul,
	title        = {{COVID-QA: A Question Answering Dataset for COVID-19}},
	booktitle    = {{Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020}},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	url          = {https://aclanthology.org/2020.nlpcovid19-acl.18},
	abstract     = {We present COVID-QA, a Question Answering dataset consisting of 2,019 question/answer pairs annotated by volunteer biomedical experts on scientific articles related to COVID-19. To evaluate the dataset we compared a RoBERTa base model fine-tuned on SQuAD with the same model trained on SQuAD and our COVID-QA dataset. We found that the additional training on this domain-specific data leads to significant gains in performance. Both the trained model and the annotated dataset have been open-sourced at: https://github.com/deepset-ai/COVID-QA}
}

@inproceedings{moon2020confidence,
	author       = {Moon, Jooyoung and Kim, Jihyo and Shin, Younghak and Hwang, Sangheum},
	year         = 2020,
	title        = {{Confidence-aware learning for deep neural networks}},
	booktitle    = {{international conference on machine learning}},
	pages        = {7034--7044},
	organization = {Pmlr}
}

@book{moravec1988mind,
	author       = {Moravec, Hans},
	year         = 1988,
	title        = {{Mind children: The future of robot and human intelligence}},
	publisher    = {Harvard University Press}
}

@article{moreno2012unifying,
	author       = {Moreno-Torres, Jose G and Raeder, Troy and Alaiz-Rodr{\'\i}guez, Roc{\'\i}o and Chawla, Nitesh V and Herrera, Francisco},
	year         = 2012,
	title        = {{A unifying view on dataset shift in classification}},
	journal      = {Pattern Recognition},
	publisher    = {Elsevier},
	volume       = 45,
	number       = 1,
	pages        = {521--530}
}

@inproceedings{mou-etal-2019-discreteness,
	author       = {Mou, Lili  and Zhou, Hao  and Li, Lei},
	year         = 2019,
	month        = nov,
	title        = {{Discreteness in Neural Natural Language Processing}},
	booktitle    = {{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP-IJCNLP)}},
	publisher    = {Association for Computational Linguistics},
	address      = {Hong Kong, China},
	abstract     = {This tutorial provides a comprehensive guide to the process of discreteness in neural NLP.As a gentle start, we will briefly introduce the background of deep learning based NLP, where we point out the ubiquitous discreteness of natural language and its challenges in neural information processing. Particularly, we will focus on how such discreteness plays a role in the input space, the latent space, and the output space of a neural network. In each part, we will provide examples, discuss machine learning techniques, as well as demonstrate NLP applications.''}
}

@inproceedings{mukherjee2020uncertainty,
	author       = {Mukherjee, Subhabrata  and Hassan Awadallah, Ahmed},
	year         = 2020,
	title        = {{Uncertainty-aware Self-training for Few-shot Text Classification}},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	address      = {Online},
	url          = {https://papers.nips.cc/paper/2020/file/f23d125da1e29e34c552f448610ff25f-Paper.pdf}
}

@article{mukhoti2018importance,
	author       = {Mukhoti, Jishnu and Stenetorp, Pontus and Gal, Yarin},
	year         = 2018,
	title        = {{On the importance of strong baselines in Bayesian deep learning}},
	journal      = {arXiv preprint arXiv:1811.09385}
}

@article{mukhoti2020calibrating,
	author       = {Mukhoti, Jishnu and Kulharia, Viveka and Sanyal, Amartya and Golodetz, Stuart and Torr, Philip HS and Dokania, Puneet K},
	year         = 2020,
	title        = {{Calibrating Deep Neural Networks using Focal Loss}},
	booktitle    = {{Advances in Neural Information Processing Systems}}
}

@article{mukhoti2021deterministic,
	author       = {Mukhoti, Jishnu and Kirsch, Andreas and van Amersfoort, Joost and Torr, Philip HS and Gal, Yarin},
	year         = 2021,
	title        = {{Deterministic neural networks with appropriate inductive biases capture epistemic and aleatoric uncertainty}},
	journal      = {arXiv preprint arXiv:2102.11582}
}

@inproceedings{mukhoti2023deep,
	author       = {Mukhoti, Jishnu and Kirsch, Andreas and van Amersfoort, Joost and Torr, Philip HS and Gal, Yarin},
	year         = 2023,
	title        = {{Deep Deterministic Uncertainty: A New Simple Baseline}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
	pages        = {24384--24394}
}

@inproceedings{mungmeeprued2022tab,
	author       = {Mungmeeprued, Thisanaporn and Ma, Yuxin and Mehta, Nisarg and Lipani, Aldo},
	year         = 2022,
	title        = {{Tab this folder of documents: page stream segmentation of business documents}},
	booktitle    = {{Proceedings of the 22nd ACM Symposium on Document Engineering}},
	pages        = {1--10}
}

@inproceedings{munirtowards,
	author       = {Munir, Muhammad Akhtar and Khan, Muhammad Haris and Sarfraz, M Saquib and Ali, Mohsen},
	year         = 2022,
	title        = {{Towards Improving Calibration in Object Detection Under Domain Shift}},
	booktitle    = {{Advances in Neural Information Processing Systems}}
}

@article{murphy1970scoring,
	author       = {Allan H. Murphy and Robert L. Winkler},
	year         = 1970,
	title        = {{Scoring rules in probability assessment and evaluation}},
	journal      = {Acta Psychologica},
	volume       = 34,
	pages        = {273--286},
	doi          = {https://doi.org/10.1016/0001-6918(70)90023-5},
	issn         = {0001-6918}
}

%%%% NEW to be ADDED!
@article{murphy1973reliability,
	author       = {Allan H.  Murphy},
	year         = 1973,
	title        = {{A New Vector Partition of the Probability Score}},
	journal      = {Journal of Applied Meteorology and Climatology},
	publisher    = {American Meteorological Society},
	address      = {Boston MA, USA},
	volume       = 12,
	number       = 4,
	pages        = {595--600},
	doi          = {10.1175/1520-0450(1973)012<0595:anvpot>2.0.co;2},
	url          = {https://journals.ametsoc.org/view/journals/apme/12/4/1520-0450\%5F1973\%5F012\%5F0595\%5Fanvpot\%5F2\%5F0\%5Fco\%5F2.xml}
}

@article{murphy1977reliability,
	author       = {Murphy, Allan H and Winkler, Robert L},
	year         = 1977,
	title        = {{Reliability of subjective probability forecasts of precipitation and temperature}},
	journal      = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	publisher    = {Wiley Online Library},
	volume       = 26,
	number       = 1,
	pages        = {41--47}
}

@inproceedings{naeini2015obtaining,
	author       = {Naeini, Mahdi Pakdaman and Cooper, Gregory and Hauskrecht, Milos},
	year         = 2015,
	title        = {{Obtaining well calibrated probabilities using Bayesian binning}},
	booktitle    = {{Proceedings of the AAAI Conference on Artificial Intelligence}},
	volume       = 29,
	number       = 1
}

@article{nakkiran2020deep,
	author       = {Nakkiran, Preetum and Neyshabur, Behnam and Sedghi, Hanie},
	year         = 2020,
	title        = {{The deep bootstrap framework: Good online learners are good offline generalizers}},
	journal      = {arXiv preprint arXiv:2010.08127}
}

@inproceedings{nalisnick2018deep,
	author       = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
	year         = 2018,
	title        = {{Do Deep Generative Models Know What They Don't Know?}},
	booktitle    = {{International Conference on Learning Representations}}
}

@misc{nalisnick2018dropout,
	author       = {Eric Nalisnick and José Miguel Hernández-Lobato and Padhraic Smyth},
	year         = 2018,
	title        = {{Dropout as a Structured Shrinkage Prior}},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {4712--4722},
	eprint       = {1810.04045},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML},
	organization = {Pmlr}
}

@book{namkoong2019reliable,
	author       = {Namkoong, Hongseok},
	year         = 2019,
	title        = {{Reliable Machine Learning via Distributional Robustness}},
	publisher    = {Stanford University}
}

@inproceedings{nassar2022tableformer,
	author       = {Nassar, Ahmed and Livathinos, Nikolaos and Lysak, Maksym and Staar, Peter},
	year         = 2022,
	title        = {{Tableformer: Table structure understanding with transformers}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
	pages        = {4614--4623}
}

@incollection{neal1992bayesian,
	author       = {Neal, Radford M},
	year         = 1992,
	title        = {{Bayesian mixture modeling}},
	booktitle    = {{Maximum Entropy and Bayesian Methods}},
	publisher    = {Springer},
	pages        = {197--211}
}

@article{neal1996bayesian,
	author       = {Neal, RM},
	year         = 1996,
	title        = {{Bayesian Learning for Neural Networks}},
	journal      = {Springer New York}
}

@incollection{Nentidis_2022,
	author       = {Anastasios Nentidis and Georgios Katsimpras and Eirini Vandorou and Anastasia Krithara and Antonio Miranda-Escalada and Luis Gasco and Martin Krallinger and Georgios Paliouras},
	year         = 2022,
	title        = {{Overview of~BioASQ 2022: The Tenth BioASQ Challenge on~Large-Scale Biomedical Semantic Indexing and~Question Answering}},
	booktitle    = {{Lecture Notes in Computer Science}},
	publisher    = {Springer International Publishing},
	pages        = {337--361},
	doi          = {10.1007/978-3-031-13643-6_22}
}

@article{neubigCalQA,
	author       = {Jiang, Zhengbao and Araki, Jun and Ding, Haibo and Neubig, Graham},
	year         = 2021,
	month        = {09},
	title        = {{How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering}},
	journal      = {Transactions of the Association for Computational Linguistics},
	volume       = 9,
	pages        = {962--977},
	doi          = {10.1162/tacl_a_00407},
	issn         = {2307-387x},
	url          = {https://doi.org/10.1162/tacl\%5Fa\%5F00407},
	abstract     = {Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, “How can we know when language models know, with confidence, the answer to a particular query?” We examine this question from the point of view of calibration, the property of a probabilistic model’s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models—T5, BART, and GPT-2—and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.},
	eprint       = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00407/1962628/tacl\_a\_00407.pdf}
}

@inproceedings{neumann1991reversibility,
	author       = {Neumann, G{\"u}nter},
	year         = 1991,
	title        = {{Reversibility and modularity in natural language generation}},
	booktitle    = {{Reversible Grammar in Natural Language Processing}}
}

@inproceedings{nguyen2015deep,
	author       = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
	year         = 2015,
	title        = {{Deep neural networks are easily fooled: High confidence predictions for unrecognizable images}},
	booktitle    = {{Proceedings of the IEEE conference on computer vision and pattern recognition}},
	pages        = {427--436}
}

@article{nguyen2015posterior,
	author       = {Nguyen, Khanh and O'Connor, Brendan},
	year         = 2015,
	title        = {{Posterior calibration and exploratory analysis for natural language processing models}},
	journal      = {arXiv preprint arXiv:1508.05154},
	booktitle    = {{Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}},
	pages        = {1587--1598}
}

@article{nguyen2016ms,
	author       = {Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li},
	year         = 2016,
	title        = {{MS MARCO: A human generated machine reading comprehension dataset}},
	journal      = {choice},
	volume       = 2640,
	pages        = 660
}

@inproceedings{nguyen2021skim,
	author       = {Nguyen, Laura and Scialom, Thomas and Staiano, Jacopo and Piwowarski, Benjamin},
	year         = 2021,
	title        = {{Skim-Attention: Learning to Focus via Document Layout}},
	booktitle    = {{Findings of the Association for Computational Linguistics: EMNLP 2021}},
	pages        = {2413--2427}
}

@article{ni2019calibration,
	author       = {Ni, Chenri and Charoenphakdee, Nontawat and Honda, Junya and Sugiyama, Masashi},
	year         = 2019,
	title        = {{On the calibration of multiclass classification with rejection}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 32
}

@inproceedings{niculescu2005predicting,
	author       = {Niculescu-Mizil, Alexandru and Caruana, Rich},
	year         = 2005,
	title        = {{Predicting good probabilities with supervised learning}},
	booktitle    = {{Proceedings of the 22nd International Conference on Machine learning}},
	pages        = {625--632}
}

@inproceedings{nikas2020two,
	author       = {Nikas, Christos and Fafalios, Pavlos and Tzitzikas, Yannis},
	year         = 2020,
	title        = {{Two-stage Semantic Answer Type Prediction for Question Answering using BERT and Class-Specificity Rewarding.}},
	booktitle    = {{SMARTat ISWC}},
	pages        = {19--28}
}

@inproceedings{nikolentzos2020message,
	author       = {Nikolentzos, Giannis and Tixier, Antoine and Vazirgiannis, Michalis},
	year         = 2020,
	title        = {{Message passing attention networks for document understanding}},
	booktitle    = {{Proceedings of the AAAI Conference on Artificial Intelligence}},
	volume       = 34,
	number       = {05},
	pages        = {8544--8551}
}

@inproceedings{NIPS2012_surrogate,
	author       = {Mroueh, Youssef and Poggio, Tomaso and Rosasco, Lorenzo and Slotine, Jean-jeacques},
	year         = 2012,
	title        = {{Multiclass Learning with Simplex Coding}},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	publisher    = {Curran Associates, Inc.},
	volume       = 25,
	url          = {https://proceedings.neurips.cc/paper/2012/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf},
	editor       = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger}
}

@inproceedings{nixon2019measuring,
	author       = {Nixon, Jeremy and Dusenberry, Michael W and Zhang, Linchuan and Jerfel, Ghassen and Tran, Dustin},
	year         = 2019,
	title        = {{Measuring Calibration in Deep Learning.}},
	booktitle    = {{CVPR Workshops}},
	volume       = 2,
	number       = 7
}

@article{nobel1996histogram,
	author       = {Nobel, Andrew},
	year         = 1996,
	title        = {{Histogram regression estimation using data-dependent partitions}},
	journal      = {The Annals of Statistics},
	publisher    = {Institute of Mathematical Statistics},
	volume       = 24,
	number       = 3,
	pages        = {1084--1105}
}

@inproceedings{novikova-etal-2017-need,
	author       = {Novikova, Jekaterina  and Du{\v{s}}ek, Ond{\v{r}}ej  and Cercas Curry, Amanda  and Rieser, Verena},
	year         = 2017,
	month        = sep,
	title        = {{Why We Need New Evaluation Metrics for NLG}},
	booktitle    = {{Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}},
	publisher    = {Association for Computational Linguistics},
	address      = {Copenhagen, Denmark},
	pages        = {2241--2252},
	doi          = {10.18653/v1/D17-1238},
	url          = {https://aclanthology.org/D17-1238},
	abstract     = {The majority of NLG evaluation relies on automatic metrics, such as BLEU . In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.}
}

@article{nowak2019general,
	author       = {Nowak-Vila, Alex and Bach, Francis and Rudi, Alessandro},
	year         = 2019,
	title        = {{A general theory for structured prediction with smooth convex surrogates}},
	journal      = {arXiv preprint arXiv:1902.01958}
}

@book{nowozin2011structured,
	author       = {Nowozin, Sebastian and Lampert, Christoph H},
	year         = 2011,
	title        = {{Structured learning and prediction in computer vision}},
	publisher    = {Now publishers Inc},
	volume       = 6,
	number       = {3-4}
}

@article{nowozin2013constructing,
	author       = {Nowozin, Sebastian},
	year         = 2013,
	title        = {{Constructing composite likelihoods in general random fields}}
}

@article{o_know_2018,
	author       = {D{\"u}rr, Oliver and Murina, Elvis and Siegismund, Daniel and Tolkachev, Vasily and Steigele, Stephan and Sick, Beate},
	year         = 2018,
	title        = {{Know When You Don't Know: A Robust Deep Learning Approach in the Presence of Unknown Phenotypes}},
	journal      = {Assay and drug development technologies},
	publisher    = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
	volume       = 16,
	number       = 6,
	pages        = {343--349}
}

@article{openai2023gpt,
	author       = {OpenAI, R},
	year         = 2023,
	title        = {{GPT-4 technical report}},
	journal      = {arXiv},
	pages        = {2303--08774}
}

@inproceedings{Osband2016RiskVU,
	author       = {Ian Osband},
	year         = 2016,
	title        = {{Risk versus Uncertainty in Deep Learning : Bayes , Bootstrap and the Dangers of Dropout}},
	booktitle    = {{NIPS Workshop on Bayesian Deep Learning}},
	volume       = 192
}

@inproceedings{osband2023epistemic,
	author       = {Ian Osband and Zheng Wen and Seyed Mohammad Asghari and Vikranth Dwaracherla and Morteza Ibrahimi and Xiuyuan Lu and Benjamin Van Roy},
	year         = 2023,
	title        = {{Epistemic Neural Networks}},
	booktitle    = {{Thirty-seventh Conference on Neural Information Processing Systems}},
	url          = {https://openreview.net/forum?id=dZqcC1qCmB}
}

@inproceedings{ott2018analyzing,
	author       = {Ott, Myle and Auli, Michael and Grangier, David and Ranzato, Marc’Aurelio},
	year         = 2018,
	title        = {{Analyzing uncertainty in neural machine translation}},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {3956--3965},
	organization = {Pmlr}
}

@inproceedings{ovadia2019trust,
	author       = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, David and Nowozin, Sebastian and Dillon, Joshua and Lakshminarayanan, Balaji and Snoek, Jasper},
	year         = 2019,
	title        = {{Can you Trust your Model's Uncertainty? Evaluating Predictive Uncertainty under Dataset Shift}},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	pages        = {13991--14002}
}

@article{pal2020multi,
	author       = {Pal, Ankit and Selvakumar, Muru and Sankarasubbu, Malaikannan},
	year         = 2020,
	title        = {{Multi-Label Text Classification using Attention-based Graph Neural Network}},
	journal      = {arXiv preprint arXiv:2003.11644}
}

@article{PAMI_places,
	author       = {Bolei Zhou and Agata Lapedriza and Aditya Khosla and Aude Oliva and Antonio Torralba},
	year         = 2018,
	month        = jun,
	title        = {{Places: A 10 Million Image Database for Scene Recognition}},
	journal      = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume       = 40,
	number       = 6,
	pages        = {1452--1464},
	doi          = {10.1109/tpami.2017.2723009},
	url          = {https://doi.org/10.1109/tpami.2017.2723009}
}

@article{pampari2020unsupervised,
	author       = {Pampari, Anusri and Ermon, Stefano},
	year         = 2020,
	title        = {{Unsupervised calibration under covariate shift}},
	journal      = {arXiv preprint arXiv:2006.16405}
}

@misc{pan2021model,
	author       = {Tai-Yu Pan and Cheng Zhang and Yandong Li and Hexiang Hu and Dong Xuan and Soravit Changpinyo and Boqing Gong and Wei-Lun Chao},
	year         = 2021,
	title        = {{On Model Calibration for Long-Tailed Object Detection and Instance Segmentation}},
	eprint       = {2107.02170},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}

@article{pang2021quality,
	author       = {Pang, Richard Yuanzhe and Parrish, Alicia and Joshi, Nitish and Nangia, Nikita and Phang, Jason and Chen, Angelica and Padmakumar, Vishakh and Ma, Johnny and Thompson, Jana and He, He and others},
	year         = 2021,
	title        = {{QuALITY: Question Answering with Long Input Texts, Yes!}},
	journal      = {arXiv preprint arXiv:2112.08608}
}

@article{paolini2021structured,
	author       = {Paolini, Giovanni and Athiwaratkun, Ben and Krone, Jason and Ma, Jie and Achille, Alessandro and Anubhai, Rishita and Santos, Cicero Nogueira dos and Xiang, Bing and Soatto, Stefano},
	year         = 2021,
	title        = {{Structured prediction as translation between augmented natural languages}},
	journal      = {arXiv preprint arXiv:2101.05779}
}

@article{papamarkou2019challenges,
	author       = {Papamarkou, Theodore and Hinkle, Jacob and Young, M Todd and Womble, David},
	year         = 2019,
	title        = {{Challenges in Bayesian inference via Markov chain Monte Carlo for neural networks}},
	journal      = {arXiv preprint arXiv:1910.06539}
}

@article{papernot2018deep,
	author       = {Papernot, Nicolas and McDaniel, Patrick},
	year         = 2018,
	title        = {{Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning}},
	journal      = {arXiv preprint arXiv:1803.04765}
}

@inproceedings{pappas-etal-2020-biomrc,
	author       = {Pappas, Dimitris  and Stavropoulos, Petros  and Androutsopoulos, Ion  and McDonald, Ryan},
	year         = 2020,
	month        = jul,
	title        = {{BioMRC: A Dataset for Biomedical Machine Reading Comprehension}},
	booktitle    = {{Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing}},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {140--149},
	doi          = {10.18653/v1/2020.bionlp-1.15},
	url          = {https://aclanthology.org/2020.bionlp-1.15},
	abstract     = {We introduceBIOMRC, a large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018). Experiments show that simple heuristics do not perform well on the new dataset and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new dataset is indeed less noisy or at least that its task is more feasible. Non-expert human performance is also higher on the new dataset compared to BIOREAD, and biomedical experts perform even better. We also introduce a new BERT-based MRC model, the best version of which substantially outperforms all other methods tested, reaching or surpassing the accuracy of biomedical experts in some experiments. We make the new dataset available in three different sizes, also releasing our code, and providing a leaderboard.}
}

@inproceedings{park2019cord,
	author       = {Park, Seunghyun and Shin, Seung and Lee, Bado and Lee, Junyeop and Surh, Jaeheung and Seo, Minjoon and Lee, Hwalsuk},
	year         = 2019,
	title        = {{CORD: a consolidated receipt dataset for post-OCR parsing}},
	booktitle    = {{Workshop on Document Intelligence at NeurIPS 2019}}
}

@inproceedings{park2019relational,
	author       = {Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
	year         = 2019,
	title        = {{Relational knowledge distillation}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}}
}

@inproceedings{park2020calibrated,
	author       = {Park, Sangdon and Bastani, Osbert and Weimer, James and Lee, Insup},
	year         = 2020,
	title        = {{Calibrated prediction with covariate shift via unsupervised domain adaptation}},
	booktitle    = {{International Conference on Artificial Intelligence and Statistics}},
	pages        = {3219--3229},
	organization = {Pmlr}
}

@inproceedings{passalis2020heterogeneous,
	author       = {Passalis, Nikolaos and Tzelepi, Maria and Tefas, Anastasios},
	year         = 2020,
	title        = {{Heterogeneous knowledge distillation using information flow modeling}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
	pages        = {2339--2348}
}

@article{pasupat2015compositional,
	author       = {Pasupat, Panupong and Liang, Percy},
	year         = 2015,
	title        = {{Compositional semantic parsing on semi-structured tables}},
	journal      = {arXiv preprint arXiv:1508.00305}
}

@inproceedings{patel2022question,
	author       = {Patel, Pruthvi and Mishra, Swaroop and Parmar, Mihir and Baral, Chitta},
	year         = 2022,
	title        = {{Is a Question Decomposition Unit All We Need?}},
	booktitle    = {{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}},
	pages        = {4553--4569}
}

,
  note         = {Accessed: 2023-03-08},
  howpublished = {\url{https://spacy.io/models/en#en_core_web_lg-labels}}
}
@inproceedings{paz-argaman-tsarfaty-2019-run,
	author       = {Paz-Argaman, Tzuf  and Tsarfaty, Reut},
	year         = 2019,
	month        = nov,
	title        = {{RUN through the Streets: A New Dataset and Baseline Models for Realistic Urban Navigation}},
	booktitle    = {{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}},
	publisher    = {Association for Computational Linguistics},
	address      = {Hong Kong, China},
	pages        = {6449--6455},
	doi          = {10.18653/v1/D19-1681},
	url          = {https://aclanthology.org/D19-1681},
	abstract     = {Following navigation instructions in natural language (NL) requires a composition of language, action, and knowledge of the environment. Knowledge of the environment may be provided via visual sensors or as a symbolic world representation referred to as a map. Previous work on map-based NL navigation relied on small artificial worlds with a fixed set of entities known in advance. Here we introduce the Realistic Urban Navigation (RUN) task, aimed at interpreting NL navigation instructions based on a real, dense, urban map. Using Amazon Mechanical Turk, we collected a dataset of 2515 instructions aligned with actual routes over three regions of Manhattan. We then empirically study which aspects of a neural architecture are important for the RUN success, and empirically show that entity abstraction, attention over words and worlds, and a constantly updating world-state, significantly contribute to task accuracy.}
}

@inproceedings{pearce2018uncertainty,
	author       = {Pearce, Tim and Leibfried, Felix and Brintrup, Alexandra},
	year         = 2020,
	title        = {{Uncertainty in neural networks: Approximately Bayesian ensembling}},
	booktitle    = {{International Conference on Artificial Intelligence and Statistics}},
	pages        = {234--244},
	organization = {Pmlr}
}

@article{perchet2011internal,
	author       = {Perchet, Vianney},
	year         = 2011,
	title        = {{Internal Regret with Partial Monitoring: Calibration-Based Optimal Algorithms.}},
	journal      = {Journal of Machine Learning Research},
	volume       = 12,
	number       = 6
}

@article{pereyra2017regularizing,
	author       = {Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
	year         = 2017,
	title        = {{Regularizing neural networks by penalizing confident output distributions}},
	journal      = {ICLR Workshops}
}

@article{peters2021smoothing,
	author       = {Peters, Ben and Martins, Andr{\'e} FT},
	year         = 2021,
	title        = {{Smoothing and Shrinking the Sparse Seq2Seq Search Space}},
	journal      = {arXiv preprint arXiv:2103.10291}
}

@inproceedings{petryk2024simple,
	author       = {Petryk, Suzanne and Whitehead, Spencer and Gonzalez, Joseph E and Darrell, Trevor and Rohrbach, Anna and Rohrbach, Marcus},
	year         = 2024,
	title        = {{Simple token-level confidence improves caption correctness}},
	booktitle    = {{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}},
	pages        = {5742--5752}
}

@inproceedings{pfitzmann2022doclaynet,
	author       = {Pfitzmann, Birgit and Auer, Christoph and Dolfi, Michele and Nassar, Ahmed S and Staar, Peter},
	year         = 2022,
	title        = {{DocLayNet: A Large Human-Annotated Dataset for Document-Layout Segmentation}},
	booktitle    = {{Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}},
	pages        = {3743--3751}
}

@inproceedings{pham2018efficient,
	author       = {Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
	year         = 2018,
	title        = {{Efficient neural architecture search via parameters sharing}},
	booktitle    = {{International conference on machine learning}},
	pages        = {4095--4104},
	organization = {Pmlr}
}

@inproceedings{phuong2019distillation,
	author       = {Phuong, Mary and Lampert, Christoph H},
	year         = 2019,
	title        = {{Distillation-based training for multi-exit architectures}},
	booktitle    = {{Proceedings of the IEEE/CVF international conference on computer vision}},
	pages        = {1355--1364}
}

@inproceedings{pietruszka-etal-2022-sparsifying,
	author       = {Pietruszka, Micha{\l}  and Borchmann, {\L}ukasz  and Garncarek, {\L}ukasz},
	year         = 2022,
	month        = may,
	title        = {{Sparsifying Transformer Models with Trainable Representation Pooling}},
	booktitle    = {{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland},
	pages        = {8616--8633},
	doi          = {10.18653/v1/2022.acl-long.590},
	url          = {https://aclanthology.org/2022.acl-long.590},
	editor       = {Muresan, Smaranda  and Nakov, Preslav  and Villavicencio, Aline},
	abstract     = {We propose a novel method to sparsify attention in the Transformer model by learning to select the most-informative token representations during the training process, thus focusing on the task-specific parts of an input. A reduction of quadratic time and memory complexity to sublinear was achieved due to a robust trainable top-$k$ operator.Our experiments on a challenging long document summarization task show that even our simple baseline performs comparably to the current SOTA, and with trainable pooling we can retain its top quality, while being $1.8\times$ faster during training, $4.5\times$ faster during inference, and up to $13\times$ more computationally efficient in the decoder.}
}

@article{pilan2022text,
	author       = {Pil{\'a}n, Ildik{\'o} and Lison, Pierre and {\O}vrelid, Lilja and Papadopoulou, Anthi and S{\'a}nchez, David and Batet, Montserrat},
	year         = 2022,
	title        = {{The text anonymization benchmark (tab): A dedicated corpus and evaluation framework for text anonymization}},
	journal      = {Computational Linguistics},
	publisher    = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…},
	volume       = 48,
	number       = 4,
	pages        = {1053--1101}
}

@article{pimentel_review_2014,
	author       = {Pimentel, Marco AF and Clifton, David A and Clifton, Lei and Tarassenko, Lionel},
	year         = 2014,
	title        = {{A Review of Novelty Detection}},
	journal      = {Signal Processing},
	publisher    = {Elsevier},
	volume       = 99,
	pages        = {215--249}
}

@article{pistone1995infinite,
	author       = {Pistone, Giovanni and Sempi, Carlo},
	year         = 1995,
	title        = {{An infinite-dimensional geometric structure on the space of all the probability measures equivalent to a given one}},
	journal      = {The annals of statistics},
	publisher    = {Jstor},
	pages        = {1543--1561}
}

@article{platt1999probabilistic,
	author       = {Platt, John and others},
	year         = 1999,
	title        = {{Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods}},
	journal      = {Advances in large margin classifiers},
	publisher    = {Cambridge, MA},
	volume       = 10,
	number       = 3,
	pages        = {61--74}
}

@book{pml2Book,
	author       = {Kevin P. Murphy},
	year         = 2023,
	title        = {{Probabilistic Machine Learning: Advanced Topics}},
	publisher    = {MIT Press},
	url          = {probml.ai}
}

@inproceedings{pmlr-v74-szymanski17a,
	author       = {Szymański, Piotr and Kajdanowicz, Tomasz},
	year         = 2017,
	month        = {22 Sep},
	title        = {{A Network Perspective on Stratification of Multi-Label Data}},
	booktitle    = {{Proceedings of the First International Workshop on Learning with Imbalanced Domains: Theory and Applications}},
	publisher    = {Pmlr},
	series       = {Proceedings of Machine Learning Research},
	volume       = 74,
	pages        = {22--35},
	url          = {https://proceedings.mlr.press/v74/szyma\%C5\%84ski17a.html},
	editor       = {Luís Torgo, Paula Branco and Moniz, Nuno},
	pdf          = {http://proceedings.mlr.press/v74/szymański17a/szymański17a.pdf},
	abstract     = {We present a new approach to stratifying multi-label data for classification purposes based on the iterative stratification approach proposed by Sechidis et. al. in an ECML PKDD 2011 paper. Our method extends the iterative approach to take into account second-order relationships between labels. Obtained results are evaluated using statistical properties of obtained strata as presented by Sechidis. We also propose new statistical measures relevant to second-order quality: label pairs distribution, the percentage of label pairs without positive evidence in folds and label pair - fold pairs that have no positive evidence for the label pair. We verify the impact of new methods on classification performance of Binary Relevance, Label Powerset and a fast greedy community detection based label space partitioning classifier. The proposed approach lowers the variance of classification quality, improves label pair oriented measures and example distribution while maintaining a competitive quality in label-oriented measures. We also witness an increase in stability of network characteristics.}
}

@article{poggio1987computational,
	author       = {Poggio, Tomaso and Torre, Vincent and Koch, Christof},
	year         = 1987,
	title        = {{Computational vision and regularization theory}},
	journal      = {Readings in Computer Vision},
	publisher    = {Elsevier},
	pages        = {638--643}
}

@phdthesis{poostchimohammadabadi2019improving,
	author       = {Poostchimohammadabadi, Hanieh},
	year         = 2019,
	title        = {{Improving structured prediction for named-entity recognition}}
}

@inproceedings{popordanoska2021relationship,
	author       = {Popordanoska, Teodora and Bertels, Jeroen and Vandermeulen, Dirk and Maes, Frederik and Blaschko, Matthew B},
	year         = 2021,
	title        = {{On the Relationship Between Calibrated Predictors and Unbiased Volume Estimation}},
	booktitle    = {{International Conference on Medical Image Computing and Computer-Assisted Intervention}},
	pages        = {678--688},
	organization = {Springer}
}

@article{popordanoska2022consistent,
	author       = {Popordanoska, Teodora and Sayer, Raphael and Blaschko, Matthew},
	year         = 2022,
	title        = {{A consistent and differentiable lp canonical calibration error estimator}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 35,
	pages        = {7933--7946}
}

@inproceedings{Powalski2021GoingFB,
	author       = {Rafal Powalski and Łukasz Borchmann and Dawid Jurkiewicz and Tomasz Dwojak and Michal Pietruszka and Gabriela Pałka},
	year         = 2021,
	title        = {{Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer}},
	booktitle    = {{Icdar}}
}

@inproceedings{pradhan2012conll,
	author       = {Pradhan, Sameer and Moschitti, Alessandro and Xue, Nianwen and Uryupina, Olga and Zhang, Yuchen},
	year         = 2012,
	title        = {{CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes}},
	booktitle    = {{Joint Conference on EMNLP and CoNLL-Shared Task}},
	pages        = {1--40}
}

@article{pramanik2020towards,
	author       = {Pramanik, Subhojeet and Mujumdar, Shashank and Patel, Hima},
	year         = 2020,
	title        = {{Towards a multi-modal, multi-task learning based pre-training framework for document representation learning}},
	journal      = {arXiv preprint arXiv:2009.14457}
}

@phdthesis{Prasad2022,
	author       = {Adarsh Prasad},
	year         = 2022,
	month        = 4,
	title        = {{Towards Robust and Resilient Machine Learning}},
	doi          = {10.1184/R1/19552420.v1},
	url          = {https://kilthub.cmu.edu/articles/thesis/Towards\%5FRobust\%5Fand\%5FResilient\%5FMachine\%5FLearning/19552420}
}

@article{PRAVIN2023104060,
	author       = {Chandresh Pravin and Ivan Martino and Giuseppe Nicosia and Varun Ojha},
	year         = 2023,
	title        = {{Fragility, robustness and antifragility in deep learning}},
	journal      = {Artificial Intelligence},
	pages        = 104060,
	doi          = {https://doi.org/10.1016/j.artint.2023.104060},
	issn         = {0004-3702},
	url          = {https://www.sciencedirect.com/science/article/pii/S0004370223002060},
	keywords     = {Deep neural networks, Robustness analysis, Adversarial attacks, Parameter filtering},
	abstract     = {We propose a systematic analysis of deep neural networks (DNNs) based on a signal processing technique for network parameter removal, in the form of synaptic filters that identifies the fragility, robustness and antifragility characteristics of DNN parameters. Our proposed analysis investigates if the DNN performance is impacted negatively, invariantly, or positively on both clean and adversarially perturbed test datasets when the DNN undergoes synaptic filtering. We define three filtering scores for quantifying the fragility, robustness and antifragility characteristics of DNN parameters based on the performances for (i) clean dataset, (ii) adversarial dataset, and (iii) the difference in performances of clean and adversarial datasets. We validate the proposed systematic analysis on ResNet-18, ResNet-50, SqueezeNet-v1.1 and ShuffleNet V2 x1.0 network architectures for MNIST, CIFAR10 and Tiny ImageNet datasets. The filtering scores, for a given network architecture, identify network parameters that are invariant in characteristics across different datasets over learning epochs. Vice-versa, for a given dataset, the filtering scores identify the parameters that are invariant in characteristics across different network architectures. We show that our synaptic filtering method improves the test accuracy of ResNet and ShuffleNet models on adversarial dataset when only the robust and antifragile parameters are selectively retrained at any given epoch, thus demonstrating applications of the proposed strategy in improving model robustness.}
}

@inproceedings{press2021train,
	author       = {Press, Ofir and Smith, Noah and Lewis, Mike},
	year         = 2021,
	title        = {{Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation}},
	booktitle    = {{International Conference on Learning Representations}}
}

@misc{projectid,
	author       = {Kirsch, Andreas},
	year         = 2023,
	title        = {{Player of Jeopardy: ChatGPT Evaluation}},
	booktitle    = {{GitHub}},
	url          = {https://github.com/BlackHC/player\%5Fof\%5Fjeopardy}
}

@inproceedings{qi-etal-2022-dureadervis,
	author       = {Qi, Le  and Lv, Shangwen  and Li, Hongyu  and Liu, Jing  and Zhang, Yu  and She, Qiaoqiao  and Wu, Hua  and Wang, Haifeng  and Liu, Ting},
	year         = 2022,
	month        = may,
	title        = {{$\textrm{DuReader}_\textrm{vis}$: A Chinese Dataset for Open-domain Document Visual Question Answering}},
	booktitle    = {{Findings of the Association for Computational Linguistics: ACL 2022}},
	publisher    = {Association for Computational Linguistics},
	address      = {Dublin, Ireland},
	pages        = {1338--1351},
	doi          = {10.18653/v1/2022.findings-acl.105},
	url          = {https://aclanthology.org/2022.findings-acl.105},
	abstract     = {Open-domain question answering has been used in a wide range of applications, such as web search and enterprise search, which usually takes clean texts extracted from various formats of documents (e.g., web pages, PDFs, or Word documents) as the information source. However, designing different text extraction approaches is time-consuming and not scalable. In order to reduce human cost and improve the scalability of QA systems, we propose and study an $\textbf{Open-domain}$ $\textbf{Doc}$ument $\textbf{V}$isual $\textbf{Q}$uestion $\textbf{A}$nswering (Open-domain DocVQA) task, which requires answering questions based on a collection of document images directly instead of only document texts, utilizing layouts and visual features additionally. Towards this end, we introduce the first Chinese Open-domain DocVQA dataset called $\textrm{DuReader}_{\textrm{vis}}$, containing about 15K question-answering pairs and 158K document images from the Baidu search engine. There are three main challenges in $\textrm{DuReader}_{\textrm{vis}}$: (1) long document understanding, (2) noisy texts, and (3) multi-span answer extraction. The extensive experiments demonstrate that the dataset is challenging. Additionally, we propose a simple approach that incorporates the layout and visual features, and the experimental results show the effectiveness of the proposed approach. The dataset and code will be publicly available at https://github.com/baidu/DuReader/tree/master/DuReader-vis.}
}

@inproceedings{qian-etal-2021-structural,
	author       = {Qian, Peng  and Naseem, Tahira  and Levy, Roger  and Fernandez Astudillo, Ram{\'o}n},
	year         = 2021,
	month        = aug,
	title        = {{Structural Guidance for Transformer Language Models}},
	booktitle    = {{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {3735--3745},
	doi          = {10.18653/v1/2021.acl-long.289},
	url          = {https://aclanthology.org/2021.acl-long.289}
}

@inproceedings{qiao2021lgpma,
	author       = {Qiao, Liang and Li, Zaisheng and Cheng, Zhanzhan and Zhang, Peng and Pu, Shiliang and Niu, Yi and Ren, Wenqi and Tan, Wenming and Wu, Fei},
	year         = 2021,
	title        = {{LGPMA: Complicated table structure recognition with local and global pyramid mask alignment}},
	booktitle    = {{Document Analysis and Recognition--ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5--10, 2021, Proceedings, Part I}},
	pages        = {99--114},
	organization = {Springer}
}

@article{qin2022t5score,
	author       = {Qin, Yiwei and Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
	year         = 2022,
	title        = {{T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics}},
	journal      = {arXiv preprint arXiv:2212.05726}
}

@inproceedings{quinonero2005evaluating,
	author       = {Quinonero-Candela, Joaquin and Rasmussen, Carl Edward and Sinz, Fabian and Bousquet, Olivier and Sch{\"o}lkopf, Bernhard},
	year         = 2005,
	title        = {{Evaluating predictive uncertainty challenge}},
	booktitle    = {{Machine Learning Challenges Workshop}},
	pages        = {1--27},
	organization = {Springer}
}

@article{radford2019language,
	author       = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
	year         = 2019,
	title        = {{Language models are unsupervised multitask learners}},
	journal      = {OpenAI blog},
	volume       = 1,
	number       = 8,
	pages        = 9
}

@inproceedings{radford2021learning,
	author       = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
	year         = 2021,
	title        = {{Learning transferable visual models from natural language supervision}},
	booktitle    = {{International conference on machine learning}},
	pages        = {8748--8763},
	organization = {Pmlr}
}

@article{raffel2020exploring,
	author       = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
	year         = 2020,
	title        = {{Exploring the limits of transfer learning with a unified text-to-text transformer.}},
	journal      = {J. Mach. Learn. Res.},
	volume       = 21,
	number       = 140,
	pages        = {1--67}
}

@inproceedings{raghavan-etal-2021-emrkbqa,
	author       = {Raghavan, Preethi  and Liang, Jennifer J  and Mahajan, Diwakar  and Chandra, Rachita  and Szolovits, Peter},
	year         = 2021,
	month        = jun,
	title        = {{emrKBQA: A Clinical Knowledge-Base Question Answering Dataset}},
	booktitle    = {{Proceedings of the 20th Workshop on Biomedical Language Processing}},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {64--73},
	doi          = {10.18653/v1/2021.bionlp-1.7},
	url          = {https://aclanthology.org/2021.bionlp-1.7},
	abstract     = {We present emrKBQA, a dataset for answering physician questions from a structured patient record. It consists of questions, logical forms and answers. The questions and logical forms are generated based on real-world physician questions and are slot-filled and answered from patients in the MIMIC-III KB through a semi-automated process. This community-shared release consists of over 940000 question, logical form and answer triplets with 389 types of questions and {\textasciitilde}7.5 paraphrases per question type. We perform experiments to validate the quality of the dataset and set benchmarks for question to logical form learning that helps answer questions on this dataset.}
}

@inproceedings{rajpurkar-etal-2018-know,
	author       = {Rajpurkar, Pranav  and Jia, Robin  and Liang, Percy},
	year         = 2018,
	month        = jul,
	title        = {{Know What You Don't Know: Unanswerable Questions for SQuAD}},
	journal      = {arXiv preprint arXiv:1806.03822},
	booktitle    = {{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}},
	publisher    = {Association for Computational Linguistics},
	address      = {Melbourne, Australia},
	pages        = {784--789},
	doi          = {10.18653/v1/P18-2124},
	url          = {https://aclanthology.org/P18-2124},
	abstract     = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.}
}

@article{rajpurkar2016squad,
	author       = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	year         = 2016,
	title        = {{Squad: 100,000+ questions for machine comprehension of text}},
	journal      = {arXiv preprint arXiv:1606.05250}
}

@inproceedings{ramponi2020neural,
	author       = {Ramponi, Alan and Plank, Barbara},
	year         = 2020,
	title        = {{Neural Unsupervised Domain Adaptation in NLP—A Survey}},
	booktitle    = {{Proceedings of the 28th International Conference on Computational Linguistics}},
	pages        = {6838--6855}
}

@inproceedings{ranzato2015sequence,
	author       = {Ranzato, Marc'Aurelio and Chopra, Sumit and Auli, Michael and Zaremba, Wojciech},
	year         = 2015,
	title        = {{Sequence level training with recurrent neural networks}},
	booktitle    = {{International Conference on Learning Representations}},
	organization = {Iclr}
}

@article{raschka2018model,
	author       = {Raschka, Sebastian},
	year         = 2018,
	title        = {{Model evaluation, model selection, and algorithm selection in machine learning}},
	journal      = {arXiv preprint arXiv:1811.12808}
}

@inproceedings{rasmussen2003gaussian,
	author       = {Rasmussen, Carl Edward},
	year         = 2003,
	title        = {{Gaussian processes in machine learning}},
	booktitle    = {{Summer School on Machine Learning}},
	pages        = {63--71},
	organization = {Springer}
}

@misc{rawat2017adversarial,
	author       = {Ambrish Rawat and Martin Wistuba and Maria-Irina Nicolae},
	year         = 2017,
	title        = {{Adversarial Phenomenon in the Eyes of Bayesian Deep Learning}},
	eprint       = {1711.08244},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}

@article{reddy1977speech,
	author       = {Reddy, D Raj and others},
	year         = 1977,
	title        = {{Speech understanding systems: A summary of results of the five-year research effort}},
	journal      = {Department of Computer Science. Camegie-Mell University, Pittsburgh, PA},
	volume       = 17,
	pages        = 138
}

@inproceedings{reich2020ensemble,
	author       = {Reich, Steven and Mueller, David and Andrews, Nicholas},
	year         = 2020,
	title        = {{Ensemble Distillation for Structured Prediction: Calibrated, Accurate, Fast—Choose Three}},
	booktitle    = {{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}},
	pages        = {5583--5595}
}

@article{ren2021simple,
	author       = {Ren, Jie and Fort, Stanislav and Liu, Jeremiah and Roy, Abhijit Guha and Padhy, Shreyas and Lakshminarayanan, Balaji},
	year         = 2021,
	title        = {{A simple fix to mahalanobis distance for improving near-ood detection}},
	journal      = {arXiv preprint arXiv:2106.09022}
}

@article{ren2023self,
	author       = {Ren, Jie and Zhao, Yao and Vu, Tu and Liu, Peter J and Lakshminarayanan, Balaji},
	year         = 2023,
	title        = {{Self-Evaluation Improves Selective Generation in Large Language Models}},
	journal      = {arXiv preprint arXiv:2312.09300}
}

@inproceedings{riedel2010relaxed,
	author       = {Riedel, Sebastian and Smith, David A},
	year         = 2010,
	title        = {{Relaxed marginal inference and its application to dependency parsing}},
	booktitle    = {{Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics}},
	pages        = {760--768}
}

@misc{Rimol2021,
	author       = {M. Rimol},
	year         = 2021,
	title        = {{Gartner Forecasts Worldwide Hyperautomation-Enabling Software Market to Reach Nearly \$600 Billion by 2022}},
	note         = {[Online; accessed Feb. 19, 2022]},
	howpublished = {\url{https://www.gartner.com/en/newsroom/press-releases/2021-04-28-gartner-forecasts-worldwide-hyperautomation-enabling-softwaremarket-to-reach-nearly-600-billion-by-2022}}
}

@article{roelofs2020mitigating,
	author       = {Roelofs, Rebecca and Cain, Nicholas and Shlens, Jonathon and Mozer, Michael C},
	year         = 2020,
	title        = {{Mitigating bias in calibration error estimation}},
	journal      = {arXiv preprint arXiv:2012.08668},
	booktitle    = {{International Conference on Artificial Intelligence and Statistics}},
	pages        = {4036--4054},
	organization = {Pmlr}
}

@article{roller2004max,
	author       = {Roller, BTCGD and Taskar, C and Guestrin, D},
	year         = 2004,
	title        = {{Max-margin markov networks}},
	journal      = {Advances in neural information processing systems},
	volume       = 16,
	pages        = 25
}

@article{romero2014fitnets,
	author       = {Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
	year         = 2014,
	title        = {{Fitnets: Hints for thin deep nets}},
	journal      = {arXiv preprint arXiv:1412.6550}
}

@inproceedings{rooney2001iec61508,
	author       = {Rooney, John Peter},
	year         = 2001,
	title        = {{IEC61508: an opportunity for reliability}},
	booktitle    = {{Annual Reliability and Maintainability Symposium. 2001 Proceedings. International Symposium on Product Quality and Integrity (Cat. No. 01CH37179)}},
	pages        = {272--277},
	organization = {Ieee}
}

@article{rosenblatt1958perceptron,
	author       = {Rosenblatt, Frank},
	year         = 1958,
	title        = {{The perceptron: a probabilistic model for information storage and organization in the brain.}},
	journal      = {Psychological review},
	publisher    = {American Psychological Association},
	volume       = 65,
	number       = 6,
	pages        = 386
}

@inproceedings{rosenthal_semeval-2017,
	author       = {Rosenthal, Sara and Farra, Noura and Nakov, Preslav},
	year         = 2017,
	title        = {{SemEval-2017 Task 4: Sentiment Analysis in Twitter}},
	booktitle    = {{Proceedings of the 11th international workshop on semantic evaluation (SemEval-2017)}},
	pages        = {502--518}
}

@misc{rossum2022practicalbenchmarks,
	author       = {Skalicky, Matyas and Simsa, Stepan and Uricar, Michal and Sulc, Milan},
	year         = 2022,
	title        = {{Business Document Information Extraction: Towards Practical Benchmarks}},
	publisher    = {arXiv},
	keywords     = {Information Retrieval (cs.IR), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}

@inproceedings{roth1998learning,
	author       = {Roth, Dan},
	year         = 1998,
	title        = {{Learning to resolve natural language ambiguities: A unified approach}},
	booktitle    = {{Aaai/iaai}},
	pages        = {806--813}
}

@article{rothe2020leveraging,
	author       = {Rothe, Sascha and Narayan, Shashi and Severyn, Aliaksei},
	year         = 2020,
	title        = {{Leveraging pre-trained checkpoints for sequence generation tasks}},
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press},
	volume       = 8,
	pages        = {264--280}
}

@inproceedings{rubenstein2019practical,
	author       = {Rubenstein, Paul K and Bousquet, Olivier and Djolonga, Josip and Riquelme, Carlos and Tolstikhin, Ilya},
	year         = 2019,
	title        = {{Practical and consistent estimation of f-divergences}},
	booktitle    = {{Advances in Neural Information Processing Systems}}
}

@inproceedings{rui2018fixbatchnoise,
	author       = {Luo, Rui and Wang, Jianhong and Yang, Yaodong and WANG, Jun and Zhu, Zhanxing},
	year         = 2018,
	title        = {{Thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for Bayesian learning}},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	publisher    = {Curran Associates, Inc.},
	volume       = 31,
	editor       = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett}
}

@article{rumelhart1986learning,
	author       = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	year         = 1986,
	title        = {{Learning representations by back-propagating errors}},
	journal      = {nature},
	publisher    = {Nature Publishing Group UK London},
	volume       = 323,
	number       = 6088,
	pages        = {533--536}
}

@article{russakovsky2015imagenet,
	author       = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
	year         = 2015,
	title        = {{Imagenet large scale visual recognition challenge}},
	journal      = {International journal of computer vision},
	publisher    = {Springer},
	volume       = 115,
	pages        = {211--252}
}

@article{saad2023pdftriage,
	author       = {Saad-Falcon, Jon and Barrow, Joe and Siu, Alexa and Nenkova, Ani and Rossi, Ryan A and Dernoncourt, Franck},
	year         = 2023,
	title        = {{PDFTriage: Question Answering over Long, Structured Documents}},
	journal      = {arXiv preprint arXiv:2309.08872}
}

@inproceedings{sagawa2021extending,
	author       = {Sagawa, Shiori and Koh, Pang Wei and Lee, Tony and Gao, Irena and Xie, Sang Michael and Shen, Kendrick and Kumar, Ananya and Hu, Weihua and Yasunaga, Michihiro and Marklund, Henrik and others},
	year         = 2021,
	title        = {{Extending the WILDS Benchmark for Unsupervised Adaptation}},
	booktitle    = {{NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications}}
}

@inproceedings{sage2021data,
	author       = {Sage, Cl{\'e}ment and Douzon, Thibault and Aussem, Alex and Eglin, V{\'e}ronique and Elghazel, Haytham and Duffner, Stefan and Garcia, Christophe and Espinas, J{\'e}r{\'e}my},
	year         = 2021,
	title        = {{Data-efficient information extraction from documents with pre-trained language models}},
	booktitle    = {{Document Analysis and Recognition--ICDAR 2021 Workshops: Lausanne, Switzerland, September 5--10, 2021, Proceedings, Part II 16}},
	pages        = {455--469},
	organization = {Springer}
}

@inproceedings{sage2021dataefficient,
	author       = {Sage, C. and Douzon, T. and Aussem, A. and Eglin, V. and Elghazel, H. and Duffner, S. and Garcia, C. and Espinas, J.},
	year         = 2021,
	month        = {September},
	title        = {{Data-Efficient Information Extraction from Documents with Pre-Trained Language Models}},
	booktitle    = {{ICDAR 2021 Workshop on Document Images and Language}}
}

@article{saikh2022scienceqa,
	author       = {Saikh, Tanik and Ghosal, Tirthankar and Mittal, Amish and Ekbal, Asif and Bhattacharyya, Pushpak},
	year         = 2022,
	title        = {{ScienceQA: a novel resource for question answering on scholarly articles}},
	journal      = {International Journal on Digital Libraries},
	publisher    = {Springer},
	volume       = 23,
	number       = 3,
	pages        = {289--301}
}

@article{sainz2023gollie,
	author       = {Sainz, Oscar and Garc{\'\i}a-Ferrero, Iker and Agerri, Rodrigo and de Lacalle, Oier Lopez and Rigau, German and Agirre, Eneko},
	year         = 2023,
	title        = {{Gollie: Annotation guidelines improve zero-shot information-extraction}},
	journal      = {arXiv preprint arXiv:2310.03668}
}

@article{sakaguchi2021winogrande,
	author       = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
	year         = 2021,
	title        = {{Winogrande: An adversarial winograd schema challenge at scale}},
	journal      = {Communications of the ACM},
	publisher    = {ACM New York, NY, USA},
	volume       = 64,
	number       = 9,
	pages        = {99--106}
}

@article{samuel1959some,
	author       = {Samuel, Arthur L},
	year         = 1959,
	title        = {{Some studies in machine learning using the game of checkers}},
	journal      = {IBM Journal of research and development},
	publisher    = {Ibm},
	volume       = 3,
	number       = 3,
	pages        = {210--229}
}

@article{sarawagi2004semi,
	author       = {Sarawagi, Sunita and Cohen, William W},
	year         = 2004,
	title        = {{Semi-markov conditional random fields for information extraction}},
	journal      = {Advances in neural information processing systems},
	publisher    = {Citeseer},
	volume       = 17,
	pages        = {1185--1192}
}

@inproceedings{sassioui2023visually,
	author       = {Sassioui, Abdellatif and Benouini, Rachid and El Ouargui, Yasser and El Kamili, Mohamed and Chergui, Meriyem and Ouzzif, Mohammed},
	year         = 2023,
	title        = {{Visually-Rich Document Understanding: Concepts, Taxonomy and Challenges}},
	booktitle    = {{2023 10th International Conference on Wireless Networks and Mobile Communications (WINCOM)}},
	pages        = {1--7},
	organization = {Ieee}
}

@article{saul1995exploiting,
	author       = {Saul, Lawrence and Jordan, Michael},
	year         = 1995,
	title        = {{Exploiting tractable substructures in intractable networks}},
	journal      = {Advances in neural information processing systems},
	volume       = 8
}

@inproceedings{saxena-etal-2021-question,
	author       = {Saxena, Apoorv  and Chakrabarti, Soumen  and Talukdar, Partha},
	year         = 2021,
	month        = aug,
	title        = {{Question Answering Over Temporal Knowledge Graphs}},
	booktitle    = {{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {6663--6676},
	doi          = {10.18653/v1/2021.acl-long.520},
	url          = {https://aclanthology.org/2021.acl-long.520},
	abstract     = {Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by providing temporal scopes (start and end times) on each edge in the KG. While Question Answering over KG (KGQA) has received some attention from the research community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored area. Lack of broad coverage datasets has been another factor limiting progress in this area. We address this challenge by presenting CRONQUESTIONS, the largest known Temporal KGQA dataset, clearly stratified into buckets of structural complexity. CRONQUESTIONS expands the only known previous dataset by a factor of 340x. We find that various state-of-the-art KGQA methods fall far short of the desired performance on this new dataset. In response, we also propose CRONKGQA, a transformer-based solution that exploits recent advances in Temporal KG embeddings, and achieves performance superior to all baselines, with an increase of 120{\%} in accuracy over the next best performing method. Through extensive experiments, we give detailed insights into the workings of CRONKGQA, as well as situations where significant further improvements appear possible. In addition to the dataset, we have released our code as well.}
}

@article{scalia2020evaluating,
	author       = {Scalia, Gabriele and Grambow, Colin A and Pernici, Barbara and Li, Yi-Pei and Green, William H},
	year         = 2020,
	title        = {{Evaluating Scalable Uncertainty Estimation Methods for Deep Learning-Based Molecular Property Prediction}},
	journal      = {Journal of Chemical Information and Modeling},
	publisher    = {ACS Publications}
}

@article{schmidhuber2015deep,
	author       = {Schmidhuber, J{\"u}rgen},
	year         = 2015,
	title        = {{Deep learning in neural networks: An overview}},
	journal      = {Neural networks},
	publisher    = {Elsevier},
	volume       = 61,
	pages        = {85--117}
}

@book{schuler2005verbnet,
	author       = {Schuler, Karin Kipper},
	year         = 2005,
	title        = {{VerbNet: A broad-coverage, comprehensive verb lexicon}},
	publisher    = {University of Pennsylvania}
}

@misc{seedat2019calibrated,
	author       = {Nabeel Seedat and Christopher Kanan},
	year         = 2019,
	title        = {{Towards calibrated and scalable uncertainty representations for neural networks}},
	journal      = {arXiv preprint arXiv:1911.00104},
	eprint       = {1911.00104},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

@article{sennrich2015neural,
	author       = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	year         = 2015,
	title        = {{Neural machine translation of rare words with subword units}},
	journal      = {arXiv preprint arXiv:1508.07909}
}

@article{settles1995active,
	author       = {Settles, Burr},
	year         = 1995,
	title        = {{Active Learning Literature Survey}},
	journal      = {Science},
	publisher    = {University of Wisconsin--Madison},
	volume       = 10,
	number       = 3,
	pages        = {237--304}
}

@article{sezgin2004survey,
	author       = {Sezgin, M. and Sankur, B.},
	year         = 2004,
	title        = {{Survey over image thresholding techniques and quantitative performance evaluation}},
	journal      = {Journal of Electronic Imaging},
	volume       = 13,
	number       = 1,
	pages        = {146--165}
}

@inproceedings{sha2003shallow,
	author       = {Sha, Fei and Pereira, Fernando},
	year         = 2003,
	title        = {{Shallow parsing with conditional random fields}},
	booktitle    = {{Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics}},
	pages        = {213--220}
}

@inproceedings{Shafaei2019,
	author       = {Shafaei, Alireza and Schmidt, Mark and Little, James},
	year         = 2019,
	title        = {{A Less Biased Evaluation of Out-of-distribution Sample Detectors}},
	booktitle    = {{Bmvc}}
}

@article{Shannon48,
	author       = {Shannon, Claude E},
	year         = 1948,
	title        = {{A Mathematical Theory of Communication}},
	journal      = {The Bell System Technical Journal},
	publisher    = {Nokia Bell Labs},
	volume       = 27,
	number       = 3,
	pages        = {379--423}
}

@inproceedings{shaw2018selfattention,
	author       = {Shaw, P. and Uszkoreit, J. and Vaswani, A.},
	year         = 2018,
	month        = {June},
	title        = {{Self-Attention with Relative Position Representations}},
	booktitle    = {{Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)}},
	pages        = {464--468}
}

@inproceedings{shen2020large,
	author       = {Shen, Zejiang and Zhang, Kaixuan and Dell, Melissa},
	year         = 2020,
	title        = {{A large dataset of historical Japanese documents with complex layouts}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops}},
	pages        = {548--549}
}

@article{shen2022vila,
	author       = {Shen, Zejiang and Lo, Kyle and Wang, Lucy Lu and Kuehl, Bailey and Weld, Daniel S and Downey, Doug},
	year         = 2022,
	title        = {{VILA: Improving structured content extraction from scientific PDFs using visual layout groups}},
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…},
	volume       = 10,
	pages        = {376--392}
}

@article{sheng2023s,
	author       = {Sheng, Ying and Cao, Shiyi and Li, Dacheng and Hooper, Coleman and Lee, Nicholas and Yang, Shuo and Chou, Christopher and Zhu, Banghua and Zheng, Lianmin and Keutzer, Kurt and others},
	year         = 2023,
	title        = {{S-lora: Serving thousands of concurrent lora adapters}},
	journal      = {arXiv preprint arXiv:2311.03285}
}

@article{shi2020uniquerandomizer,
	author       = {Kensen Shi and David Bieber and Charles Sutton},
	year         = 2020,
	title        = {{Incremental Sampling Without Replacement for Sequence Models}},
	booktitle    = {{Proceedings of the 37th International Conference on Machine Learning}}
}

@inproceedings{shi2023sequence,
	author       = {Shi, Jiaxin and Wang, Ke Alexander and Fox, Emily},
	year         = 2023,
	title        = {{Sequence Modeling with Multiresolution Convolutional Memory}},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {31312--31327},
	organization = {Pmlr}
}

@article{shimodaira2000improving,
	author       = {Shimodaira, Hidetoshi},
	year         = 2000,
	title        = {{Improving predictive inference under covariate shift by weighting the log-likelihood function}},
	journal      = {Journal of Statistical Planning and Inference},
	publisher    = {Elsevier},
	volume       = 90,
	number       = 2,
	pages        = {227--244}
}

@misc{shridhar2018uncertainty,
	author       = {Kumar Shridhar and Felix Laumann and Marcus Liwicki},
	year         = 2018,
	title        = {{Uncertainty Estimations by Softplus normalization in Bayesian Convolutional Neural Networks with Variational Inference}},
	eprint       = {1806.05978},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

@article{shrikumar2019calibration,
	author       = {Shrikumar, Avanti and Kundaje, Anshul},
	year         = 2020,
	title        = {{Maximum Likelihood with Bias-Corrected Calibration is Hard-To-Beat at Label Shift Adaptation}},
	journal      = {Proceedings of the 37th International Conference on Machine Learning}
}

@inproceedings{si2022prompting,
	author       = {Si, Chenglei and Gan, Zhe and Yang, Zhengyuan and Wang, Shuohang and Wang, Jianfeng and Boyd-Graber, Jordan Lee and Wang, Lijuan},
	year         = 2022,
	title        = {{Prompting GPT-3 To Be Reliable}},
	booktitle    = {{The Eleventh International Conference on Learning Representations}}
}

@inproceedings{silva2009learning,
	author       = {Silva, A. C.},
	year         = 2009,
	month        = {July},
	title        = {{Learning rich hidden Markov models in document analysis: Table location}},
	booktitle    = {{2009 10th International Conference on Document Analysis and Recognition}},
	pages        = {843--847},
	organization = {Ieee}
}

@article{silva2023classifier,
	author       = {Silva Filho, Telmo and Song, Hao and Perello-Nieto, Miquel and Santos-Rodriguez, Raul and Kull, Meelis and Flach, Peter},
	year         = 2023,
	title        = {{Classifier calibration: a survey on how to assess and improve predicted class probabilities}},
	journal      = {Machine Learning},
	publisher    = {Springer},
	pages        = {1--50}
}

@inproceedings{SimKD,
	author       = {D. Chen and J. Mei and H. Zhang and C. Wang and Y. Feng and C. Chen},
	year         = 2022,
	title        = {{Knowledge Distillation with the Reused Teacher Classifier}},
	booktitle    = {{2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}},
	publisher    = {IEEE Computer Society}
}

@article{SIMPSON_1949,
	author       = {E. H. Simpson},
	year         = 1949,
	month        = {apr},
	title        = {{Measurement of Diversity}},
	journal      = {Nature},
	publisher    = {Springer Science and Business Media {LLC}},
	volume       = 163,
	number       = 4148,
	pages        = {688--688},
	doi          = {10.1038/163688a0},
	url          = {https://doi.org/10.1038\%2F163688a0}
}

@article{simsa2023docile,
	author       = {{\v{S}}imsa, {\v{S}}t{\v{e}}p{\'a}n and {\v{S}}ulc, Milan and U{\v{r}}i{\v{c}}{\'a}{\v{r}}, Michal and Patel, Yash and Hamdi, Ahmed and Koci{\'a}n, Mat{\v{e}}j and Skalick{\`y}, Maty{\'a}{\v{s}} and Matas, Ji{\v{r}}{\'\i} and Doucet, Antoine and Coustaty, Micka{\"e}l and others},
	year         = 2023,
	title        = {{DocILE Benchmark for Document Information Localization and Extraction}},
	journal      = {arXiv preprint arXiv:2302.05658}
}

@inproceedings{singh2019towards,
	author       = {Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
	year         = 2019,
	title        = {{Towards vqa models that can read}},
	booktitle    = {{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}},
	pages        = {8317--8326}
}

@inproceedings{singh2022flava,
	author       = {Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
	year         = 2022,
	title        = {{Flava: A foundational language and vision alignment model}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
	pages        = {15638--15650}
}

@misc{SlideVQA,
	author       = {Tanaka, Ryota and Nishida, Kyosuke and Nishida, Kosuke and Hasegawa, Taku and Saito, Itsumi and Saito, Kuniko},
	year         = 2023,
	title        = {{SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images}},
	publisher    = {arXiv},
	doi          = {10.48550/arxiv.2301.04883},
	url          = {https://arxiv.org/abs/2301.04883},
	copyright    = {Creative Commons Attribution 4.0 International},
	keywords     = {Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences}
}

@article{slossberg2020calibration,
	author       = {Slossberg, Ron and Anschel, Oron and Markovitz, Amir and Litman, Ron and Aberdam, Aviad and Tsiper, Shahar and Mazor, Shai and Wu, Jon and Manmatha, R},
	year         = 2020,
	title        = {{On Calibration of Scene-Text Recognition Models}},
	journal      = {arXiv preprint arXiv:2012.12643}
}

@inproceedings{smith_understanding_2018,
	author       = {Smith, Lewis and Gal, Yarin},
	year         = 2018,
	title        = {{Understanding measures of uncertainty for adversarial example detection}},
	booktitle    = {{Proceedings of the Conference on Uncertainty on Artificial Intelligence (UAI)}},
	url          = {http://arxiv.org/abs/1803.08533},
	urldate      = {2020-04-17},
	abstract     = {Measuring uncertainty is a promising technique for detecting adversarial examples, crafted inputs on which the model predicts an incorrect class with high confidence. But many measures of uncertainty exist, including predictive en- tropy and mutual information, each capturing different types of uncertainty. We study these measures, and shed light on why mutual information seems to be effective at the task of adversarial example detection. We highlight failure modes for {MC} dropout, a widely used approach for estimating uncertainty in deep models. This leads to an improved understanding of the drawbacks of current methods, and a proposal to improve the quality of uncertainty estimates using probabilistic model ensembles. We give illustrative experiments using {MNIST} to demonstrate the intuition underlying the different measures of uncertainty, as well as experiments on a real world Kaggle dogs vs cats classification dataset.},
	journaltitle = {{arXiv}:1803.08533 [cs, stat]},
	date         = {2018-03-22},
	eprinttype   = {arxiv},
	eprint       = {1803.08533},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file         = {arXiv Fulltext PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/ISKRS6WD/Smith and Gal - 2018 - Understanding Measures of Uncertainty for Adversar.pdf:application/pdf;arXiv.org Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/HQFUCSB9/1803.html:text/html}
}

@article{smith1993bayesian,
	author       = {Smith, Adrian FM and Roberts, Gareth O},
	year         = 1993,
	title        = {{Bayesian computation via the Gibbs sampler and related Markov chain Monte Carlo methods}},
	journal      = {Journal of the Royal Statistical Society: Series B (Methodological)},
	publisher    = {Wiley Online Library},
	volume       = 55,
	number       = 1,
	pages        = {3--23}
}

@inproceedings{smock2022pubtables,
	author       = {Smock, Brandon and Pesala, Rohith and Abraham, Robin},
	year         = 2022,
	title        = {{PubTables-1M: Towards comprehensive table extraction from unstructured documents}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
	pages        = {4634--4642}
}

@inproceedings{song2019distribution,
	author       = {Song, Hao and Diethe, Tom and Kull, Meelis and Flach, Peter},
	year         = 2019,
	title        = {{Distribution calibration for regression}},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {5897--5906},
	organization = {Pmlr}
}

@misc{SpaCyNER,
	title        = {{SpaCy \texttt{en\_core\_web\_lg} Label Scheme}}}
}

@inproceedings{srihari1992document,
	author       = {Srihari, Sargur and Lam, Stephen and Govindaraju, Venu and Srihari, Rohini and Hull, Jonathan and Yair, E},
	year         = 1992,
	title        = {{Document understanding: Research directions}},
	booktitle    = {{DARPA Document Understanding Workshop, Xerox PARC, Palo Alto, CA}},
	organization = {Citeseer}
}

}
@article{Sriperumbudur2011,
	author       = {B. K. Sriperumbudur and K. Fukumizu and G. R.G. Lanckriet},
	year         = 2011,
	title        = {{Universality, Characteristic Kernels and RKHS Embedding of Measures}},
	journal      = {Journal of Machine Learning Research},
	volume       = 12,
	number       = 70,
	pages        = {2389--2410}
}

,
}
@article{srivastava_dropout,
	author       = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year         = 2014,
	title        = {{Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting}},
	journal      = {The Journal of Machine Learning Research},
	publisher    = {JMLR. org},
	volume       = 15,
	number       = 1,
	pages        = {1929--1958}
}

@inproceedings{stanislawek2021kleister,
	author       = {Stanis{\l}awek, Tomasz and Grali{\'n}ski, Filip and Wr{\'o}blewska, Anna and Lipi{\'n}ski, Dawid and Kaliska, Agnieszka and Rosalska, Paulina and Topolski, Bartosz and Biecek, Przemys{\l}aw},
	year         = 2021,
	title        = {{Kleister: key information extraction datasets involving long documents with complex layouts}},
	booktitle    = {{International Conference on Document Analysis and Recognition}},
	pages        = {564--579},
	organization = {Springer}
}

@article{stanton2021does,
	author       = {Stanton, Samuel and Izmailov, Pavel and Kirichenko, Polina and Alemi, Alexander A and Wilson, Andrew G},
	year         = 2021,
	title        = {{Does knowledge distillation really work?}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34,
	pages        = {6906--6919}
}

@article{stephenson2008two,
	author       = {Stephenson, David B and Coelho, Caio AS and Jolliffe, Ian T},
	year         = 2008,
	title        = {{Two extra components in the Brier score decomposition}},
	journal      = {Weather and Forecasting},
	volume       = 23,
	number       = 4,
	pages        = {752--757}
}

@misc{straydeepform,
	author       = {Stray, J and Svetlichnaya, S},
	title        = {{DeepForm: extract information from documents (2020)}}
}

@inproceedings{sun2019fine,
	author       = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
	year         = 2019,
	title        = {{How to fine-tune BERT for text classification?}},
	booktitle    = {{China National Conference on Chinese Computational Linguistics}},
	pages        = {194--206},
	organization = {Springer}
}

@article{sun2019functional,
	author       = {Sun, Shengyang and Zhang, Guodong and Shi, Jiaxin and Grosse, Roger},
	year         = 2019,
	title        = {{Functional Variational Bayesian Neural Networks}},
	journal      = {arXiv preprint arXiv:1903.05779}
}

@article{surismenon2023vipergpt,
	author       = {D\'idac Sur\'is and Sachit Menon and Carl Vondrick},
	year         = 2023,
	title        = {{ViperGPT: Visual Inference via Python Execution for Reasoning}},
	journal      = {Proceedings of IEEE International Conference on Computer Vision (ICCV)}
}

@article{sutton2006introduction,
	author       = {Sutton, Charles and McCallum, Andrew},
	year         = 2006,
	title        = {{An introduction to conditional random fields for relational learning}},
	journal      = {Introduction to statistical relational learning},
	publisher    = {MIT press Cambridge, MA},
	volume       = 2,
	pages        = {93--128}
}

@article{sutton2007dynamic,
	author       = {Sutton, Charles and McCallum, Andrew and Rohanimanesh, Khashayar},
	year         = 2007,
	title        = {{Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.}},
	journal      = {Journal of Machine Learning Research},
	volume       = 8,
	number       = 3
}

@inproceedings{sutton2007piecewise,
	author       = {Sutton, Charles and McCallum, Andrew},
	year         = 2007,
	title        = {{Piecewise pseudolikelihood for efficient training of conditional random fields}},
	booktitle    = {{Proceedings of the 24th international conference on Machine learning}},
	pages        = {863--870}
}

@article{T5,
	author       = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	year         = 2020,
	title        = {{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
	journal      = {Journal of Machine Learning Research},
	volume       = 21,
	number       = 140,
	pages        = {1--67},
	url          = {http://jmlr.org/papers/v21/20-074.html}
}

@article{taleb2013mathematical,
	author       = {Taleb, Nassim Nicholas and Douady, Raphael},
	year         = 2013,
	title        = {{Mathematical definition, mapping, and detection of (anti) fragility}},
	journal      = {Quantitative Finance},
	publisher    = {Taylor \& Francis},
	volume       = 13,
	number       = 11,
	pages        = {1677--1689}
}

@inproceedings{tan2019efficientnet,
	author       = {Tan, Mingxing and Le, Quoc},
	year         = 2019,
	title        = {{Efficientnet: Rethinking model scaling for convolutional neural networks}},
	booktitle    = {{International conference on machine learning}},
	pages        = {6105--6114},
	organization = {Pmlr}
}

@inproceedings{tang2023unifying,
	author       = {Tang, Zineng and Yang, Ziyi and Wang, Guoxin and Fang, Yuwei and Liu, Yang and Zhu, Chenguang and Zeng, Michael and Zhang, Cha and Bansal, Mohit},
	year         = 2023,
	title        = {{Unifying vision, text, and layout for universal document processing}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
	pages        = {19254--19264}
}

@incollection{tanwani2009classification,
	author       = {Tanwani, Ajay Kumar and Farooq, Muddassar},
	year         = 2009,
	title        = {{Classification Potential vs. Classification Accuracy: a Comprehensive Study of Evolutionary Algorithms with Biomedical Datasets}},
	booktitle    = {{Learning Classifier Systems}},
	publisher    = {Springer},
	pages        = {127--144}
}

@article{teo2010bundle,
	author       = {Teo, Choon Hui and Vishwanathan, SVN and Smola, Alex and Le, Quoc V},
	year         = 2010,
	title        = {{Bundle Methods for Regularized Risk Minimization.}},
	journal      = {Journal of Machine Learning Research},
	volume       = 11,
	number       = 1
}

@article{teye_bayesian_2018,
	author       = {Teye, Mattias and Azizpour, Hossein and Smith, Kevin},
	year         = 2018,
	month        = jul,
	title        = {{Bayesian Uncertainty Estimation for Batch Normalized Deep Networks}},
	journal      = {arXiv:1802.06455 [stat]},
	url          = {http://arxiv.org/abs/1802.06455},
	urldate      = {2020-04-24},
	note         = {arXiv: 1802.06455},
	abstract     = {We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models. We further demonstrate that this finding allows us to make meaningful estimates of the model uncertainty using conventional architectures, without modifications to the network or the training procedure. Our approach is thoroughly validated by measuring the quality of uncertainty in a series of empirical experiments on different tasks. It outperforms baselines with strong statistical significance, and displays competitive performance with recent Bayesian approaches.},
	keywords     = {Statistics - Machine Learning},
	file         = {arXiv Fulltext PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/RB42E7LT/Teye et al. - 2018 - Bayesian Uncertainty Estimation for Batch Normaliz.pdf:application/pdf;arXiv.org Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/FPHG2G49/1802.html:text/html}
}

@misc{TFDS,
	title        = {{TensorFlow Datasets, A collection of ready-to-use datasets}},
	howpublished = {\url{https://www.tensorflow.org/datasets}}
}

@inproceedings{thorne-etal-2018-fever,
	author       = {Thorne, James  and Vlachos, Andreas  and Christodoulopoulos, Christos  and Mittal, Arpit},
	year         = 2018,
	month        = jun,
	title        = {{FEVER: a Large-scale Dataset for Fact Extraction and VERification}},
	booktitle    = {{Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)}},
	publisher    = {Association for Computational Linguistics},
	address      = {New Orleans, Louisiana},
	pages        = {809--819},
	doi          = {10.18653/v1/N18-1074},
	url          = {https://aclanthology.org/N18-1074},
	abstract     = {In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87{\%}, while if we ignore the evidence we achieve 50.91{\%}. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.}
}

@inproceedings{tian2019contrastive,
	author       = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
	year         = 2019,
	title        = {{Contrastive representation distillation}},
	booktitle    = {{International Conference on Learning Representations (ICLR)}}
}

@inproceedings{tishby2015deep,
	author       = {Tishby, Naftali and Zaslavsky, Noga},
	year         = 2015,
	title        = {{Deep learning and the information bottleneck principle}},
	booktitle    = {{2015 ieee information theory workshop (itw)}},
	pages        = {1--5},
	organization = {Ieee}
}

@inproceedings{tito2021document,
	author       = {Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest},
	year         = 2021,
	title        = {{Document collection visual question answering}},
	booktitle    = {{Document Analysis and Recognition--ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5--10, 2021, Proceedings, Part II 16}},
	pages        = {778--792},
	organization = {Springer}
}

@inproceedings{tito2021icdar,
	author       = {Tito, Rub{\`e}n and Mathew, Minesh and Jawahar, CV and Valveny, Ernest and Karatzas, Dimosthenis},
	year         = 2021,
	title        = {{Icdar 2021 competition on document visual question answering}},
	booktitle    = {{International Conference on Document Analysis and Recognition}},
	pages        = {635--649},
	organization = {Springer}
}

@article{tito2022hierarchical,
	author       = {Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest},
	year         = 2023,
	title        = {{Hierarchical multimodal transformers for Multipage DocVQA}},
	journal      = {Pattern Recognition},
	publisher    = {Elsevier},
	volume       = 144,
	pages        = 109834
}

@inproceedings{tjongkimsang2003conll,
	author       = {Tjong Kim Sang, Erik F. and De Meulder, Fien},
	year         = 2003,
	title        = {{Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition}},
	booktitle    = {{Proceedings of CoNLL-2003}},
	publisher    = {Edmonton, Canada},
	pages        = {142--147},
	editor       = {Walter Daelemans and Miles Osborne}
}

@inbook{ToE,
	author       = {Nicholas Rescher},
	year         = 2013,
	title        = {{Chapter 1: Holistic explanation and the idea of a grand unified theory}},
	booktitle    = {{Volume 9 Studies in Metaphilosophy}},
	publisher    = {De Gruyter},
	address      = {Berlin, Boston},
	pages        = {1--10},
	isbn         = 9783110326420,
	url          = {https://doi.org/10.1515/9783110326420.1},
	lastchecked  = {2022-10-28}
}

@article{touvron2023llama,
	author       = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
	year         = 2023,
	title        = {{Llama 2: Open foundation and fine-tuned chat models}},
	journal      = {arXiv preprint arXiv:2307.09288}
}

@inproceedings{tran2019bayesian,
	author       = {Dustin Tran and Michael W. Dusenberry and Danijar Hafner and Mark van der Wilk},
	year         = 2019,
	title        = {{Bayesian Layers: A module for neural network uncertainty}},
	booktitle    = {{Neural Information Processing Systems}}
}

@article{tran2020all,
	author       = {Tran, Ba-Hien and Rossi, Simone and Milios, Dimitrios and Filippone, Maurizio},
	year         = 2020,
	title        = {{All You Need is a Good Functional Prior for Bayesian Deep Learning}},
	journal      = {arXiv preprint arXiv:2011.12829}
}

@inproceedings{tran2022plex,
	author       = {Tran, Dustin and Liu, Jeremiah Zhe and Dusenberry, Michael W and Phan, Du and Collier, Mark and Ren, Jie and Han, Kehang and Wang, Zi and Mariet, Zelda E and Hu, Huiyi and others},
	year         = 2022,
	title        = {{Plex: Towards Reliability using Pretrained Large Model Extensions}},
	booktitle    = {{First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022}}
}

@article{treviso2023efficient,
	author       = {Treviso, Marcos and Lee, Ji-Ung and Ji, Tianchu and Aken, Betty van and Cao, Qingqing and Ciosici, Manuel R and Hassid, Michael and Heafield, Kenneth and Hooker, Sara and Raffel, Colin and others},
	year         = 2023,
	title        = {{Efficient methods for natural language processing: A survey}},
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…},
	volume       = 11,
	pages        = {826--860}
}

@article{trischler2016newsqa,
	author       = {Trischler, Adam and Wang, Tong and Yuan, Xingdi and Harris, Justin and Sordoni, Alessandro and Bachman, Philip and Suleman, Kaheer},
	year         = 2016,
	title        = {{Newsqa: A machine comprehension dataset}},
	journal      = {arXiv preprint arXiv:1611.09830}
}

@inproceedings{trivedi2017lc,
	author       = {Trivedi, Priyansh and Maheshwari, Gaurav and Dubey, Mohnish and Lehmann, Jens},
	year         = 2017,
	title        = {{Lc-quad: A corpus for complex question answering over knowledge graphs}},
	booktitle    = {{International Semantic Web Conference}},
	pages        = {210--218},
	organization = {Springer}
}

@article{tsochantaridis2005large,
	author       = {Tsochantaridis, Ioannis and Joachims, Thorsten and Hofmann, Thomas and Altun, Yasemin and Singer, Yoram},
	year         = 2005,
	title        = {{Large margin methods for structured and interdependent output variables.}},
	journal      = {Journal of machine learning research},
	volume       = 6,
	number       = 9
}

@article{tsymbalov2020dropout,
	author       = {Tsymbalov, Evgenii and Fedyanin, Kirill and Panov, Maxim},
	year         = 2020,
	title        = {{Dropout Strikes Back: Improved Uncertainty Estimation via Diversity Sampled Implicit Ensembles}},
	journal      = {arXiv preprint arXiv:2003.03274}
}

@inproceedings{tu2019benchmarking,
	author       = {Tu, Lifu and Gimpel, Kevin},
	year         = 2019,
	title        = {{Benchmarking Approximate Inference Methods for Neural Structured Prediction}},
	booktitle    = {{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}},
	pages        = {3313--3324}
}

@incollection{turnersahani2011,
	author       = {R. E. Turner and M. Sahani},
	year         = 2011,
	title        = {{Two problems with variational expectation maximisation for time-series models}},
	booktitle    = {{Bayesian Time series models}},
	publisher    = {Cambridge University Press},
	pages        = {109--130},
	editor       = {D. Barber and T. Cemgil and S. Chiappa},
	chapter      = 5,
	abstract     = {Variational methods are a key component of the approximate inference and learning toolbox. These methods fill an important middle ground, retaining distributional information about uncertainty in latent variables, unlike maximum a posteriori methods (MAP), and yet generally requiring less computational time than Monte Carlo Markov Chain methods. In particular the variational Expectation Maximisation (vEM) and variational Bayes algorithms, both involving variational optimisation of a free-energy, are widely used in time-series modelling. Here, we investigate the success of vEM in simple probabilistic time-series models. First we consider the inference step of vEM, and show that a consequence of the well-known compactness property of variational inference is a failure to propagate uncertainty in time, thus limiting the usefulness of the retained distributional information. In particular, the uncertainty may appear to be smallest precisely when the approximation is poorest. Second, we consider parameter learning and analytically reveal systematic biases in the parameters found by vEM. Surprisingly, simpler variational approximations (such a mean-field) can lead to less bias than more complicated structured approximations.}
}

@article{turski2023ccpdf,
	author       = {Turski, Micha{\l} and Stanis{\l}awek, Tomasz and Kaczmarek, Karol and Dyda, Pawe{\l} and Grali{\'n}ski, Filip},
	year         = 2023,
	title        = {{CCpdf: Building a High Quality Corpus for Visually Rich Documents from Web Crawl Data}},
	journal      = {arXiv preprint arXiv:2304.14953}
}

@article{unionbound,
	author       = {David Hunter},
	year         = 1976,
	title        = {{An Upper Bound for the Probability of a Union}},
	journal      = {Journal of Applied Probability},
	publisher    = {Applied Probability Trust},
	volume       = 13,
	number       = 3,
	pages        = {597--603},
	issn         = {00219002},
	url          = {http://www.jstor.org/stable/3212481},
	urldate      = {2022-04-25},
	abstract     = {The problem of bounding P (∪ Ai) given P(Ai) and P(AiA j) for i ≠ j = 1, ⋯, k goes back to Boole (1854) and Bonferroni (1936). In this paper a new family of upper bounds is derived using results in graph theory. This family contains the bound of Kounias (1968), and the smallest upper bound in the family for a given application is easily derivable via the minimal spanning tree algorithm of Kruskal (1956). The properties of the algorithm and of the multivariate normal and t distributions are shown to provide considerable simplifications when approximating tail probabilities of maxima from these distributions.}
}

@inproceedings{vadera2020generalized,
	author       = {Vadera, Meet and Jalaian, Brian and Marlin, Benjamin},
	year         = 2020,
	title        = {{Generalized Bayesian posterior expectation distillation for deep neural networks}},
	booktitle    = {{Conference on Uncertainty in Artificial Intelligence}},
	pages        = {719--728},
	organization = {Pmlr}
}

@article{vadera2020ursabench,
	author       = {Vadera, Meet P and Cobb, Adam D and Jalaian, Brian and Marlin, Benjamin M},
	year         = 2020,
	title        = {{URSABench: Comprehensive Benchmarking of Approximate Bayesian Inference Methods for Deep Neural Networks}},
	journal      = {arXiv preprint arXiv:2007.04466}
}

@inproceedings{vaicenavicius2019evaluating,
	author       = {Vaicenavicius, Juozas and Widmann, David and Andersson, Carl and Lindsten, Fredrik and Roll, Jacob and Sch{\"o}n, Thomas},
	year         = 2019,
	title        = {{Evaluating model calibration in classification}},
	booktitle    = {{The 22nd International Conference on Artificial Intelligence and Statistics}},
	pages        = {3459--3467},
	organization = {Pmlr}
}

@article{van2023beyond,
	author       = {Van Landeghem, Jordy and Biswas, Sanket and Blaschko, Matthew B and Moens, Marie-Francine},
	year         = 2023,
	title        = {{Beyond Document Page Classification: Design, Datasets, and Challenges}},
	journal      = {arXiv preprint arXiv:2308.12896}
}

@inproceedings{vanamersfoort20duq,
	author       = {Van Amersfoort, Joost and Smith, Lewis and Teh, Yee Whye and Gal, Yarin},
	year         = 2020,
	month        = {13--18 Jul},
	title        = {{Uncertainty Estimation Using a Single Deep Deterministic Neural Network}},
	booktitle    = {{Proceedings of the 37th International Conference on Machine Learning}},
	publisher    = {Pmlr},
	pages        = {9690--9700},
	editor       = {Hal Daumé III and Aarti Singh}
}

@misc{vanlandeghem2023document,
	author       = {Jordy Van Landeghem and Sanket Biswas and Matthew B. Blaschko and Marie-Francine Moens},
	year         = 2023,
	title        = {{Beyond Document Page Classification: Design, Datasets, and Challenges}},
	booktitle    = {{International Conference on Computer Vision}},
	eprint       = {2308.12896},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}

@inproceedings{vapnik1992principles,
	author       = {Vapnik, Vladimir},
	year         = 1992,
	title        = {{Principles of risk minimization for learning theory}},
	booktitle    = {{Advances in neural information processing systems}},
	pages        = {831--838}
}

@article{vaswani2017attention,
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	year         = 2017,
	title        = {{Attention is all you need}},
	journal      = {Advances in neural information processing systems},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	volume       = 30,
	pages        = {5998--6008}
}

@inproceedings{vazhentsev2023hybrid,
	author       = {Vazhentsev, Artem and Kuzmin, Gleb and Tsvigun, Akim and Panchenko, Alexander and Panov, Maxim and Burtsev, Mikhail and Shelmanov, Artem},
	year         = 2023,
	title        = {{Hybrid uncertainty quantification for selective text classification in ambiguous tasks}},
	booktitle    = {{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}},
	pages        = {11659--11681}
}

@article{veale2021demystifying,
	author       = {Veale, Michael and Zuiderveen Borgesius, Frederik},
	year         = 2021,
	title        = {{Demystifying the Draft EU Artificial Intelligence Act—Analysing the good, the bad, and the unclear elements of the proposed approach}},
	journal      = {Computer Law Review International},
	publisher    = {Verlag Dr. Otto Schmidt},
	volume       = 22,
	number       = 4,
	pages        = {97--112}
}

@inproceedings{vembu2009probabilistic,
	author       = {Vembu, Shankar and G{\"a}rtner, Thomas and Boley, Mario},
	year         = 2009,
	title        = {{Probabilistic structured predictors}},
	booktitle    = {{Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence}},
	pages        = {557--564}
}

@article{veres2023self,
	author       = {Veres, Csaba and Sampson, Jennifer},
	year         = 2023,
	title        = {{Self supervised learning and the poverty of the stimulus}},
	journal      = {Data \& Knowledge Engineering},
	publisher    = {Elsevier},
	volume       = 147,
	pages        = 102208
}

@article{vernekar2019outofdistribution,
	author       = {Vernekar, Sachin and Gaurav, Ashish and Abdelzad, Vahdat and Denouden, Taylor and Salay, Rick and Czarnecki, Krzysztof},
	year         = 2019,
	title        = {{Out-of-distribution Detection in Classifiers via Generation}},
	journal      = {arXiv preprint arXiv:1910.04241}
}

@article{vijayakumar2016diverse,
	author       = {Vijayakumar, Ashwin K and Cogswell, Michael and Selvaraju, Ramprasath R and Sun, Qing and Lee, Stefan and Crandall, David and Batra, Dhruv},
	year         = 2016,
	title        = {{Diverse beam search: Decoding diverse solutions from neural sequence models}},
	journal      = {arXiv preprint arXiv:1610.02424}
}

@inproceedings{VisualMRC2021,
	author       = {Ryota Tanaka and Kyosuke Nishida and Sen Yoshida},
	year         = 2021,
	title        = {{VisualMRC: Machine Reading Comprehension on Document Images}},
	booktitle    = {{Aaai}}
}

@inproceedings{vojivr2023calibrated,
	author       = {Voj{\'\i}{{r}}, Tom{\'a}{{s}} and {{S}}ochman, Jan and Aljundi, Rahaf and Matas, Ji{{r}}{\'\i}},
	year         = 2023,
	title        = {{Calibrated Out-of-Distribution Detection with a Generic Representation}},
	booktitle    = {{2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)}},
	pages        = {4509--4518},
	organization = {Ieee}
}

@inproceedings{vrdgcn_naacl19,
	author       = {Liu, Xiaojing  and Gao, Feiyu  and Zhang, Qiong  and Zhao, Huasha},
	year         = 2019,
	title        = {{Graph Convolution for Multimodal Information Extraction from Visually Rich Documents}},
	booktitle    = {{Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)}},
	pages        = {32--39}
}

@article{wald1945statistical,
	author       = {Wald, Abraham},
	year         = 1945,
	title        = {{Statistical decision functions which minimize the maximum risk}},
	journal      = {Annals of Mathematics},
	publisher    = {Jstor},
	pages        = {265--280}
}

@article{wan2019long,
	author       = {Wan, Lulu and Papageorgiou, George and Seddon, Michael and Bernardoni, Mirko},
	year         = 2019,
	title        = {{Long-length Legal Document Classification}},
	journal      = {arXiv preprint arXiv:1912.06905}
}

@inproceedings{wang2019symmetric,
	author       = {Wang, Yisen and Ma, Xingjun and Chen, Zaiyi and Luo, Yuan and Yi, Jinfeng and Bailey, James},
	year         = 2019,
	title        = {{Symmetric cross entropy for robust learning with noisy labels}},
	booktitle    = {{Proceedings of the IEEE/CVF international conference on computer vision}},
	pages        = {322--330}
}

@article{wang2020inference,
	author       = {Wang, Shuo and Tu, Zhaopeng and Shi, Shuming and Liu, Yang},
	year         = 2020,
	title        = {{On the inference calibration of neural machine translation}},
	journal      = {arXiv preprint arXiv:2005.00963}
}

@article{wang2020investigating,
	author       = {Wang, Liang and Liu, Jinlong and Liu, Jingming},
	year         = 2020,
	title        = {{Investigating Label Bias in Beam Search for Open-ended Text Generation}},
	journal      = {arXiv preprint arXiv:2005.11009}
}

@inproceedings{wang2020transferablecalibration,
	author       = {Wang, Ximei and Long, Mingsheng and Wang, Jianmin and Jordan, Michael},
	year         = 2020,
	title        = {{Transferable Calibration with Lower Bias and Variance in Domain Adaptation}},
	booktitle    = {{Advances in Neural Information Processing Systems}},
	publisher    = {Curran Associates, Inc.},
	volume       = 33,
	pages        = {19212--19223},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin}
}

@article{wang2021unified,
	author       = {Wang, Wei and Li, Baopu and Yang, Shuhui and Sun, Jing and Ding, Zhengming and Chen, Junyang and Dong, Xiao and Wang, Zhihui and Li, Haojie},
	year         = 2021,
	title        = {{A Unified Joint Maximum Mean Discrepancy for Domain Adaptation}},
	journal      = {arXiv preprint arXiv:2101.09979}
}

@article{wang2022benchmark,
	author       = {Wang, Zilong and Zhou, Yichao and Wei, Wei and Lee, Chen-Yu and Tata, Sandeep},
	year         = 2022,
	title        = {{A Benchmark for Structured Extractions from Complex Documents}},
	journal      = {arXiv preprint arXiv:2211.15421}
}

@article{wang2022efficient,
	author       = {Wang, Chaofei and Yang, Qisen and Huang, Rui and Song, Shiji and Huang, Gao},
	year         = 2022,
	title        = {{Efficient knowledge distillation from model checkpoints}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 35,
	pages        = {607--619}
}

@inproceedings{wang2022lilt,
	author       = {Wang, Jiapeng and Jin, Lianwen and Ding, Kai},
	year         = 2022,
	title        = {{LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding}},
	booktitle    = {{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}},
	pages        = {7747--7757}
}

@misc{wang2023docllm,
	author       = {Dongsheng Wang and Natraj Raman and Mathieu Sibue and Zhiqiang Ma and Petr Babkin and Simerjot Kaur and Yulong Pei and Armineh Nourbakhsh and Xiaomo Liu},
	year         = 2023,
	title        = {{DocLLM: A layout-aware generative language model for multimodal document understanding}},
	eprint       = {2401.00908},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@article{wang2023layout,
	author       = {Wang, Wenjin and Li, Yunhao and Ou, Yixin and Zhang, Yin},
	year         = 2023,
	title        = {{Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering}},
	journal      = {arXiv preprint arXiv:2306.00526}
}

@inproceedings{wang2023vrdu,
	author       = {Wang, Zilong and Zhou, Yichao and Wei, Wei and Lee, Chen-Yu and Tata, Sandeep},
	year         = 2023,
	title        = {{Vrdu: A benchmark for visually-rich document understanding}},
	booktitle    = {{Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}},
	pages        = {5184--5193}
}

@article{webb2023emergent,
	author       = {Webb, Taylor and Holyoak, Keith J and Lu, Hongjing},
	year         = 2023,
	title        = {{Emergent analogical reasoning in large language models}},
	journal      = {Nature Human Behaviour},
	publisher    = {Nature Publishing Group UK London},
	volume       = 7,
	number       = 9,
	pages        = {1526--1541}
}

@article{wei2022emergent,
	author       = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
	year         = 2022,
	title        = {{Emergent abilities of large language models}},
	journal      = {arXiv preprint arXiv:2206.07682}
}

@inproceedings{wei2022mitigating,
	author       = {Wei, Hongxin and Xie, Renchunzi and Cheng, Hao and Feng, Lei and An, Bo and Li, Yixuan},
	year         = 2022,
	title        = {{Mitigating neural network overconfidence with logit normalization}},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {23631--23644},
	organization = {Pmlr}
}

@article{welbl2018constructing,
	author       = {Welbl, Johannes and Stenetorp, Pontus and Riedel, Sebastian},
	year         = 2018,
	title        = {{Constructing datasets for multi-hop reading comprehension across documents}},
	journal      = {Transactions of the Association for Computational Linguistics},
	volume       = 6,
	pages        = {287--302}
}

@inproceedings{wen2019batchensemble,
	author       = {Wen, Yeming and Tran, Dustin and Ba, Jimmy},
	year         = 2019,
	title        = {{BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning}},
	booktitle    = {{International Conference on Learning Representations}}
}

@inproceedings{wen2019bayesian,
	author       = {Wen, Jun and Zheng, Nenggan and Yuan, Junsong and Gong, Zhefeng and Chen, Changyou},
	year         = 2019,
	month        = 7,
	title        = {{Bayesian Uncertainty Matching for Unsupervised Domain Adaptation}},
	booktitle    = {{Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19}},
	pages        = {3849--3855},
	doi          = {10.24963/ijcai.2019/534},
	url          = {https://doi.org/10.24963/ijcai.2019/534}
}

@inproceedings{wenger2020calibration,
	author       = {Jonathan Wenger and Hedvig Kjellstr{\"{o}}m and Rudolph Triebel},
	year         = 2020,
	title        = {{Non-Parametric Calibration for Classification}},
	booktitle    = {{23rd International Conference on Artificial Intelligence and Statistics (AISTATS)}},
	publisher    = {Pmlr},
	pages        = {178--190},
	keywords     = {calibration, non-parametric, gaussian processes, classification},
	pdf          = {http://proceedings.mlr.press/v108/wenger20a/wenger20a.pdf}
}

@inproceedings{wenzel2020good,
	author       = {Wenzel, Florian and Roth, Kevin and Veeling, Bastiaan and Swiatkowski, Jakub and Tran, Linh and Mandt, Stephan and Snoek, Jasper and Salimans, Tim and Jenatton, Rodolphe and Nowozin, Sebastian},
	year         = 2020,
	title        = {{How Good is the Bayes Posterior in Deep Neural Networks Really?}},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {10248--10259},
	organization = {Pmlr}
}

@article{weston2023system,
	author       = {Weston, Jason and Sukhbaatar, Sainbayar},
	year         = 2023,
	title        = {{System 2 Attention (is something you might need too)}},
	journal      = {arXiv preprint arXiv:2311.11829}
}

@incollection{widmann2019,
	author       = {David Widmann and Fredrik Lindsten and Dave Zachariah},
	year         = 2019,
	title        = {{Calibration tests in multi-class classification: A unifying framework}},
	booktitle    = {{Proceedings of the 32th International Conference on Neural Information Processing Systems}},
	pages        = {12236--12246}
}

@inproceedings{widmann2021calibration,
	author       = {David Widmann and Fredrik Lindsten and Dave Zachariah},
	year         = 2021,
	title        = {{Calibration tests beyond classification}},
	booktitle    = {{International Conference on Learning Representations}}
}

@article{wiedemann2021multi,
	author       = {Wiedemann, Gregor and Heyer, Gerhard},
	year         = 2021,
	title        = {{Multi-modal page stream segmentation with convolutional neural networks}},
	journal      = {Language Resources and Evaluation},
	publisher    = {Springer},
	volume       = 55,
	pages        = {127--150}
}

@article{wilks2010sampling,
	author       = {Wilks, Daniel S},
	year         = 2010,
	title        = {{Sampling distributions of the Brier score and Brier skill score under serial dependence}},
	journal      = {Quarterly Journal of the Royal Meteorological Society},
	publisher    = {Wiley Online Library},
	volume       = 136,
	number       = 653,
	pages        = {2109--2118}
}

@article{williamson2016composite,
	author       = {Williamson, Robert C and Vernet, Elodie and Reid, Mark D},
	year         = 2016,
	title        = {{Composite Multiclass Losses}},
	journal      = {Journal of Machine Learning Research},
	volume       = 17,
	pages        = {1--52}
}

@article{wilson_case_2020,
	author       = {Wilson, Andrew Gordon},
	year         = 2020,
	title        = {{The Case for Bayesian Deep Learning}},
	journal      = {arXiv preprint arXiv:2001.10995}
}

@inproceedings{wu2016sentiment,
	author       = {Wu, Fangzhao and Huang, Yongfeng},
	year         = 2016,
	title        = {{Sentiment domain adaptation with multiple sources}},
	booktitle    = {{Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}},
	pages        = {301--310}
}

@misc{wu2019detectron2,
	author       = {Yuxin Wu and Alexander Kirillov and Francisco Massa and Wan-Yen Lo and Ross Girshick},
	year         = 2019,
	title        = {{Detectron2}},
	howpublished = {\url{https://github.com/facebookresearch/detectron2}}
}

@article{wu2020multi,
	author       = {Wu, Guoqiang and Zhu, Jun},
	year         = 2020,
	title        = {{Multi-label classification: do Hamming loss and subset accuracy really conflict with each other?}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33
}

@inproceedings{wu2022region,
	author       = {Wu, Xinya and Zheng, Duo and Wang, Ruonan and Sun, Jiashen and Hu, Minzhen and Feng, Fangxiang and Wang, Xiaojie and Jiang, Huixing and Yang, Fan},
	year         = 2022,
	title        = {{A Region-based Document VQA}},
	booktitle    = {{Proceedings of the 30th ACM International Conference on Multimedia}},
	pages        = {4909--4920}
}

@inproceedings{xiao_quantifying_2018,
	author       = {Xiao, Yijun and Wang, William Yang},
	year         = 2019,
	title        = {{Quantifying Uncertainties in Natural Language Processing Tasks}},
	booktitle    = {{Proceedings of the AAAI Conference on Artificial Intelligence}},
	volume       = 33,
	pages        = {7322--7329},
	abstract     = {Reliable uncertainty quantification is a first step towards building explainable, transparent, and accountable artificial intelligent systems. Recent progress in Bayesian deep learning has made such quantification realizable. In this paper, we propose novel methods to study the benefits of characterizing model and data uncertainties for natural language processing ({NLP}) tasks. With empirical experiments on sentiment analysis, named entity recognition, and language modeling using convolutional and recurrent neural network models, we show that explicitly modeling uncertainties is not only necessary to measure output confidence levels, but also useful at enhancing model performances in various {NLP} tasks.}
}

@article{xing2004graph,
	author       = {Xing, Eric P and Jordan, Michael I and Russell, Stuart},
	year         = 2004,
	title        = {{Graph partition strategies for generalized mean field inference}},
	journal      = {Proc Uncertainty in Artificial Intelligence 20 (UAI2004)},
	publisher    = {AUAI Press}
}

@inproceedings{xing2020early,
	author       = {Xing, Qunliang and Xu, Mai and Li, Tianyi and Guan, Zhenyu},
	year         = 2020,
	title        = {{Early exit or not: Resource-efficient blind quality enhancement for compressed images}},
	booktitle    = {{European Conference on Computer Vision}},
	pages        = {275--292},
	organization = {Springer}
}

@inproceedings{xu2020layoutlm,
	author       = {Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
	year         = 2020,
	title        = {{Layoutlm: Pre-training of text and layout for document image understanding}},
	booktitle    = {{Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}},
	pages        = {1192--1200}
}

@article{xu2020layoutlmv2,
	author       = {Xu, Yang and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wei, Furu and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Che, Wanxiang and others},
	year         = 2020,
	title        = {{Layoutlmv2: Multi-modal pre-training for visually-rich document understanding}},
	journal      = {arXiv preprint arXiv:2012.14740}
}

@inproceedings{xu2021how,
	author       = {Keyulu Xu and Mozhi Zhang and Jingling Li and Simon Shaolei Du and Ken-Ichi Kawarabayashi and Stefanie Jegelka},
	year         = 2021,
	title        = {{How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks}},
	booktitle    = {{International Conference on Learning Representations}},
	url          = {https://openreview.net/forum?id=UH-cmocLJC}
}

@article{xu2022survey,
	author       = {Xu, Canwen and McAuley, Julian},
	year         = 2022,
	title        = {{A survey on dynamic neural networks for natural language processing}},
	journal      = {arXiv preprint arXiv:2202.07101}
}

@inproceedings{yang-etal-2015-wikiqa,
	author       = {Yang, Yi  and Yih, Wen-tau  and Meek, Christopher},
	year         = 2015,
	month        = sep,
	title        = {{WikiQA: A Challenge Dataset for Open-Domain Question Answering}},
	booktitle    = {{Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}},
	publisher    = {Association for Computational Linguistics},
	address      = {Lisbon, Portugal},
	pages        = {2013--2018},
	doi          = {10.18653/v1/D15-1237},
	url          = {https://aclanthology.org/D15-1237}
}

@inproceedings{yang2018hotpotqa,
	author       = {Yang, Zhilin  and Qi, Peng  and Zhang, Saizheng  and Bengio, Yoshua  and Cohen, William  and Salakhutdinov, Ruslan  and Manning, Christopher D.},
	year         = 2018,
	title        = {{HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering}},
	journal      = {arXiv preprint arXiv:1809.09600},
	booktitle    = {{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}},
	publisher    = {Association for Computational Linguistics},
	address      = {Brussels, Belgium},
	pages        = {2369--2380},
	doi          = {10.18653/v1/D18-1259},
	url          = {https://aclanthology.org/D18-1259}
}

@inproceedings{yang2018sgm,
	author       = {Yang, Pengcheng  and Sun, Xu  and Li, Wei  and Ma, Shuming  and Wu, Wei  and Wang, Houfeng},
	year         = 2018,
	month        = aug,
	title        = {{SGM: Sequence Generation Model for Multi-label Classification}},
	booktitle    = {{Proceedings of the 27th International Conference on Computational Linguistics}},
	publisher    = {Association for Computational Linguistics},
	address      = {Santa Fe, New Mexico, USA},
	pages        = {3915--3926}
}

@article{yang2022multi,
	author       = {Yang, Yuzhe and Wang, Hao and Katabi, Dina},
	year         = 2022,
	title        = {{On Multi-Domain Long-Tailed Recognition, Generalization and Beyond}},
	journal      = {arXiv preprint arXiv:2203.09513},
	booktitle    = {{Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XX}},
	location     = {Tel Aviv, Israel},
	publisher    = {Springer-Verlag},
	address      = {Berlin, Heidelberg},
	pages        = {57–75},
	doi          = {10.1007/978-3-031-20044-1_4},
	isbn         = {978-3-031-20043-4},
	url          = {https://doi.org/10.1007/978-3-031-20044-1\%5F4},
	numpages     = 19
}

@article{yang2022vitkd,
	author       = {Yang, Zhendong and Li, Zhe and Zeng, Ailing and Li, Zexian and Yuan, Chun and Li, Yu},
	year         = 2022,
	title        = {{ViTKD: Practical Guidelines for ViT feature knowledge distillation}},
	journal      = {arXiv preprint arXiv:2209.02432}
}

@article{yang2023knowledge,
	author       = {Yang, Zhendong and Zeng, Ailing and Li, Zhe and Zhang, Tianke and Yuan, Chun and Li, Yu},
	year         = 2023,
	title        = {{From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels}},
	journal      = {arXiv preprint arXiv:2303.13005}
}

@article{yao2019quality,
	author       = {Yao, Jiayu and Pan, Weiwei and Ghosh, Soumya and Doshi-Velez, Finale},
	year         = 2019,
	title        = {{Quality of uncertainty quantification for Bayesian neural network inference}},
	journal      = {arXiv preprint arXiv:1906.09686}
}

@article{ye2021pack,
	author       = {Ye, Deming and Lin, Yankai and Sun, Maosong},
	year         = 2021,
	title        = {{Pack Together: Entity and Relation Extraction with Levitated Marker}},
	journal      = {arXiv preprint arXiv:2109.06067}
}

@misc{ye2023mplugdocowl,
	author       = {Jiabo Ye and Anwen Hu and Haiyang Xu and Qinghao Ye and Ming Yan and Yuhao Dan and Chenlin Zhao and Guohai Xu and Chenliang Li and Junfeng Tian and Qian Qi and Ji Zhang and Fei Huang},
	year         = 2023,
	title        = {{mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding}},
	eprint       = {2307.02499},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@misc{ye2023ureader,
	author       = {Jiabo Ye and Anwen Hu and Haiyang Xu and Qinghao Ye and Ming Yan and Guohai Xu and Chenliang Li and Junfeng Tian and Qi Qian and Ji Zhang and Qin Jin and Liang He and Xin Alex Lin and Fei Huang},
	year         = 2023,
	title        = {{UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model}},
	eprint       = {2310.05126},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}

@inproceedings{yim2017gift,
	author       = {Yim, Junho and Joo, Donggyu and Bae, Jihoon and Kim, Junmo},
	year         = 2017,
	title        = {{A gift from knowledge distillation: Fast optimization, network minimization and transfer learning}},
	booktitle    = {{Proceedings of the IEEE conference on computer vision and pattern recognition}},
	pages        = {4133--4141}
}

@article{yin2023survey,
	author       = {Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
	year         = 2023,
	title        = {{A Survey on Multimodal Large Language Models}},
	journal      = {arXiv preprint arXiv:2306.13549}
}

@inproceedings{yoshikawa-etal-2017-stair,
	author       = {Yoshikawa, Yuya  and Shigeto, Yutaro  and Takeuchi, Akikazu},
	year         = 2017,
	month        = jul,
	title        = {{STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset}},
	booktitle    = {{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}},
	publisher    = {Association for Computational Linguistics},
	address      = {Vancouver, Canada},
	pages        = {417--421},
	doi          = {10.18653/v1/P17-2066},
	url          = {https://aclanthology.org/P17-2066},
	abstract     = {In recent years, automatic generation of image descriptions (captions), that is, image captioning, has attracted a great deal of attention. In this paper, we particularly consider generating Japanese captions for images. Since most available caption datasets have been constructed for English language, there are few datasets for Japanese. To tackle this problem, we construct a large-scale Japanese image caption dataset based on images from MS-COCO, which is called STAIR Captions. STAIR Captions consists of 820,310 Japanese captions for 164,062 images. In the experiment, we show that a neural network trained using STAIR Captions can generate more natural and better Japanese captions, compared to those generated using English-Japanese machine translation after generating English captions.}
}

@inproceedings{you-etal-2022-end,
	author       = {You, Chenyu  and Chen, Nuo  and Liu, Fenglin  and Ge, Shen  and Wu, Xian  and Zou, Yuexian},
	year         = 2022,
	month        = jul,
	title        = {{End-to-end Spoken Conversational Question Answering: Task, Dataset and Model}},
	booktitle    = {{Findings of the Association for Computational Linguistics: NAACL 2022}},
	publisher    = {Association for Computational Linguistics},
	address      = {Seattle, United States},
	pages        = {1219--1232},
	doi          = {10.18653/v1/2022.findings-naacl.91},
	url          = {https://aclanthology.org/2022.findings-naacl.91},
	abstract     = {In spoken question answering, the systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling the systems to model complex dialogues flow given the speech documents. In this task, our main objective is to build the system to deal with conversational questions based on the audio recordings, and to explore the plausibility of providing more cues from different modalities with systems in information gathering. To this end, instead of directly adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNet, which effectively ingests cross-modal information to achieve fine-grained representations of the speech and language modalities. Moreover, we propose a simple and novel mechanism, termed Dual Attention, by encouraging better alignments between audio and text to ease the process of knowledge transfer. To evaluate the capacity of SCQA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 40k question-answer pairs from 4k conversations. We first show that the performance of the existing state-of-the-art methods significantly degrade on our dataset, hence demonstrating the necessity of incorporating cross-modal information to achieve good performance gains. Our experimental results demonstrate that our proposed method achieves superior performance in spoken conversational question answering. Codes and datasets will be made publicly available.}
}

@inproceedings{you2017learning,
	author       = {You, Shan and Xu, Chang and Xu, Chao and Tao, Dacheng},
	year         = 2017,
	title        = {{Learning from multiple teacher networks}},
	booktitle    = {{Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}},
	pages        = {1285--1294}
}

@article{yu2011calibration,
	author       = {Yu, Dong and Li, Jinyu and Deng, Li},
	year         = 2011,
	title        = {{Calibration of confidence measures in speech recognition}},
	journal      = {IEEE Transactions on Audio, Speech, and Language Processing},
	publisher    = {Ieee},
	volume       = 19,
	number       = 8,
	pages        = {2461--2473}
}

@inproceedings{yu2020reclor,
	author       = {Yu, Weihao and Jiang, Zihang and Dong, Yanfei and Feng, Jiashi},
	year         = 2020,
	month        = {April},
	title        = {{ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning}},
	booktitle    = {{International Conference on Learning Representations (ICLR)}}
}

@inproceedings{yu2021pick,
	author       = {Yu, Wenwen and Lu, Ning and Qi, Xianbiao and Gong, Ping and Xiao, Rong},
	year         = 2021,
	title        = {{Pick: Processing key information extraction from documents using improved graph learning-convolutional networks}},
	booktitle    = {{2020 25th International Conference on Pattern Recognition (ICPR)}},
	pages        = {4363--4370},
	organization = {Ieee}
}

@inproceedings{yuan2020central,
	author       = {Yuan, Li and Wang, Tao and Zhang, Xiaopeng and Tay, Francis EH and Jie, Zequn and Liu, Wei and Feng, Jiashi},
	year         = 2020,
	title        = {{Central similarity quantization for efficient image and video retrieval}},
	booktitle    = {{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}},
	pages        = {3083--3092}
}

@inproceedings{yun2021re,
	author       = {Yun, Sangdoo and Oh, Seong Joon and Heo, Byeongho and Han, Dongyoon and Choe, Junsuk and Chun, Sanghyuk},
	year         = 2021,
	title        = {{Re-labeling imagenet: from single to multi-labels, from global to localized labels}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
	pages        = {2340--2350}
}

@inproceedings{zadrozny2002transforming,
	author       = {Zadrozny, Bianca and Elkan, Charles},
	year         = 2002,
	title        = {{Transforming classifier scores into accurate multiclass probability estimates}},
	booktitle    = {{Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}},
	pages        = {694--699}
}

% Big Bird
@article{zaheer2020big,
	author       = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
	year         = 2020,
	title        = {{Big bird: Transformers for longer sequences}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {17283--17297}
}

@inproceedings{Zaragoza1998ConfidenceMF,
	author       = {Zaragoza, Hugo and d’Alch{\'e}-Buc, Florence},
	year         = 1998,
	title        = {{Confidence Measures for Neural Network Classifiers}},
	booktitle    = {{Proceedings of the Seventh International Conference Information Processing and Management of Uncertainty in Knowledge Based Systems}}
}

@incollection{Zaremba2013,
	author       = {Zaremba, W. and Gretton, A. and Blaschko, M.},
	year         = 2013,
	title        = {{B-test: A Non-parametric, Low Variance Kernel Two-sample Test}},
	booktitle    = {{Advances in Neural Information Processing Systems 26}},
	pages        = {755--763}
}

@inproceedings{zarharan-etal-2021-parsfever,
	author       = {Zarharan, Majid  and Ghaderan, Mahsa  and Pourdabiri, Amin  and Sayedi, Zahra  and Minaei-Bidgoli, Behrouz  and Eetemadi, Sauleh  and Pilehvar, Mohammad Taher},
	year         = 2021,
	month        = aug,
	title        = {{ParsFEVER: a Dataset for Farsi Fact Extraction and Verification}},
	booktitle    = {{Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics}},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {99--104},
	doi          = {10.18653/v1/2021.starsem-1.9},
	url          = {https://aclanthology.org/2021.starsem-1.9},
	abstract     = {Training and evaluation of automatic fact extraction and verification techniques require large amounts of annotated data which might not be available for low-resource languages. This paper presents ParsFEVER: the first publicly available Farsi dataset for fact extraction and verification. We adopt the construction procedure of the standard English dataset for the task, i.e., FEVER, and improve it for the case of low-resource languages. Specifically, claims are extracted from sentences that are carefully selected to be more informative. The dataset comprises nearly 23K manually-annotated claims. Over 65{\%} of the claims in ParsFEVER are many-hop (require evidence from multiple sources), making the dataset a challenging benchmark (only 13{\%} of the claims in FEVER are many-hop). Also, despite having a smaller training set (around one-ninth of that in Fever), a model trained on ParsFEVER attains similar downstream performance, indicating the quality of the dataset. We release the dataset and the annotation guidelines at https://github.com/Zarharan/ParsFEVER.}
}

@article{zhang_cyclical_2019,
	author       = {Zhang, Ruqi and Li, Chunyuan and Zhang, Jianyi and Chen, Changyou and Wilson, Andrew Gordon},
	year         = 2019,
	month        = feb,
	title        = {{Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning}},
	journal      = {arXiv:1902.03932 [cs, stat]},
	url          = {http://arxiv.org/abs/1902.03932},
	urldate      = {2020-04-24},
	note         = {arXiv: 1902.03932},
	abstract     = {The posteriors over neural network weights are high dimensional and multimodal. Each mode typically characterizes a meaningfully different representation of the data. We develop Cyclical Stochastic Gradient MCMC (SG-MCMC) to automatically explore such distributions. In particular, we propose a cyclical stepsize schedule, where larger steps discover new modes, and smaller steps characterize each mode. We prove that our proposed learning rate schedule provides faster convergence to samples from a stationary distribution than SG-MCMC with standard decaying schedules. Moreover, we provide extensive experimental results to demonstrate the effectiveness of cyclical SG-MCMC in learning complex multimodal distributions, especially for fully Bayesian inference with modern deep neural networks.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file         = {arXiv Fulltext PDF:/home/jordy/snap/zotero-snap/common/Zotero/storage/4DTSSE5G/Zhang et al. - 2019 - Cyclical Stochastic Gradient MCMC for Bayesian Dee.pdf:application/pdf;arXiv.org Snapshot:/home/jordy/snap/zotero-snap/common/Zotero/storage/8CHNF9GR/1902.html:text/html}
}

@article{zhang_sensitivity_2016,
	author       = {Zhang, Ye and Wallace, Byron},
	year         = 2015,
	title        = {{A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification}},
	journal      = {arXiv preprint arXiv:1510.03820}
}

@inproceedings{zhang-etal-2021-noahqa-numerical,
	author       = {Zhang, Qiyuan  and Wang, Lei  and Yu, Sicheng  and Wang, Shuohang  and Wang, Yang  and Jiang, Jing  and Lim, Ee-Peng},
	year         = 2021,
	month        = nov,
	title        = {{NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset}},
	booktitle    = {{Findings of the Association for Computational Linguistics: EMNLP 2021}},
	publisher    = {Association for Computational Linguistics},
	address      = {Punta Cana, Dominican Republic},
	pages        = {4147--4161},
	doi          = {10.18653/v1/2021.findings-emnlp.350},
	url          = {https://aclanthology.org/2021.findings-emnlp.350},
	abstract     = {While diverse question answering (QA) datasets have been proposed and contributed significantly to the development of deep learning models for QA tasks, the existing datasets fall short in two aspects. First, we lack QA datasets covering complex questions that involve answers as well as the reasoning processes to get them. As a result, the state-of-the-art QA research on numerical reasoning still focuses on simple calculations and does not provide the mathematical expressions or evidence justifying the answers. Second, the QA community has contributed a lot of effort to improve the interpretability of QA models. However, they fail to explicitly show the reasoning process, such as the evidence order for reasoning and the interactions between different pieces of evidence. To address the above shortcoming, we introduce NOAHQA, a conversational and bilingual QA dataset with questions requiring numerical reasoning with compound mathematical expressions. With NOAHQA, we develop an interpretable reasoning graph as well as the appropriate evaluation metric to measure the answer quality. We evaluate the state-of-the-art QA models trained using existing QA datasets on NOAHQA and show that the best among them can only achieve 55.5 exact match scores, while the human performance is 89.7. We also present a new QA model for generating a reasoning graph where the reasoning graph metric still has a large gap compared with that of humans, eg, 28 scores.}
}

@article{zhang2018advances,
	author       = {Zhang, Cheng and B{\"u}tepage, Judith and Kjellstr{\"o}m, Hedvig and Mandt, Stephan},
	year         = 2018,
	title        = {{Advances in variational inference}},
	journal      = {IEEE transactions on pattern analysis and machine intelligence},
	publisher    = {Ieee},
	volume       = 41,
	number       = 8,
	pages        = {2008--2026}
}

@inproceedings{zhang2018deep,
	author       = {Zhang, Ying and Xiang, Tao and Hospedales, Timothy M and Lu, Huchuan},
	year         = 2018,
	title        = {{Deep mutual learning}},
	booktitle    = {{Proceedings of the IEEE conference on computer vision and pattern recognition}},
	pages        = {4320--4328}
}

@inproceedings{zhang2019complex,
	author       = {Zhang, Haoyu and Cai, Jingjing and Xu, Jianjun and Wang, Ji},
	year         = 2019,
	title        = {{Complex question decomposition for semantic parsing}},
	booktitle    = {{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}},
	pages        = {4477--4486}
}

@inproceedings{zhang2019mitigating,
	author       = {Zhang, Xuchao  and Chen, Fanglan  and Lu, Chang-Tien  and Ramakrishnan, Naren},
	year         = 2019,
	month        = jun,
	title        = {{Mitigating Uncertainty in Document Classification}},
	journal      = {arXiv preprint arXiv:1907.07590},
	booktitle    = {{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics}},
	publisher    = {Association for Computational Linguistics},
	address      = {Minneapolis, Minnesota},
	pages        = {3126--3136},
	doi          = {10.18653/v1/N19-1316}
}

@inproceedings{zhang2019your,
	author       = {Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng},
	year         = 2019,
	title        = {{Be your own teacher: Improve the performance of convolutional neural networks via self distillation}},
	booktitle    = {{Proceedings of the IEEE/CVF International Conference on Computer Vision}}
}

@inproceedings{zhang2020distilling,
	author       = {Zhang, Zizhao and Zhang, Han and Arik, Sercan O and Lee, Honglak and Pfister, Tomas},
	year         = 2020,
	title        = {{Distilling effective supervision from severe label noise}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
	pages        = {9294--9303}
}

@inproceedings{zhang2020effect,
	author       = {Zhang, Yunfeng and Liao, Q Vera and Bellamy, Rachel KE},
	year         = 2020,
	title        = {{Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making}},
	booktitle    = {{Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency}},
	pages        = {295--305}
}

@inproceedings{zhang2020mix,
	author       = {Zhang, Jize and Kailkhura, Bhavya and Han, T Yong-Jin},
	year         = 2020,
	title        = {{Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning}},
	booktitle    = {{International Conference on Machine Learning}},
	pages        = {11117--11128},
	organization = {Pmlr}
}

@article{zhang2020revisiting,
	author       = {Zhang, Tianyi and Wu, Felix and Katiyar, Arzoo and Weinberger, Kilian Q and Artzi, Yoav},
	year         = 2020,
	title        = {{Revisiting Few-sample BERT Fine-tuning}},
	journal      = {arXiv preprint arXiv:2006.05987}
}

@article{zhang2021knowing,
	author       = {Zhang, Shujian and Gong, Chengyue and Choi, Eunsol},
	year         = 2021,
	title        = {{Knowing More About Questions Can Help: Improving Calibration in Question Answering}},
	journal      = {arXiv preprint arXiv:2106.01494}
}

@misc{zhang2022nail,
	author       = {Xinbo Zhang and Changzhi Sun and Yue Zhang and Lei Li and Hao Zhou},
	year         = 2022,
	title        = {{NAIL: A Challenging Benchmark for Na\textbackslash}''ive Logical Reasoning}}
}

@article{zhang2023instruction,
	author       = {Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others},
	year         = 2023,
	title        = {{Instruction Tuning for Large Language Models: A Survey}},
	journal      = {arXiv preprint arXiv:2308.10792}
}

@article{zhang2023openood,
	author       = {Zhang, Jingyang and Yang, Jingkang and Wang, Pengyun and Wang, Haoqi and Lin, Yueqian and Zhang, Haoran and Sun, Yiyou and Du, Xuefeng and Zhou, Kaiyang and Zhang, Wayne and Li, Yixuan and Liu, Ziwei and Chen, Yiran and Li, Hai},
	year         = 2023,
	title        = {{OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection}},
	journal      = {arXiv preprint arXiv:2306.09301}
}

,
}
@article{zhang2023survey,
	author       = {Zhang, Xu-Yao and Xie, Guo-Sen and Li, Xiuli and Mei, Tao and Liu, Cheng-Lin},
	year         = 2023,
	title        = {{A survey on learning to reject}},
	journal      = {Proceedings of the IEEE},
	publisher    = {Ieee},
	volume       = 111,
	number       = 2,
	pages        = {185--215}
}

@inproceedings{zhang2023uncertainty,
	author       = {Zhang, Dell and Sensoy, Murat and Makrehchi, Masoud and Taneva-Popova, Bilyana and Gui, Lin and He, Yulan},
	year         = 2023,
	title        = {{Uncertainty quantification for text classification}},
	booktitle    = {{Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval}},
	pages        = {3426--3429}
}

@inproceedings{zhang2023vqacl,
	author       = {Zhang, Xi and Zhang, Feifei and Xu, Changsheng},
	year         = 2023,
	title        = {{VQACL: A Novel Visual Question Answering Continual Learning Setting}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
	pages        = {19102--19112}
}

}
@article{zhao2020review,
	author       = {Zhao, Sicheng and Yue, Xiangyu and Zhang, Shanghang and Li, Bo and Zhao, Han and Wu, Bichen and Krishna, Ravi and Gonzalez, Joseph E and Sangiovanni-Vincentelli, Alberto L and Seshia, Sanjit A and others},
	year         = 2020,
	title        = {{A Review of Single-Source Deep Unsupervised Visual Domain Adaptation}},
	journal      = {IEEE Transactions on Neural Networks and Learning Systems},
	publisher    = {Ieee}
}

@inproceedings{zhao2021calibrating,
	author       = {Shengjia Zhao and Michael P. Kim and Roshni Sahoo and Tengyu Ma and Stefano Ermon},
	year         = 2021,
	title        = {{Calibrating Predictions to Decisions: A Novel Approach to Multi-Class Calibration}},
	booktitle    = {{Thirty-Fifth Conference on Neural Information Processing Systems}},
	url          = {https://openreview.net/forum?id=iFF-zKCgzS}
}

@inproceedings{zhao2022decoupled,
	author       = {Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun},
	year         = 2022,
	title        = {{Decoupled knowledge distillation}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition}},
	pages        = {11953--11962}
}

@article{zhao2023survey,
	author       = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
	year         = 2023,
	title        = {{A survey of large language models}},
	journal      = {arXiv preprint arXiv:2303.18223}
}

@inproceedings{zheng2021global,
	author       = {Zheng, Xinyi and Burdick, Douglas and Popa, Lucian and Zhong, Xu and Wang, Nancy Xin Ru},
	year         = 2021,
	title        = {{Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context}},
	booktitle    = {{Proceedings of the IEEE/CVF winter conference on applications of computer vision}},
	pages        = {697--706}
}

@inproceedings{zhong2019publaynet,
	author       = {Zhong, Xu and Tang, Jianbin and Yepes, Antonio Jimeno},
	year         = 2019,
	title        = {{Publaynet: largest dataset ever for document layout analysis}},
	booktitle    = {{2019 International Conference on Document Analysis and Recognition (ICDAR)}},
	pages        = {1015--1022},
	organization = {Ieee}
}

@inproceedings{zhong2020image,
	author       = {Zhong, Xu and ShafieiBavani, Elaheh and Jimeno Yepes, Antonio},
	year         = 2020,
	title        = {{Image-based table recognition: data, model, and evaluation}},
	booktitle    = {{Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXI 16}},
	pages        = {564--580},
	organization = {Springer}
}

@inproceedings{zhou2015neural,
	author       = {Zhou, Hao and Zhang, Yue and Huang, Shujian and Chen, Jiajun},
	year         = 2015,
	title        = {{A neural probabilistic structured-prediction model for transition-based dependency parsing}},
	booktitle    = {{Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}},
	pages        = {1213--1222}
}

@article{zhou2020bert,
	author       = {Zhou, Wangchunshu and Xu, Canwen and Ge, Tao and McAuley, Julian and Xu, Ke and Wei, Furu},
	year         = 2020,
	title        = {{Bert loses patience: Fast and robust inference with early exit}},
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {18330--18341}
}

@article{Zhu_2017,
	author       = {Zhu, Lingxue and Laptev, Nikolay},
	year         = 2017,
	month        = {Nov},
	title        = {{Deep and Confident Prediction for Time Series at Uber}},
	journal      = {2017 IEEE International Conference on Data Mining Workshops (ICDMW)},
	publisher    = {Ieee},
	doi          = {10.1109/icdmw.2017.19},
	isbn         = 9781538638002,
	url          = {http://dx.doi.org/10.1109/ICDMW.2017.19}
}

@inproceedings{Zhu_2022,
	author       = {Fengbin Zhu and Wenqiang Lei and Fuli Feng and Chao Wang and Haozhou Zhang and Tat-Seng Chua},
	year         = 2022,
	month        = {oct},
	title        = {{Towards Complex Document Understanding By Discrete Reasoning}},
	booktitle    = {{Proceedings of the 30th ACM International Conference on Multimedia}},
	publisher    = {Acm},
	doi          = {10.1145/3503161.3548422},
	url          = {https://doi.org/10.1145}
}

@inproceedings{zhu-etal-2023-beyond-layout,
	author       = {Zhu, Xi  and Han, Xue  and Peng, Shuyuan  and Lei, Shuo  and Deng, Chao  and Feng, Junlan},
	year         = 2023,
	month        = dec,
	title        = {{Beyond Layout Embedding: Layout Attention with Gaussian Biases for Structured Document Understanding}},
	booktitle    = {{Findings of the Association for Computational Linguistics: EMNLP 2023}},
	publisher    = {Association for Computational Linguistics},
	address      = {Singapore},
	pages        = {7773--7784},
	doi          = {10.18653/v1/2023.findings-emnlp.521},
	url          = {https://aclanthology.org/2023.findings-emnlp.521},
	editor       = {Bouamor, Houda  and Pino, Juan  and Bali, Kalika},
	abstract     = {Effectively encoding layout information is a central problem in structured document understanding. Most existing methods rely heavily on millions of trainable parameters to learn the layout features of each word from Cartesian coordinates. However, two unresolved questions remain: (1) Is the Cartesian coordinate system the optimal choice for layout modeling? (2) Are massive learnable parameters truly necessary for layout representation? In this paper, we address these questions by proposing Layout Attention with Gaussian Biases (LAGaBi): Firstly, we find that polar coordinates provide a superior choice over Cartesian coordinates as they offer a measurement of both distance and angle between word pairs, capturing relative positions more effectively. Furthermore, by feeding the distances and angles into 2-D Gaussian kernels, we model intuitive inductive layout biases, i.e., the words closer within a document should receive more attention, which will act as the attention biases to revise the textual attention distribution. LAGaBi is model-agnostic and language-independent, which can be applied to a range of transformer-based models, such as the text pre-training models from the BERT series and the LayoutLM series that incorporate visual features. Experimental results on three widely used benchmarks demonstrate that, despite reducing the number of layout parameters from millions to 48, LAGaBi achieves competitive or even superior performance.}
}

@inproceedings{zhu2007automatic,
	author       = {Zhu, Guangyu and Doermann, David},
	year         = 2007,
	title        = {{Automatic document logo detection}},
	booktitle    = {{Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)}},
	volume       = 2,
	pages        = {864--868},
	organization = {Ieee}
}

@article{zhu2017prune,
	author       = {Zhu, Michael and Gupta, Suyog},
	year         = 2017,
	title        = {{To prune, or not to prune: exploring the efficacy of pruning for model compression}},
	journal      = {arXiv preprint arXiv:1710.01878}
}

@inproceedings{zhu2022rethinking,
	author       = {Zhu, Fei and Cheng, Zhen and Zhang, Xu-Yao and Liu, Cheng-Lin},
	year         = 2022,
	title        = {{Rethinking Confidence Calibration for Failure Prediction}},
	booktitle    = {{Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXV}},
	pages        = {518--536},
	organization = {Springer}
}

@inproceedings{zhu2022towards,
	author       = {Zhu, Fengbin and Lei, Wenqiang and Feng, Fuli and Wang, Chao and Zhang, Haozhou and Chua, Tat-Seng},
	year         = 2022,
	title        = {{Towards complex document understanding by discrete reasoning}},
	booktitle    = {{Proceedings of the 30th ACM International Conference on Multimedia}},
	pages        = {4857--4866}
}

@inproceedings{zhu2023openmix,
	author       = {Zhu, Fei and Cheng, Zhen and Zhang, Xu-Yao and Liu, Cheng-Lin},
	year         = 2023,
	title        = {{OpenMix: Exploring Outlier Samples for Misclassification Detection}},
	booktitle    = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}},
	pages        = {12074--12083}
}

@article{zhu2023survey,
	author       = {Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
	year         = 2023,
	title        = {{A survey on model compression for large language models}},
	journal      = {arXiv preprint arXiv:2308.07633}
}

@inproceedings{ziser2016neural,
	author       = {Ziser, Yftah  and Reichart, Roi},
	year         = 2017,
	month        = aug,
	title        = {{Neural Structural Correspondence Learning for Domain Adaptation}},
	booktitle    = {{Proceedings of the 21st Conference on Computational Natural Language Learning}},
	publisher    = {Association for Computational Linguistics},
	address      = {Vancouver, Canada},
	pages        = {400--410},
	doi          = {10.18653/v1/K17-1040},
	url          = {https://www.aclweb.org/anthology/K17-1040}
}
